<?xml version="1.0" encoding="UTF-8"?>
<codemap>
  <repository>
    <metadata>
      <name>shaneholloman/codemapper</name>
      <description>Repository contents for LLM context</description>
      <fetch_date>2024-11-24T05:18:37.470Z</fetch_date>
      <branch>main</branch>
    </metadata>
    <contents>
      <directory name=".github" path=".github">
        <file name="FUNDING.yml" path=".github/FUNDING.yml">
          <content><![CDATA[
            # These are supported funding model platforms

            github: [shaneholloman]
            patreon: # Replace with a single Patreon username
            open_collective: # Replace with a single Open Collective username
            ko_fi: # Replace with a single Ko-fi username
            tidelift: # Replace with a single Tidelift platform-name/package-name e.g., npm/babel
            community_bridge: # Replace with a single Community Bridge project-name e.g., cloud-foundry
            liberapay: # Replace with a single Liberapay username
            issuehunt: # Replace with a single IssueHunt username
            lfx_crowdfunding: # Replace with a single LFX Crowdfunding project-name e.g., cloud-foundry
            polar: # Replace with a single Polar username
            buy_me_a_coffee: # Replace with a single Buy Me a Coffee username
            thanks_dev: # Replace with a single thanks.dev username
            custom: # Replace with up to 4 custom sponsorship URLs e.g., ['link1', 'link2']

          ]]></content>
        </file>
        <directory name="workflows" path=".github/workflows">
          <file name="pylint.yml" path=".github/workflows/pylint.yml">
            <content><![CDATA[
              name: Pylint

              on: [push, workflow_dispatch]

              jobs:
                build:
                  runs-on: ubuntu-latest
                  strategy:
                    matrix:
                      python-version: ['3.10', '3.11', '3.12']
                  steps:
                    - uses: actions/checkout@v4
                    - name: Set up Python ${{ matrix.python-version }}
                      uses: actions/setup-python@v5
                      with:
                        python-version: ${{ matrix.python-version }}
                    - name: Install dependencies
                      run: |
                        python -m pip install --upgrade pip
                        pip install setuptools pylint codemapper openpyxl xlrd pandas ezodf pytest
                    - name: Analyzing the code with pylint
                      run: |
                        pylint $(git ls-files '*.py')

            ]]></content>
          </file>
          <file name="pytest.yml" path=".github/workflows/pytest.yml">
            <content><![CDATA[
              name: PyTest

              on:
                push:
                  branches: ['main']
                pull_request:
                workflow_dispatch:

              jobs:
                test:
                  runs-on: ubuntu-latest
                  strategy:
                    matrix:
                      python-version: ['3.10', '3.11', '3.12']
                  steps:
                    - name: Checkout code
                      uses: actions/checkout@v4

                    - name: Set up Python
                      uses: actions/setup-python@v5
                      with:
                        python-version: ${{ matrix.python-version }}

                    - name: Install dependencies
                      run: |
                        python -m pip install --upgrade pip
                        pip install -r requirements.txt
                        pip install pytest pathspec

                    - name: Install codemapper
                      run: |
                        pip install .

                    - name: Run tests
                      env:
                        PYTHONPATH: src
                      run: |
                        pytest -v tests/test_github_codemapper.py

            ]]></content>
          </file>
        </directory>
      </directory>
      <file name="LICENSE" path="LICENSE">
        <content><![CDATA[
          MIT License

          Copyright (c) 2024 Shane Holloman

          Permission is hereby granted, free of charge, to any person obtaining a copy
          of this software and associated documentation files (the "Software"), to deal
          in the Software without restriction, including without limitation the rights
          to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
          copies of the Software, and to permit persons to whom the Software is
          furnished to do so, subject to the following conditions:

          The above copyright notice and this permission notice shall be included in all
          copies or substantial portions of the Software.

          THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
          IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
          FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
          AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
          LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
          OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
          SOFTWARE.

        ]]></content>
      </file>
      <file name="README.md" path="README.md">
        <content><![CDATA[
          # CodeMapper

          [![Pylint](https://github.com/shaneholloman/codemapper/actions/workflows/pylint.yml/badge.svg)](https://github.com/shaneholloman/codemapper/actions/workflows/pylint.yml)
          [![PyTest](https://github.com/shaneholloman/codemapper/actions/workflows/pytest.yml/badge.svg)](https://github.com/shaneholloman/codemapper/actions/workflows/pytest.yml)
          [![TODO](https://img.shields.io/badge/âœ”%20RoadMap-45-blue)](notes/todo.md)
          ![logo](codemapper-outlined.webp)

          <!-- TOC -->

          - [CodeMapper](#codemapper)
            - [Overview](#overview)
            - [Key Features](#key-features)
            - [AI Chat Integration](#ai-chat-integration)
            - [Use Cases](#use-cases)
              - [For AI Engineers](#for-ai-engineers)
              - [For Hobbyists](#for-hobbyists)
            - [Getting Started](#getting-started)
            - [Example Output](#example-output)
            - [Future Development](#future-development)
            - [Target Audiences](#target-audiences)
              - [AI Engineers Will Appreciate](#ai-engineers-will-appreciate)
              - [Hobbyists Will Value](#hobbyists-will-value)
            - [Installation Options](#installation-options)
            - [Contributing](#contributing)
            - [Resources](#resources)
            - [License](#license)
            - [Acknowledgments](#acknowledgments)
            - [Version History](#version-history)

          <!-- /TOC -->

          ## Overview

          > [!IMPORTANT]
          > CodeMapper: Bridging Code Understanding for AI and Human Analysis
          > Install: `pip install codemapper`

          CodeMapper is a powerful Python tool that transforms complex codebases into navigable, single-file Markdown artifacts, with a unique ability to bootstrap AI chat prompts for code analysis. Designed with both AI engineers and hobbyist developers in mind, it serves as a bridge between traditional code exploration and modern AI-assisted development workflows. Whether you're training large language models, conducting interactive AI-assisted code reviews, or simply trying to understand a new project, CodeMapper provides a unified, accessible view of your codebase that's optimized for both human readability and AI consumption.

          ## Key Features

          - **Unified Code Visualization**: Automatically generates a comprehensive Markdown representation of your entire codebase, including:
            - Complete directory structure visualization
            - Syntax-highlighted code content
            - Intelligent file categorization
            - Documentation aggregation

          - **AI Integration Optimizations**:
            - Structured output format ideal for LLM training and analysis
            - Consistent formatting for improved token efficiency
            - Built-in support for common documentation patterns
            - Metadata preservation for enhanced context

          - **Git-Aware Processing**:
            - Respects `.gitignore` rules by default
            - Direct GitHub repository URL support
            - Handles large repositories efficiently
            - Smart binary file detection

          - **Documentation Focus**:
            - Dedicated DocMap generation for documentation-heavy projects
            - README file prioritization
            - Support for multiple documentation directory conventions
            - Markdown-native output for universal compatibility

          ## AI Chat Integration

          CodeMapper excels at bootstrapping AI chat interactions for code analysis. When you need to understand, debug, or enhance your code with AI assistance:

          1. Generate a codemap of your project
          2. Copy the generated markdown into your AI chat
          3. Start asking detailed questions about your codebase

          The AI assistant receives:

          - Complete project structure
          - All file contents with proper syntax highlighting
          - Documentation in context
          - Clear navigation structure

          This enables the AI to provide more accurate, contextual responses about your code without manual file copying or context limitations.

          ## Use Cases

          ### For AI Engineers

          - **AI Chat Prompt Bootstrapping**: Instantly generate context-rich prompts for AI chat interactions about your code
          - **Interactive Code Analysis**: Seamlessly feed comprehensive codebase context to AI assistants
          - **Training Data Preparation**: Generate consistently formatted codebase representations for model training
          - **Documentation Generation**: Train models on well-structured documentation patterns
          - **Code Understanding**: Feed entire codebases to LLMs for comprehensive analysis

          Example AI Chat Workflow:

          ```bash
          # Generate a codemap for your project
          codemapper /path/to/project

          # The generated codemap can be directly used in AI chat prompts:
          "Here's my project structure and code, help me understand the dependency flow:
          [paste _codemap.md content]"
          ```

          ### For Hobbyists

          - **Project Exploration**: Quickly understand new codebases without complex IDE setup
          - **Documentation Creation**: Generate comprehensive project documentation automatically
          - **Code Reviews**: Facilitate easier code reviews with a unified view
          - **Learning Tool**: Study and understand how different projects are structured

          ## Getting Started

          ```bash
          # Install from PyPI
          pip install codemapper

          # Basic usage
          codemapper /path/to/project

          # Generate documentation map
          codemapper --docs /path/to/project

          # Process GitHub repository
          codemapper https://github.com/username/repo
          ```

          ## Example Output

          CodeMapper generates two main types of outputs:

          1. **CodeMap** (`project_codemap.md`):
             - Complete directory tree
             - File contents with syntax highlighting
             - Smart handling of binary and large files
             - Navigation-optimized structure

          2. **DocMap** (`project_docmap.md`):
             - Documentation-focused view
             - README files
             - Documentation directory contents
             - Structured for easy consumption

          ## Future Development

          CodeMapper is actively developing focused on:

          [ToDos](docs/todo.md)

          - Expanded format support (JSON, YAML, reStructuredText)
          - Enhanced AI integration capabilities
          - Real-time code change tracking
          - Collaborative annotation features
          - Intelligent code pattern recognition
          - API access for programmatic integration

          ## Target Audiences

          ### AI Engineers Will Appreciate

          - Consistent, clean output format for training data
          - Efficient handling of large repositories
          - Structured metadata preservation
          - Integration-ready design

          ### Hobbyists Will Value

          - Simple, straightforward installation
          - Clear, readable output
          - No complex configuration required
          - Immediate utility for project understanding

          ## Installation Options

          ```bash
          # For basic installation
          pip install codemapper

          # For development installation
          git clone https://github.com/shaneholloman/codemapper
          cd codemapper
          pip install -e .
          ```

          ## Contributing

          CodeMapper welcomes contributions from both AI engineers and hobbyist developers. The project maintains a balance between sophisticated features for AI integration and accessibility for general use.

          Visit the [GitHub repository](https://github.com/shaneholloman/codemapper) to:

          - Report issues
          - Submit feature requests
          - Contribute code
          - Join the discussion

          ## Resources

          - [GitHub Repository](https://github.com/shaneholloman/codemapper)
          - [PyPI Package](https://pypi.org/project/codemapper/)
          - [Issue Tracker](https://github.com/shaneholloman/codemapper/issues)
          - [Changelog](https://github.com/shaneholloman/codemapper/blob/main/changelog.md)

          CodeMapper represents a bridge between traditional code exploration and modern AI-assisted development, making codebases more accessible and understandable for everyone from AI researchers to hobbyist developers.

          ## License

          This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

          ## Acknowledgments

          - Thanks to the `pathspec` and `chardet` libraries for enhancing CodeMapper's functionality.

          ## Version History

          For a detailed version history, please refer to the [changelog.md](changelog.md).

          ---

          If you find CodeMapper useful, don't forget to star this repository!

        ]]></content>
      </file>
      <file name="changelog.md" path="changelog.md">
        <content><![CDATA[
          # Changelog

          ## Version History

          - 4.0.1 (2024-11-24):
            - feature: Add DocMap functionality for documentation mapping
              - Added new CLI options:
                - --docs flag to generate documentation maps
                - --docs-dir option for custom documentation directory paths
              - Generates repository-name_docmap.md with focus on documentation
              - Automatically includes root README.md content
              - Scans standard doc directories (docs, doc, documentation)
            - refactor: Implement DocMapConfig dataclass for parameter handling
              - Added new docmap.py module for documentation functionality
              - Introduced DocMapConfig for improved parameter organization
              - Added documentation directory constants in config.py
              - Enhanced error handling for missing documentation
              - Achieved perfect pylint score (10.00/10)
            - [View Full Changelog](https://github.com/shaneholloman/codemapper/compare/v3.8.0...v4.0.1)

          - 3.8.0 (2024-11-23):
            - refactor: Split codebase into modular structure
              - Split main functionality into separate modules:
                - main.py: Entry point and CLI handling
                - utils.py: Core functionality and helper functions
                - config.py: Configuration constants and settings
              - Improved code organization and maintainability
              - Fixed linting issues and long lines
            - [View Full Changelog](https://github.com/shaneholloman/codemapper/compare/v3.7.0...v3.8.0)

          - 3.7.0 (2024-10-02):
            - Added setuptools to dependencies in pylint workflow
            - Updated pylint workflow trigger to include manual dispatch
            - Updated Python versions in pylint workflow to support 3.10, 3.11, and 3.12
            - [View Full Changelog](https://github.com/shaneholloman/codemapper/compare/v3.6.0...v3.7.0)

          - 3.6.0 (2024-10-01):
            - Added MIT License to the repository
            - Updated README to include upcoming output formats for CodeMapper
            - Added requirements.txt and updated README for CodeMapper enhancements
            - Rigorously reconciled our readme and function docstrings with each other and with the code itself
            - [View Full Changelog](https://github.com/shaneholloman/codemapper/compare/v3.5.3...v3.6.0)

          - 3.5.3 (2024-10-01):
            - Improved large file handling and simplified command-line options
            - [View Full Changelog](https://github.com/shaneholloman/codemapper/compare/v3.5.0...v3.5.3)

          - 3.5.0 (2024-09-27):
            - Enhanced Table of Contents (TOC) generation and path normalization
            - Improved handling of large and binary files
            - Simplified command-line options
            - Added PyPI installation support
            - [View Full Changelog](https://github.com/shaneholloman/codemapper/compare/v3.4.5...v3.5.0)

          - 3.3.0 (2024-09-24):
            - Improved Table of Contents (TOC) generation:
              - Fixed issue with double dots appearing for hidden files and directories
              - Standardized path separators to forward slashes for cross-platform consistency
              - Corrected handling of file paths with backslashes on Windows systems
            - Enhanced path normalization in file collection process:
              - Ensured consistent use of forward slashes in file paths across all platforms
            - Refined TOC link generation:
              - Removed unwanted '%' characters from TOC links
              - Improved handling of special characters in file and directory names
            - Updated `generate_toc` function for better accuracy and readability:
              - Preserved original path structure, including single leading dots for hidden items
              - Eliminated redundant dot addition for already dot-prefixed paths
            - Optimized `collect_file_paths` function:
              - Implemented consistent path normalization to forward slashes
              - Improved cross-platform compatibility in file path handling

          - 3.2.0 (2024-09-23):
            - Improved handling of large and binary files:
              - Large and binary files are now always acknowledged without attempting to print their contents
              - File type and size information is displayed for large and binary files
            - Removed option to include large file contents as it's not practical for binary files
            - Simplified command-line options by removing flags related to large file handling
            - Added PyPI installation support

          - 3.1.2 (2024-09-23):
            - Restored important formatting functionality in generate_markdown_document function
            - Ensures proper spacing between file contents and correct placement of the concluding message

          - 3.1.1 (2024-09-23):
            - Fixed unused variable issue in generate_markdown_document function
            - Improved code quality without changing functionality

          - 3.1.0 (2024-09-23):
            - Added support for GitHub repositories
            - Implemented large file handling (now default in 3.2.0)

          - 3.0.0 (2024-09-23):
            - Major refactor
            - Added '_codemaps' output directory for generated markdown files
            - Improved error handling and user feedback

          - 2.5.0 (2024-09-10):
            - Initial version
            - Basic functionality for local directory mapping
          <!-- TOC -->

          - [Changelog](#changelog)
            - [Version History](#version-history)

          <!-- /TOC -->

        ]]></content>
      </file>
      <file name="codemapper-outlined.svg" path="codemapper-outlined.svg">
        <content><![CDATA[
          <?xml version="1.0" encoding="UTF-8" standalone="no"?>
          <!-- Created with Inkscape (http://www.inkscape.org/) -->

          <svg
             version="1.1"
             id="svg1"
             width="1024"
             height="1024"
             viewBox="0 0 1024 1024"
             xmlns="http://www.w3.org/2000/svg"
             xmlns:svg="http://www.w3.org/2000/svg">
            <defs
               id="defs1" />
            <g
               id="g1">
              <path
                 style="fill:#ffffff;fill-opacity:1;stroke:#000000;stroke-width:1.9;stroke-dasharray:none;stroke-opacity:1"
                 d="m 508.24988,874.06605 c -2.22761,-1.61367 -4.37305,-3.45632 -4.76763,-4.09477 -0.39459,-0.63845 -0.64085,-106.62135 -0.54726,-235.51755 l 0.17016,-234.35672 -5.68548,-6.29851 c -9.24163,-10.23809 -99.48162,-99.80096 -104.75676,-103.97072 -6.03661,-4.77167 -8.67765,-4.84321 -17.43787,-0.4724 -3.69877,1.84546 -7.97483,3.65712 -9.50235,4.02591 -2.0471,0.49423 -43.34876,3.53811 -48.97269,3.60923 -0.4125,0.005 -0.74834,0.57198 -0.7463,1.25948 0.002,0.6875 2.8215,2.70689 6.26549,4.48754 C 332.08345,307.8118 358,331.82475 358,335.84387 c 0,0.7635 -1.50378,3.47347 -3.34174,6.02215 -1.83796,2.54869 -3.6382,5.4589 -4.00053,6.46714 -0.36233,1.00823 -1.06478,1.68323 -1.561,1.5 -0.49622,-0.18324 -8.25848,-7.48546 -17.24947,-16.22716 -8.99099,-8.7417 -18.06049,-17.27519 -20.15444,-18.96331 -2.09395,-1.68813 -5.38278,-5.66305 -7.30852,-8.83316 -1.92574,-3.17011 -3.92432,-6.02525 -4.44129,-6.34475 -0.51696,-0.3195 -3.33623,1.6854 -6.26502,4.45533 -9.15495,8.65837 -18.94984,11.59632 -28.95969,8.68638 -8.2224,-2.39032 -13.57732,-6.93943 -17.7183,-15.05206 -3.0818,-6.03758 -3.48776,-7.70509 -3.39753,-13.95566 0.12653,-8.76508 2.50915,-14.48811 8.47298,-20.35197 8.68893,-8.54329 21.76868,-10.66359 32.54936,-5.27642 7.17805,3.58691 10.51533,6.91357 14.14177,14.09676 l 2.74279,5.43286 32.99531,0.61276 c 18.14743,0.33702 35.80782,0.96362 39.24532,1.39246 5.17294,0.64533 6.25,0.52767 6.25,-0.68272 C 380,276.91033 341.66064,239 339.72683,239 c -0.81542,0 -3.09808,0.675 -5.07259,1.5 -10.12286,4.2296 -22.82943,-1.72361 -29.3769,-13.7635 -1.28914,-2.37055 -1.76237,-5.53172 -1.7687,-11.81492 -0.008,-7.81107 0.26208,-9.0163 3.0178,-13.47356 4.4581,-7.21081 10.83965,-10.82974 19.87617,-11.27163 6.26135,-0.30619 7.74311,-0.008 12.57911,2.53101 3.16131,1.65979 6.85366,4.67681 8.72278,7.12739 5.15721,6.76155 5.85994,11.03733 4.06824,24.75312 -0.20534,1.57194 3.68114,5.7406 15.72726,16.86912 8.8,8.12967 21.85,20.49565 29,27.47995 C 412.45896,284.52611 421.81112,293 423.057,293 c 1.02628,0 1.35493,-53.69033 0.35913,-58.66933 -0.46055,-2.30279 -1.63042,-3.39944 -5.53944,-5.19274 -11.85503,-5.4386 -17.84952,-23.39052 -12.1258,-36.3136 8.69386,-19.62909 32.3967,-24.06476 46.72713,-8.74437 6.81916,7.29023 9.14307,14.72311 7.32233,23.42004 -1.92395,9.18996 -8.58104,18.92388 -14.66589,21.44431 l -2.84395,1.178 -0.52267,40.68884 -0.52267,40.68885 26.07364,26.5 c 26.32104,26.75145 34.11346,33.90791 35.44968,32.55656 0.40233,-0.40689 0.73151,-12.47172 0.73151,-26.81072 v -26.07092 l 9,-0.0875 9,-0.0875 0.64147,6 c 0.3528,3.3 0.6903,15.5625 0.75,27.25 0.0597,11.6875 0.44603,21.24311 0.85853,21.23469 1.66759,-0.034 4.82039,-3.00711 32.53124,-30.67663 L 585.13233,312.5 l -0.25585,-40 c -0.14071,-22 -0.4357,-40.54182 -0.65552,-41.20405 -0.21982,-0.66223 -2.39346,-2.16923 -4.83031,-3.34889 C 571.96219,224.35101 566,213.47599 566,203.52257 c 0,-24.8827 31.20826,-38.2624 48.30805,-20.71077 13.32422,13.6763 9.53775,37.77652 -7.30805,46.51446 l -4.5,2.33416 -0.2647,30.19553 c -0.2311,26.3626 -0.0702,30.27017 1.26757,30.78353 1.02795,0.39446 5.07314,-2.82312 12.29022,-9.77575 5.91687,-5.70005 21.54646,-20.69637 34.73243,-33.32514 18.28562,-17.51294 23.89243,-23.44515 23.6287,-25 -0.19018,-1.12123 -0.54097,-5.41359 -0.77953,-9.53859 -0.39966,-6.91044 -0.19985,-7.90247 2.54175,-12.61997 5.59842,-9.63327 17.78321,-14.88446 28.81114,-12.41651 13.77566,3.08285 22.74853,18.00714 18.86274,31.37377 -4.16147,14.31498 -22.19492,24.20105 -33.38508,18.30195 l -3.29477,-1.7369 -19.45523,18.70438 C 656.75486,266.89412 648,275.91389 648,276.65066 c 0,1.09641 6.96519,1.33682 38.37177,1.32447 l 38.37178,-0.0151 1.65344,-4.32948 c 5.32277,-13.93747 23.00376,-21.97965 37.63714,-17.11922 6.14923,2.04244 13.13015,8.01679 16.29298,13.94371 1.75188,3.28288 2.15855,5.74055 2.15855,13.04495 0,10.95822 -1.69832,14.75184 -10.00118,22.34019 -11.89583,10.87211 -25.27339,10.42997 -38.2137,-1.26299 -6.31598,-5.70718 -7.55016,-5.53788 -13.08661,1.79509 -2.19891,2.91243 -9.65105,10.61204 -16.56032,17.11024 -6.90927,6.49819 -15.54382,15.34798 -19.18789,19.66618 -3.64406,4.31821 -7.17201,7.85129 -7.83988,7.85129 -1.54621,0 -9.59529,-11.59188 -9.59848,-13.82325 -0.001,-0.92221 8.77369,-10.28559 19.5,-20.8075 C 698.22392,305.84733 707,297.01574 707,296.74349 c 0,-0.27225 -9.3375,-0.72622 -20.75,-1.00881 -24.11671,-0.59717 -30.47892,-1.62928 -38.91248,-6.31256 C 643.94815,287.53995 640.74301,286 640.21497,286 c -1.16301,0 -10.20373,7.17123 -20.17107,16 -4.03604,3.575 -27.52482,27.137 -52.19729,52.35999 l -44.85904,45.85999 0.0125,233.14001 c 0.007,128.22701 0.01,233.9069 0.006,234.8442 -0.006,1.53109 -8.26493,8.79581 -10.00001,8.79581 -0.38839,0 -2.52875,-1.32028 -4.75636,-2.93395 z M 541,648.69194 V 450.38388 l 18.25,-17.98314 C 594.24985,397.91267 606.72403,386 607.83785,386 c 0.61683,0 2.09214,1.9125 3.27848,4.25 1.18634,2.3375 3.26989,5.58074 4.63011,7.20719 l 2.47312,2.9572 -2.85978,2.42471 C 606.59446,410.27092 576,440.84693 576,442.17514 c 0,1.70348 10.00057,2.63066 32.36093,3.00028 9.56239,0.15807 10.98232,-0.0374 11.87633,-1.6349 2.41117,-4.30851 8.63551,-10.02832 13.26274,-12.18767 9.36902,-4.37217 20.5821,-2.11691 28.41381,5.71481 4.53917,4.53916 6.08619,8.65149 6.08619,16.17843 0,5.98062 -1.45091,10.07363 -7.74542,21.84986 -0.1752,0.32777 1.60835,1.82063 3.96344,3.31747 2.35509,1.49684 7.92899,6.59975 12.38645,11.3398 l 8.10448,8.61827 5.08091,-2.18574 c 7.14194,-3.07238 12.59879,-2.83132 20.03602,0.8851 7.42778,3.71171 11.71188,8.6148 13.81917,15.81585 3.71191,12.68437 -2.31723,25.4609 -14.64505,31.03476 -8.75176,3.95699 -17.82229,2.38307 -24.79643,-4.30269 -6.77865,-6.49836 -9.45593,-12.98504 -9.41305,-22.8065 l 0.0357,-8.16639 -5.73544,-6.07294 c -5.50034,-5.82401 -14.02449,-17.35196 -15.31248,-20.70841 -0.49299,-1.2847 -2.16445,-1.59464 -8.44644,-1.56626 -11.69223,0.0528 -16.94303,-3.2018 -23.06796,-14.29827 l -1.9319,-3.5 -28.91598,-0.52956 c -15.90379,-0.29126 -29.3113,-0.21094 -29.79447,0.17849 -1.36384,1.09923 -0.26429,75.60829 1.13644,77.00902 0.86161,0.86161 2.66433,-0.2593 7.26042,-4.51447 3.36281,-3.11337 7.54687,-6.38805 9.29792,-7.27706 5.82466,-2.95721 5.81795,-2.92335 2.11379,-10.65262 -4.20188,-8.76785 -4.55159,-14.75383 -1.15628,-19.79212 8.1051,-12.02716 26.75899,-11.7396 33.08152,0.50997 6.17921,11.9719 -3.51564,29.01255 -17.25715,30.33294 -4.01647,0.38593 -5.17874,0.91391 -5.71225,2.59485 -0.36839,1.16069 -5.92378,7.51714 -12.3453,14.12545 C 560.09365,562.95388 562,557.35734 562,591.57584 c 0,20.00896 0.34458,29.51909 1.08619,29.97743 0.61485,0.38 10.05291,-8.20174 21.75,-19.7766 11.36509,-11.24634 23.25131,-22.33606 26.41381,-24.64383 l 5.75,-4.19593 v -8.50057 c 0,-4.96989 0.57382,-9.87392 1.38154,-11.80706 2.82655,-6.7649 14.44154,-14.63909 21.5648,-14.61951 8.29107,0.0228 21.23148,9.93895 24.02801,18.4125 3.1841,9.64791 -2.20845,21.80409 -12.21917,27.54516 -5.80127,3.32698 -10.73665,3.83223 -18.12674,1.85568 l -4.40025,-1.17689 -6.3641,5.0641 c -3.50025,2.78526 -18.48289,17.35109 -33.29476,32.36852 l -26.93067,27.30441 -0.37634,89.05837 -0.37634,89.05838 -9.67423,9.75 C 546.89093,842.6125 542.19158,847 541.76876,847 541.34594,847 541,757.76137 541,648.69194 Z m -63.50113,190.53649 c -2.75062,-2.60064 -6.95902,-6.3067 -9.35198,-8.23571 l -4.35086,-3.50727 0.14982,-89.21908 c 0.0926,-55.14436 -0.21535,-89.90142 -0.80633,-91.00568 -1.24066,-2.3182 -53.82724,-52.52888 -61.55054,-58.76965 -5.58979,-4.51679 -6.08275,-4.71427 -9.071,-3.6337 -4.34042,1.56951 -11.05586,1.43299 -14.39279,-0.2926 -3.8332,-1.98223 -9.94713,-8.96045 -12.20271,-13.92773 -2.24091,-4.93499 -2.48331,-13.35898 -0.51965,-18.05869 2.17206,-5.19847 8.26392,-11.28563 13.16908,-13.15893 10.77777,-4.11607 26.18708,2.25414 31.37468,12.97029 1.1572,2.39045 2.37634,7.44319 2.7092,11.2283 0.5314,6.04279 1.10107,7.53412 4.67152,12.2295 2.23648,2.94112 8.81526,9.64415 14.61951,14.89562 5.80425,5.25148 14.88802,13.98261 20.18615,19.40252 C 457.4311,615.56553 462.2686,620 462.88297,620 463.67902,620 464,611.9719 464,592.06155 v -27.93846 l -8.75,-8.85085 c -5.92275,-5.99101 -10.50419,-11.83531 -14.179,-18.08742 -3.88859,-6.6158 -6.08756,-9.3711 -7.75,-9.71069 -1.27655,-0.26077 -3.4726,-0.71438 -4.88011,-1.00802 -3.88953,-0.81144 -11.11661,-8.05012 -13.02422,-13.04513 -3.40012,-8.90308 0.48556,-18.65618 8.96724,-22.50786 5.38185,-2.444 9.00408,-2.3878 14.39808,0.22339 4.72103,2.28541 7.61503,6.07061 9.27279,12.12831 1.5369,5.61604 0.20674,10.2308 -4.67703,16.22614 -2.40776,2.95578 -4.37775,5.60844 -4.37775,5.8948 0,0.28636 2.00774,1.40877 4.46165,2.49425 2.45391,1.08548 7.61109,4.93154 11.46042,8.5468 3.84932,3.61525 7.47168,6.57319 8.04969,6.57319 0.72149,0 0.969,-12.61722 0.78958,-40.25 L 463.5,462.5 h -28.39937 -28.39937 l -1.36946,3.95559 c -2.6009,7.51252 -10.7414,13.35421 -20.95317,15.03617 -5.96024,0.9817 -6.44721,1.30825 -18.5,12.40571 -6.80825,6.26861 -12.95529,12.21489 -13.66009,13.21396 -1.01981,1.44559 -1.0423,3.11449 -0.11015,8.17343 1.55038,8.41422 -0.40917,14.60889 -6.90755,21.83665 -5.44901,6.06062 -10.925,8.85592 -17.20084,8.78045 -6.27559,-0.0755 -10.37497,-1.79007 -14.99111,-6.27017 -8.40721,-8.15945 -11.47708,-21.60111 -7.12276,-31.1876 C 309.29659,500.93568 320.07842,494 328.34025,494 c 1.64118,0 5.37193,0.75274 8.29055,1.67275 l 5.30658,1.67275 8.78131,-8.4465 c 4.82972,-4.64558 10.45303,-9.7147 12.49624,-11.2647 l 3.71494,-2.81818 -3.96494,-7.88586 C 359.64996,460.33711 359,458.05192 359,452.99008 c 0,-7.65892 1.62836,-11.52825 6.82485,-16.21735 5.5914,-5.04544 10.52625,-7.01719 17.67515,-7.06221 10.07428,-0.0634 16.03191,3.33982 24.74021,14.1327 1.5456,1.91557 2.73028,2.15678 10.593,2.15678 C 436.227,446 452,444.77493 452,443.42398 c 0,-0.69776 -9.43381,-10.38688 -20.96402,-21.53136 -11.53021,-11.14449 -21.19561,-20.86621 -21.47866,-21.60384 -0.47507,-1.23801 7.65465,-13.42891 9.3849,-14.07309 0.92958,-0.34609 35.7269,32.97301 53.30778,51.04322 L 485,450.36378 v 196.81811 c 0,156.79933 -0.25416,196.81372 -1.25,196.79654 -0.6875,-0.0119 -3.50051,-2.14937 -6.25113,-4.75 z M 261.5,490.70494 c -5.64227,-2.71043 -12.23352,-9.38327 -15.66165,-15.85553 -3.45558,-6.52409 -3.29656,-18.45953 0.33774,-25.34941 3.43613,-6.51419 6.97256,-9.76279 14.32391,-13.15809 8.5447,-3.94647 18.89509,-3.97733 26.40943,-0.0787 2.90133,1.50526 5.79611,2.71792 6.43286,2.6948 0.63674,-0.0231 13.28522,-11.89945 28.10772,-26.39186 14.82251,-14.4924 27.47406,-26.14873 28.11455,-25.90295 0.6405,0.24578 1.70594,1.65962 2.36765,3.14186 0.66171,1.48225 2.34857,4.36764 3.74858,6.41198 l 2.54547,3.717 -28.11471,27.783 -28.1147,27.78301 0.002,5.66102 c 0.003,11.077 -6.81645,22.71784 -16.70303,28.51175 -6.62656,3.88343 -16.92962,4.33035 -23.79539,1.03217 z m 484.49353,1.36072 c -11.75088,-3.57445 -19.74918,-14.94862 -20.89759,-29.71789 l -0.63696,-8.19161 -4.97949,-4.32808 c -9.06232,-7.87679 -47.29076,-44.89109 -48.89968,-47.34662 -0.8689,-1.3261 -1.57981,-2.86993 -1.57981,-3.43073 0,-1.2914 8.26735,-13.8865 9.17682,-13.98066 0.37224,-0.0385 5.09724,4.686 10.5,10.49897 5.40275,5.81297 17.80006,18.19008 27.54959,27.5047 l 17.72641,16.93567 6.08145,-3.18782 c 5.38838,-2.82451 6.98313,-3.182 13.99331,-3.13686 14.37012,0.0925 24.05391,7.18853 28.13183,20.61417 4.54363,14.95892 -2.10267,29.57586 -16.49513,36.27705 -5.31767,2.47593 -14.21376,3.14965 -19.67075,1.48971 z m 3.14593,-92.15498 C 741.33431,397.52967 735.67727,392.47741 731.3242,384 l -2.8242,-5.5 -23.62243,-0.2678 -23.62242,-0.2678 -0.70416,-8.07228 c -0.38728,-4.43976 -0.43159,-8.34485 -0.0985,-8.67799 0.33313,-0.33313 11.16709,-0.4788 24.07546,-0.3237 l 23.46977,0.28199 2.25112,-2.75967 c 10.68325,-13.09669 22.32445,-17.18523 35.25112,-12.38064 19.55396,7.26783 24.80465,31.88663 9.83473,46.11187 -7.45599,7.08509 -18.09212,10.23862 -26.19527,7.7667 z m -489.82696,-2.494 c -13.56264,-6.5631 -20.73423,-25.22855 -14.41023,-37.50543 5.40147,-10.48597 12.59588,-15.03694 24.83287,-15.70853 7.42296,-0.40739 8.77793,-0.1864 13.73885,2.24075 3.48067,1.70294 6.86741,4.41367 9.2137,7.37462 6.01749,7.5939 4.29402,7.18191 30.04388,7.18191 h 23.34931 l -0.29044,8.25 -0.29044,8.25 -21.5,0.012 c -11.825,0.007 -22.49144,0.11908 -23.70321,0.25 -1.53688,0.16605 -2.78815,1.56041 -4.13732,4.61044 -6.61085,14.94492 -23.12162,21.68611 -36.84697,15.04428 z M 376.5,394.76386 c -8.96301,-3.22145 -15.24281,-10.35486 -17.53788,-19.92178 -5.5653,-23.19874 19.68774,-40.03036 39.76741,-26.50573 12.53525,8.4431 13.79355,27.1932 2.5776,38.40915 -7.71015,7.71016 -17.16329,10.76567 -24.80713,8.01836 z m 260.63946,0.13233 c -7.04241,-2.11796 -15.9756,-9.63452 -18.07813,-15.21129 -1.77805,-4.71613 -1.26658,-16.63593 0.92061,-21.45472 5.62397,-12.39066 20.92681,-17.79941 33.51806,-11.84689 13.4895,6.37717 18.09835,19.50724 11.63957,33.15971 -5.40049,11.41547 -18.04304,18.34771 -28.00011,15.35319 z M 503.28604,303.61342 c -14.16503,-4.64261 -21.33479,-14.04225 -21.55989,-28.26525 -0.13105,-8.27993 4.21611,-17.27027 11.61447,-24.01987 7.08674,-6.4653 13.33253,-8.64722 23.42285,-8.18263 6.96587,0.32073 8.22098,0.70477 12.59983,3.85527 18.08855,13.01439 20.5144,34.1498 5.63345,49.08186 -2.85266,2.86246 -6.43087,5.22863 -9.53193,6.30322 -6.84471,2.37184 -16.97627,2.93253 -22.17878,1.2274 z m 1.26398,-73.34365 C 504.24751,229.48143 504,220.54459 504,210.41012 v -18.42631 l -4.02284,-1.60963 C 491.89504,187.14036 485,175.28456 485,164.62142 c 0,-16.70959 11.45966,-28.65885 27.72498,-28.90949 15.45769,-0.23819 27.00609,10.83101 28.05518,26.89102 0.40439,6.19074 0.10984,7.89206 -2.29352,13.24747 -2.45012,5.4596 -9.91737,14.11935 -12.2143,14.16489 -3.41689,0.0677 -3.77109,1.8586 -4.27234,21.60119 l -0.5,19.69326 -8.19998,0.19667 c -6.33072,0.15184 -8.32535,-0.13007 -8.75,-1.23666 z"
                 id="path1" />
            </g>
          </svg>

        ]]></content>
      </file>
      <file name="codemapper.svg" path="codemapper.svg">
        <content><![CDATA[
          <?xml version="1.0" encoding="UTF-8" standalone="no"?>
          <!-- Created with Inkscape (http://www.inkscape.org/) -->

          <svg
             version="1.1"
             id="svg1"
             width="1024"
             height="1024"
             viewBox="0 0 1024 1024"
             xmlns="http://www.w3.org/2000/svg"
             xmlns:svg="http://www.w3.org/2000/svg">
            <defs
               id="defs1" />
            <g
               id="g1">
              <path
                 style="fill:#ffffff;fill-opacity:1"
                 d="m 508.24988,874.06605 c -2.22761,-1.61367 -4.37305,-3.45632 -4.76763,-4.09477 -0.39459,-0.63845 -0.64085,-106.62135 -0.54726,-235.51755 l 0.17016,-234.35672 -5.68548,-6.29851 c -9.24163,-10.23809 -99.48162,-99.80096 -104.75676,-103.97072 -6.03661,-4.77167 -8.67765,-4.84321 -17.43787,-0.4724 -3.69877,1.84546 -7.97483,3.65712 -9.50235,4.02591 -2.0471,0.49423 -43.34876,3.53811 -48.97269,3.60923 -0.4125,0.005 -0.74834,0.57198 -0.7463,1.25948 0.002,0.6875 2.8215,2.70689 6.26549,4.48754 C 332.08345,307.8118 358,331.82475 358,335.84387 c 0,0.7635 -1.50378,3.47347 -3.34174,6.02215 -1.83796,2.54869 -3.6382,5.4589 -4.00053,6.46714 -0.36233,1.00823 -1.06478,1.68323 -1.561,1.5 -0.49622,-0.18324 -8.25848,-7.48546 -17.24947,-16.22716 -8.99099,-8.7417 -18.06049,-17.27519 -20.15444,-18.96331 -2.09395,-1.68813 -5.38278,-5.66305 -7.30852,-8.83316 -1.92574,-3.17011 -3.92432,-6.02525 -4.44129,-6.34475 -0.51696,-0.3195 -3.33623,1.6854 -6.26502,4.45533 -9.15495,8.65837 -18.94984,11.59632 -28.95969,8.68638 -8.2224,-2.39032 -13.57732,-6.93943 -17.7183,-15.05206 -3.0818,-6.03758 -3.48776,-7.70509 -3.39753,-13.95566 0.12653,-8.76508 2.50915,-14.48811 8.47298,-20.35197 8.68893,-8.54329 21.76868,-10.66359 32.54936,-5.27642 7.17805,3.58691 10.51533,6.91357 14.14177,14.09676 l 2.74279,5.43286 32.99531,0.61276 c 18.14743,0.33702 35.80782,0.96362 39.24532,1.39246 5.17294,0.64533 6.25,0.52767 6.25,-0.68272 C 380,276.91033 341.66064,239 339.72683,239 c -0.81542,0 -3.09808,0.675 -5.07259,1.5 -10.12286,4.2296 -22.82943,-1.72361 -29.3769,-13.7635 -1.28914,-2.37055 -1.76237,-5.53172 -1.7687,-11.81492 -0.008,-7.81107 0.26208,-9.0163 3.0178,-13.47356 4.4581,-7.21081 10.83965,-10.82974 19.87617,-11.27163 6.26135,-0.30619 7.74311,-0.008 12.57911,2.53101 3.16131,1.65979 6.85366,4.67681 8.72278,7.12739 5.15721,6.76155 5.85994,11.03733 4.06824,24.75312 -0.20534,1.57194 3.68114,5.7406 15.72726,16.86912 8.8,8.12967 21.85,20.49565 29,27.47995 C 412.45896,284.52611 421.81112,293 423.057,293 c 1.02628,0 1.35493,-53.69033 0.35913,-58.66933 -0.46055,-2.30279 -1.63042,-3.39944 -5.53944,-5.19274 -11.85503,-5.4386 -17.84952,-23.39052 -12.1258,-36.3136 8.69386,-19.62909 32.3967,-24.06476 46.72713,-8.74437 6.81916,7.29023 9.14307,14.72311 7.32233,23.42004 -1.92395,9.18996 -8.58104,18.92388 -14.66589,21.44431 l -2.84395,1.178 -0.52267,40.68884 -0.52267,40.68885 26.07364,26.5 c 26.32104,26.75145 34.11346,33.90791 35.44968,32.55656 0.40233,-0.40689 0.73151,-12.47172 0.73151,-26.81072 v -26.07092 l 9,-0.0875 9,-0.0875 0.64147,6 c 0.3528,3.3 0.6903,15.5625 0.75,27.25 0.0597,11.6875 0.44603,21.24311 0.85853,21.23469 1.66759,-0.034 4.82039,-3.00711 32.53124,-30.67663 L 585.13233,312.5 l -0.25585,-40 c -0.14071,-22 -0.4357,-40.54182 -0.65552,-41.20405 -0.21982,-0.66223 -2.39346,-2.16923 -4.83031,-3.34889 C 571.96219,224.35101 566,213.47599 566,203.52257 c 0,-24.8827 31.20826,-38.2624 48.30805,-20.71077 13.32422,13.6763 9.53775,37.77652 -7.30805,46.51446 l -4.5,2.33416 -0.2647,30.19553 c -0.2311,26.3626 -0.0702,30.27017 1.26757,30.78353 1.02795,0.39446 5.07314,-2.82312 12.29022,-9.77575 5.91687,-5.70005 21.54646,-20.69637 34.73243,-33.32514 18.28562,-17.51294 23.89243,-23.44515 23.6287,-25 -0.19018,-1.12123 -0.54097,-5.41359 -0.77953,-9.53859 -0.39966,-6.91044 -0.19985,-7.90247 2.54175,-12.61997 5.59842,-9.63327 17.78321,-14.88446 28.81114,-12.41651 13.77566,3.08285 22.74853,18.00714 18.86274,31.37377 -4.16147,14.31498 -22.19492,24.20105 -33.38508,18.30195 l -3.29477,-1.7369 -19.45523,18.70438 C 656.75486,266.89412 648,275.91389 648,276.65066 c 0,1.09641 6.96519,1.33682 38.37177,1.32447 l 38.37178,-0.0151 1.65344,-4.32948 c 5.32277,-13.93747 23.00376,-21.97965 37.63714,-17.11922 6.14923,2.04244 13.13015,8.01679 16.29298,13.94371 1.75188,3.28288 2.15855,5.74055 2.15855,13.04495 0,10.95822 -1.69832,14.75184 -10.00118,22.34019 -11.89583,10.87211 -25.27339,10.42997 -38.2137,-1.26299 -6.31598,-5.70718 -7.55016,-5.53788 -13.08661,1.79509 -2.19891,2.91243 -9.65105,10.61204 -16.56032,17.11024 -6.90927,6.49819 -15.54382,15.34798 -19.18789,19.66618 -3.64406,4.31821 -7.17201,7.85129 -7.83988,7.85129 -1.54621,0 -9.59529,-11.59188 -9.59848,-13.82325 -0.001,-0.92221 8.77369,-10.28559 19.5,-20.8075 C 698.22392,305.84733 707,297.01574 707,296.74349 c 0,-0.27225 -9.3375,-0.72622 -20.75,-1.00881 -24.11671,-0.59717 -30.47892,-1.62928 -38.91248,-6.31256 C 643.94815,287.53995 640.74301,286 640.21497,286 c -1.16301,0 -10.20373,7.17123 -20.17107,16 -4.03604,3.575 -27.52482,27.137 -52.19729,52.35999 l -44.85904,45.85999 0.0125,233.14001 c 0.007,128.22701 0.01,233.9069 0.006,234.8442 -0.006,1.53109 -8.26493,8.79581 -10.00001,8.79581 -0.38839,0 -2.52875,-1.32028 -4.75636,-2.93395 z M 541,648.69194 V 450.38388 l 18.25,-17.98314 C 594.24985,397.91267 606.72403,386 607.83785,386 c 0.61683,0 2.09214,1.9125 3.27848,4.25 1.18634,2.3375 3.26989,5.58074 4.63011,7.20719 l 2.47312,2.9572 -2.85978,2.42471 C 606.59446,410.27092 576,440.84693 576,442.17514 c 0,1.70348 10.00057,2.63066 32.36093,3.00028 9.56239,0.15807 10.98232,-0.0374 11.87633,-1.6349 2.41117,-4.30851 8.63551,-10.02832 13.26274,-12.18767 9.36902,-4.37217 20.5821,-2.11691 28.41381,5.71481 4.53917,4.53916 6.08619,8.65149 6.08619,16.17843 0,5.98062 -1.45091,10.07363 -7.74542,21.84986 -0.1752,0.32777 1.60835,1.82063 3.96344,3.31747 2.35509,1.49684 7.92899,6.59975 12.38645,11.3398 l 8.10448,8.61827 5.08091,-2.18574 c 7.14194,-3.07238 12.59879,-2.83132 20.03602,0.8851 7.42778,3.71171 11.71188,8.6148 13.81917,15.81585 3.71191,12.68437 -2.31723,25.4609 -14.64505,31.03476 -8.75176,3.95699 -17.82229,2.38307 -24.79643,-4.30269 -6.77865,-6.49836 -9.45593,-12.98504 -9.41305,-22.8065 l 0.0357,-8.16639 -5.73544,-6.07294 c -5.50034,-5.82401 -14.02449,-17.35196 -15.31248,-20.70841 -0.49299,-1.2847 -2.16445,-1.59464 -8.44644,-1.56626 -11.69223,0.0528 -16.94303,-3.2018 -23.06796,-14.29827 l -1.9319,-3.5 -28.91598,-0.52956 c -15.90379,-0.29126 -29.3113,-0.21094 -29.79447,0.17849 -1.36384,1.09923 -0.26429,75.60829 1.13644,77.00902 0.86161,0.86161 2.66433,-0.2593 7.26042,-4.51447 3.36281,-3.11337 7.54687,-6.38805 9.29792,-7.27706 5.82466,-2.95721 5.81795,-2.92335 2.11379,-10.65262 -4.20188,-8.76785 -4.55159,-14.75383 -1.15628,-19.79212 8.1051,-12.02716 26.75899,-11.7396 33.08152,0.50997 6.17921,11.9719 -3.51564,29.01255 -17.25715,30.33294 -4.01647,0.38593 -5.17874,0.91391 -5.71225,2.59485 -0.36839,1.16069 -5.92378,7.51714 -12.3453,14.12545 C 560.09365,562.95388 562,557.35734 562,591.57584 c 0,20.00896 0.34458,29.51909 1.08619,29.97743 0.61485,0.38 10.05291,-8.20174 21.75,-19.7766 11.36509,-11.24634 23.25131,-22.33606 26.41381,-24.64383 l 5.75,-4.19593 v -8.50057 c 0,-4.96989 0.57382,-9.87392 1.38154,-11.80706 2.82655,-6.7649 14.44154,-14.63909 21.5648,-14.61951 8.29107,0.0228 21.23148,9.93895 24.02801,18.4125 3.1841,9.64791 -2.20845,21.80409 -12.21917,27.54516 -5.80127,3.32698 -10.73665,3.83223 -18.12674,1.85568 l -4.40025,-1.17689 -6.3641,5.0641 c -3.50025,2.78526 -18.48289,17.35109 -33.29476,32.36852 l -26.93067,27.30441 -0.37634,89.05837 -0.37634,89.05838 -9.67423,9.75 C 546.89093,842.6125 542.19158,847 541.76876,847 541.34594,847 541,757.76137 541,648.69194 Z m -63.50113,190.53649 c -2.75062,-2.60064 -6.95902,-6.3067 -9.35198,-8.23571 l -4.35086,-3.50727 0.14982,-89.21908 c 0.0926,-55.14436 -0.21535,-89.90142 -0.80633,-91.00568 -1.24066,-2.3182 -53.82724,-52.52888 -61.55054,-58.76965 -5.58979,-4.51679 -6.08275,-4.71427 -9.071,-3.6337 -4.34042,1.56951 -11.05586,1.43299 -14.39279,-0.2926 -3.8332,-1.98223 -9.94713,-8.96045 -12.20271,-13.92773 -2.24091,-4.93499 -2.48331,-13.35898 -0.51965,-18.05869 2.17206,-5.19847 8.26392,-11.28563 13.16908,-13.15893 10.77777,-4.11607 26.18708,2.25414 31.37468,12.97029 1.1572,2.39045 2.37634,7.44319 2.7092,11.2283 0.5314,6.04279 1.10107,7.53412 4.67152,12.2295 2.23648,2.94112 8.81526,9.64415 14.61951,14.89562 5.80425,5.25148 14.88802,13.98261 20.18615,19.40252 C 457.4311,615.56553 462.2686,620 462.88297,620 463.67902,620 464,611.9719 464,592.06155 v -27.93846 l -8.75,-8.85085 c -5.92275,-5.99101 -10.50419,-11.83531 -14.179,-18.08742 -3.88859,-6.6158 -6.08756,-9.3711 -7.75,-9.71069 -1.27655,-0.26077 -3.4726,-0.71438 -4.88011,-1.00802 -3.88953,-0.81144 -11.11661,-8.05012 -13.02422,-13.04513 -3.40012,-8.90308 0.48556,-18.65618 8.96724,-22.50786 5.38185,-2.444 9.00408,-2.3878 14.39808,0.22339 4.72103,2.28541 7.61503,6.07061 9.27279,12.12831 1.5369,5.61604 0.20674,10.2308 -4.67703,16.22614 -2.40776,2.95578 -4.37775,5.60844 -4.37775,5.8948 0,0.28636 2.00774,1.40877 4.46165,2.49425 2.45391,1.08548 7.61109,4.93154 11.46042,8.5468 3.84932,3.61525 7.47168,6.57319 8.04969,6.57319 0.72149,0 0.969,-12.61722 0.78958,-40.25 L 463.5,462.5 h -28.39937 -28.39937 l -1.36946,3.95559 c -2.6009,7.51252 -10.7414,13.35421 -20.95317,15.03617 -5.96024,0.9817 -6.44721,1.30825 -18.5,12.40571 -6.80825,6.26861 -12.95529,12.21489 -13.66009,13.21396 -1.01981,1.44559 -1.0423,3.11449 -0.11015,8.17343 1.55038,8.41422 -0.40917,14.60889 -6.90755,21.83665 -5.44901,6.06062 -10.925,8.85592 -17.20084,8.78045 -6.27559,-0.0755 -10.37497,-1.79007 -14.99111,-6.27017 -8.40721,-8.15945 -11.47708,-21.60111 -7.12276,-31.1876 C 309.29659,500.93568 320.07842,494 328.34025,494 c 1.64118,0 5.37193,0.75274 8.29055,1.67275 l 5.30658,1.67275 8.78131,-8.4465 c 4.82972,-4.64558 10.45303,-9.7147 12.49624,-11.2647 l 3.71494,-2.81818 -3.96494,-7.88586 C 359.64996,460.33711 359,458.05192 359,452.99008 c 0,-7.65892 1.62836,-11.52825 6.82485,-16.21735 5.5914,-5.04544 10.52625,-7.01719 17.67515,-7.06221 10.07428,-0.0634 16.03191,3.33982 24.74021,14.1327 1.5456,1.91557 2.73028,2.15678 10.593,2.15678 C 436.227,446 452,444.77493 452,443.42398 c 0,-0.69776 -9.43381,-10.38688 -20.96402,-21.53136 -11.53021,-11.14449 -21.19561,-20.86621 -21.47866,-21.60384 -0.47507,-1.23801 7.65465,-13.42891 9.3849,-14.07309 0.92958,-0.34609 35.7269,32.97301 53.30778,51.04322 L 485,450.36378 v 196.81811 c 0,156.79933 -0.25416,196.81372 -1.25,196.79654 -0.6875,-0.0119 -3.50051,-2.14937 -6.25113,-4.75 z M 261.5,490.70494 c -5.64227,-2.71043 -12.23352,-9.38327 -15.66165,-15.85553 -3.45558,-6.52409 -3.29656,-18.45953 0.33774,-25.34941 3.43613,-6.51419 6.97256,-9.76279 14.32391,-13.15809 8.5447,-3.94647 18.89509,-3.97733 26.40943,-0.0787 2.90133,1.50526 5.79611,2.71792 6.43286,2.6948 0.63674,-0.0231 13.28522,-11.89945 28.10772,-26.39186 14.82251,-14.4924 27.47406,-26.14873 28.11455,-25.90295 0.6405,0.24578 1.70594,1.65962 2.36765,3.14186 0.66171,1.48225 2.34857,4.36764 3.74858,6.41198 l 2.54547,3.717 -28.11471,27.783 -28.1147,27.78301 0.002,5.66102 c 0.003,11.077 -6.81645,22.71784 -16.70303,28.51175 -6.62656,3.88343 -16.92962,4.33035 -23.79539,1.03217 z m 484.49353,1.36072 c -11.75088,-3.57445 -19.74918,-14.94862 -20.89759,-29.71789 l -0.63696,-8.19161 -4.97949,-4.32808 c -9.06232,-7.87679 -47.29076,-44.89109 -48.89968,-47.34662 -0.8689,-1.3261 -1.57981,-2.86993 -1.57981,-3.43073 0,-1.2914 8.26735,-13.8865 9.17682,-13.98066 0.37224,-0.0385 5.09724,4.686 10.5,10.49897 5.40275,5.81297 17.80006,18.19008 27.54959,27.5047 l 17.72641,16.93567 6.08145,-3.18782 c 5.38838,-2.82451 6.98313,-3.182 13.99331,-3.13686 14.37012,0.0925 24.05391,7.18853 28.13183,20.61417 4.54363,14.95892 -2.10267,29.57586 -16.49513,36.27705 -5.31767,2.47593 -14.21376,3.14965 -19.67075,1.48971 z m 3.14593,-92.15498 C 741.33431,397.52967 735.67727,392.47741 731.3242,384 l -2.8242,-5.5 -23.62243,-0.2678 -23.62242,-0.2678 -0.70416,-8.07228 c -0.38728,-4.43976 -0.43159,-8.34485 -0.0985,-8.67799 0.33313,-0.33313 11.16709,-0.4788 24.07546,-0.3237 l 23.46977,0.28199 2.25112,-2.75967 c 10.68325,-13.09669 22.32445,-17.18523 35.25112,-12.38064 19.55396,7.26783 24.80465,31.88663 9.83473,46.11187 -7.45599,7.08509 -18.09212,10.23862 -26.19527,7.7667 z m -489.82696,-2.494 c -13.56264,-6.5631 -20.73423,-25.22855 -14.41023,-37.50543 5.40147,-10.48597 12.59588,-15.03694 24.83287,-15.70853 7.42296,-0.40739 8.77793,-0.1864 13.73885,2.24075 3.48067,1.70294 6.86741,4.41367 9.2137,7.37462 6.01749,7.5939 4.29402,7.18191 30.04388,7.18191 h 23.34931 l -0.29044,8.25 -0.29044,8.25 -21.5,0.012 c -11.825,0.007 -22.49144,0.11908 -23.70321,0.25 -1.53688,0.16605 -2.78815,1.56041 -4.13732,4.61044 -6.61085,14.94492 -23.12162,21.68611 -36.84697,15.04428 z M 376.5,394.76386 c -8.96301,-3.22145 -15.24281,-10.35486 -17.53788,-19.92178 -5.5653,-23.19874 19.68774,-40.03036 39.76741,-26.50573 12.53525,8.4431 13.79355,27.1932 2.5776,38.40915 -7.71015,7.71016 -17.16329,10.76567 -24.80713,8.01836 z m 260.63946,0.13233 c -7.04241,-2.11796 -15.9756,-9.63452 -18.07813,-15.21129 -1.77805,-4.71613 -1.26658,-16.63593 0.92061,-21.45472 5.62397,-12.39066 20.92681,-17.79941 33.51806,-11.84689 13.4895,6.37717 18.09835,19.50724 11.63957,33.15971 -5.40049,11.41547 -18.04304,18.34771 -28.00011,15.35319 z M 503.28604,303.61342 c -14.16503,-4.64261 -21.33479,-14.04225 -21.55989,-28.26525 -0.13105,-8.27993 4.21611,-17.27027 11.61447,-24.01987 7.08674,-6.4653 13.33253,-8.64722 23.42285,-8.18263 6.96587,0.32073 8.22098,0.70477 12.59983,3.85527 18.08855,13.01439 20.5144,34.1498 5.63345,49.08186 -2.85266,2.86246 -6.43087,5.22863 -9.53193,6.30322 -6.84471,2.37184 -16.97627,2.93253 -22.17878,1.2274 z m 1.26398,-73.34365 C 504.24751,229.48143 504,220.54459 504,210.41012 v -18.42631 l -4.02284,-1.60963 C 491.89504,187.14036 485,175.28456 485,164.62142 c 0,-16.70959 11.45966,-28.65885 27.72498,-28.90949 15.45769,-0.23819 27.00609,10.83101 28.05518,26.89102 0.40439,6.19074 0.10984,7.89206 -2.29352,13.24747 -2.45012,5.4596 -9.91737,14.11935 -12.2143,14.16489 -3.41689,0.0677 -3.77109,1.8586 -4.27234,21.60119 l -0.5,19.69326 -8.19998,0.19667 c -6.33072,0.15184 -8.32535,-0.13007 -8.75,-1.23666 z"
                 id="path1" />
            </g>
          </svg>

        ]]></content>
      </file>
      <directory name="docs" path="docs">
        <directory name="briefs" path="docs/briefs">
          <file name="brief-code-restructure.md" path="docs/briefs/brief-code-restructure.md">
            <content><![CDATA[
              # CodeMapper Code Restructure Implementation Brief

              > [!TIP]
              > Advice from Claude on restructuring our CodeMapper codebase for improved modularity, maintainability, and adherence to the single responsibility principle.

              [[toc]]

              ## Overview

              This document outlines a comprehensive plan to restructure the CodeMapper codebase to improve modularity, maintainability, and adherence to the single responsibility principle. The restructuring aims to better align the code organization with its documentation while maintaining all existing functionality.

              ## Current Issues

              ### 1. Module Organization

              - Utility functions are scattered across different modules
              - Mixed responsibilities within modules
              - Inconsistent error handling approaches
              - Lack of clear separation between core functionality and utilities

              ### 2. Documentation-Code Alignment

              - Documentation suggests a simpler structure than actual implementation
              - Missing clear boundaries between components
              - Inconsistent error handling patterns

              ### 3. Testing Challenges

              - Current structure makes unit testing difficult
              - Tightly coupled components
              - Mixed responsibilities complicate test isolation

              ## Proposed Directory Structure

              ```tree
              src/codemapper/
              â”œâ”€â”€ __init__.py
              â”œâ”€â”€ main.py                      # CLI interface and orchestration
              â”œâ”€â”€ config.py                    # Global configuration and constants
              â”œâ”€â”€ exceptions.py                # Custom exceptions
              â”œâ”€â”€ core/
              â”‚   â”œâ”€â”€ __init__.py
              â”‚   â”œâ”€â”€ processor.py            # Core mapping logic
              â”‚   â””â”€â”€ docmap.py              # Documentation mapping functionality
              â”œâ”€â”€ utils/
              â”‚   â”œâ”€â”€ __init__.py
              â”‚   â”œâ”€â”€ fs.py                  # File system operations
              â”‚   â”œâ”€â”€ git.py                 # Git operations
              â”‚   â”œâ”€â”€ logging.py            # Logging configuration and utilities
              â”‚   â”œâ”€â”€ markdown.py           # Markdown generation utilities
              â”‚   â””â”€â”€ path.py               # Path handling and validation
              â””â”€â”€ constants/
                  â”œâ”€â”€ __init__.py
                  â”œâ”€â”€ file_types.py         # File type definitions and mappings
                  â””â”€â”€ messages.py           # Error and log messages
              ```

              ## Module Responsibilities

              ### 1. Core Components

              #### main.py

              ```python
              """CLI interface and orchestration for CodeMapper."""

              def parse_arguments() -> argparse.Namespace:
                  """Handle CLI argument parsing."""

              def main() -> None:
                  """Main orchestration function."""
              ```

              #### core/processor.py

              ```python
              """Core mapping logic for CodeMapper."""

              class CodeMapper:
                  """Main class for handling codebase mapping operations."""

                  def process_directory(self) -> str:
                      """Process directory and generate markdown."""

                  def process_github_repo(self) -> str:
                      """Process GitHub repository and generate markdown."""
              ```

              #### core/docmap.py

              ```python
              """Documentation mapping functionality."""

              class DocMapper:
                  """Documentation mapping and processing."""

                  def generate_doc_map(self) -> str:
                      """Generate documentation map."""
              ```

              ### 2. Utility Modules

              #### utils/fs.py

              ```python
              """File system utilities."""

              def collect_file_paths(directory: str) -> List[str]:
                  """Collect file paths respecting gitignore."""

              def read_file_content(path: str) -> str:
                  """Read and process file content."""
              ```

              #### utils/git.py

              ```python
              """Git operations utilities."""

              def clone_repository(url: str) -> str:
                  """Clone and manage GitHub repositories."""

              def parse_gitignore(path: str) -> pathspec.PathSpec:
                  """Parse gitignore specifications."""
              ```

              #### utils/markdown.py

              ```python
              """Markdown generation utilities."""

              def generate_toc(paths: List[str]) -> str:
                  """Generate table of contents."""

              def format_code_block(content: str, language: str) -> str:
                  """Format code blocks with proper syntax highlighting."""
              ```

              ## Error Handling Strategy

              ### 1. Custom Exceptions

              ```python
              # exceptions.py

              class CodeMapperError(Exception):
                  """Base exception for CodeMapper."""

              class GitOperationError(CodeMapperError):
                  """Git operation related errors."""

              class FileSystemError(CodeMapperError):
                  """File system operation errors."""

              class ValidationError(CodeMapperError):
                  """Input validation errors."""
              ```

              ### 2. Error Handling Pattern

              ```python
              try:
                  result = operation()
              except SpecificError as e:
                  logger.error("Operation failed: %s", str(e))
                  raise CodeMapperError(f"Operation failed: {str(e)}") from e
              ```

              ## Implementation Plan

              ### Phase 1: Core Restructuring

              1. Create new directory structure
              2. Move existing code to appropriate modules
              3. Update imports and dependencies
              4. Implement custom exceptions

              ### Phase 2: Functionality Migration

              1. Migrate file system operations
              2. Migrate git operations
              3. Migrate markdown generation
              4. Update main orchestration logic

              ### Phase 3: Testing and Documentation

              1. Update test structure to match new organization
              2. Add unit tests for new modules
              3. Update documentation to reflect new structure
              4. Add inline documentation for new components

              ### Phase 4: Error Handling

              1. Implement custom exceptions
              2. Update error handling throughout codebase
              3. Standardize logging approach
              4. Add error recovery mechanisms

              ## Testing Strategy

              ### 1. Unit Tests

              ```python
              # tests/test_fs.py
              def test_collect_file_paths():
                  """Test file path collection."""

              # tests/test_git.py
              def test_clone_repository():
                  """Test repository cloning."""
              ```

              ### 2. Integration Tests

              ```python
              # tests/integration/test_processor.py
              def test_complete_mapping_flow():
                  """Test complete mapping process."""
              ```

              ## Logging Strategy

              ### 1. Configuration

              ```python
              # utils/logging.py
              def configure_logging() -> None:
                  """Configure logging with appropriate formatters and handlers."""
              ```

              ### 2. Usage Pattern

              ```python
              logger = logging.getLogger(__name__)
              logger.info("Processing directory: %s", directory_path)
              ```

              ## Migration Guide

              1. **For Contributors:**
                 - Overview of new structure
                 - Guidelines for moving code
                 - Testing requirements
                 - Documentation requirements

              2. **For Users:**
                 - No breaking changes in public API
                 - Improved error messages
                 - Better logging for debugging
                 - Enhanced documentation

              ## Success Metrics

              1. **Code Quality:**
                 - Improved pylint scores
                 - Reduced cyclomatic complexity
                 - Better test coverage

              2. **Maintenance:**
                 - Reduced time to implement new features
                 - Faster bug resolution
                 - Clearer error messages

              3. **User Experience:**
                 - More consistent error handling
                 - Better debugging information
                 - Improved documentation

              ## Timeline

              1. Phase 1: 2-3 days
              2. Phase 2: 3-4 days
              3. Phase 3: 2-3 days
              4. Phase 4: 2-3 days

              Total estimated time: 9-13 days

              ## Risks and Mitigations

              1. **Regression Risks:**
                 - Comprehensive test suite before and after
                 - Gradual migration with verification
                 - User beta testing period

              2. **Performance Impact:**
                 - Benchmark key operations
                 - Profile new structure
                 - Optimize critical paths

              3. **Migration Challenges:**
                 - Clear documentation
                 - Staged rollout
                 - Revert plan if needed

              ## Future Considerations

              1. **Extensibility:**
                 - Plugin system
                 - Custom formatters
                 - Additional output formats

              2. **Performance:**
                 - Async file operations
                 - Parallel processing
                 - Caching mechanisms

              3. **Integration:**
                 - IDE plugins
                 - CI/CD integration
                 - API endpoints

              ## Success Criteria

              1. All existing functionality maintained
              2. Improved test coverage (target: 90%+)
              3. Clear separation of concerns
              4. Consistent error handling
              5. Comprehensive documentation
              6. Maintainable codebase

            ]]></content>
          </file>
          <file name="brief-docmap-evaluation.md" path="docs/briefs/brief-docmap-evaluation.md">
            <content><![CDATA[
              # CodeMapper DocMap Feature Implementation Evaluation

              [[toc]]

              ## Overview

              This document evaluates the current implementation of CodeMapper's DocMap feature against the requirements specified in the Feature Implementation Brief.

              ## Requirements Analysis

              ### 1. Command Line Interface âœ…

              The implementation successfully meets CLI requirements:

              - Implemented required `--docs` flag in `main.py`
              - Implemented optional `--docs-dir` flag for custom directories
              - Usage matches specification:

                ```bash
                codemapper --docs repo-url           # Generate docmap with standard doc directories
                codemapper --docs --docs-dir=wiki    # Generate docmap with custom doc directory
                ```

              ### 2. Documentation Content Scope

              #### a) Main README.md File âœ…

              - Successfully processes root README.md at beginning of docmap
              - Correctly excludes READMEs from subdirectories
              - Implementation properly handles README content formatting

              #### b) Documentation Directory Content âœ…

              Standard directory search implemented in `config.py`:

              ```python
              DOC_DIRECTORIES = {
                  "docs",
                  "wiki",
                  "documentation"
              }
              ```

              Key features implemented:

              - Uses first matching directory found
              - Processes all files within documentation directory
              - Maintains proper search order

              ### 3. Output Format âœ…

              Implementation meets all format requirements:

              - Generates `repository-name_docmap.md` in _codemaps directory
              - Follows same formatting rules as codemap
              - Respects .gitignore rules
              - Handles large files appropriately
              - Maintains consistent markdown formatting

              ### 4. Technical Implementation âœ…

              File structure matches specification:

              ```tree
              src/codemapper/
              â”œâ”€â”€ __init__.py
              â”œâ”€â”€ config.py      # Added doc directory constants
              â”œâ”€â”€ main.py        # Added new CLI options
              â”œâ”€â”€ utils.py       # Updated for suffix handling
              â””â”€â”€ docmap.py      # New file for doc mapping
              ```

              ## Identified Gaps and Areas for Improvement

              ### 1. Configuration and Documentation

              - **Enhancement Needed**: Config file could benefit from additional doc directory patterns
              - **Recommended Addition**: Support for more documentation-specific file types
              - **Future Consideration**: Implementation of comprehensive documentation pattern matching

              ### 2. Error Handling

              Current gaps in error handling:

              - Basic error handling exists but could be more comprehensive
              - Need more informative messages for missing documentation directories
              - Could improve graceful fallbacks per the brief

              Recommended improvements:

              ```python
              def find_documentation_directory(base_path: str, custom_dir: Optional[str] = None) -> Optional[str]:
                  """
                  Enhanced version should include:
                  - More detailed error messages
                  - Logging of attempted paths
                  - Suggestions for common misconfigurations
                  """
              ```

              ### 3. Content Processing

              Areas for enhancement:

              - Documentation-specific markdown formatting
              - Special handling for documentation-specific file types
              - Enhanced metadata extraction from documentation files

              ### 4. Testing Coverage

              Testing gaps to address:

              - Need more comprehensive testing with various repository structures
              - Lack of test cases for error handling scenarios
              - Limited testing of custom documentation directories

              ## Recommended Next Steps

              ### Priority 1: Error Handling Enhancements

              1. Implement comprehensive error messaging
              2. Add graceful fallbacks for missing documentation
              3. Improve user feedback for configuration issues

              ### Priority 2: Testing Expansion

              1. Add test cases for various repository structures
              2. Implement error handling test scenarios
              3. Create tests for custom documentation directories

              ### Priority 3: Documentation Processing

              1. Enhance markdown formatting for documentation files
              2. Add support for additional documentation file types
              3. Implement metadata extraction

              ### Priority 4: Configuration Expansion

              1. Add more documentation patterns
              2. Implement pattern matching configuration
              3. Add documentation-specific processing options

              ## Implementation Plan Template

              For each enhancement:

              ```python
              """
              Enhancement: [Name]
              Priority: [1-4]
              Complexity: [Low/Medium/High]
              Dependencies: [List any dependencies]

              Implementation Steps:
              1. [Step 1]
              2. [Step 2]
              3. [Step 3]

              Testing Requirements:
              - [Test 1]
              - [Test 2]
              - [Test 3]

              Documentation Updates:
              - [Doc Update 1]
              - [Doc Update 2]
              """
              ```

              ## Conclusion

              While the core requirements of the DocMap feature have been successfully implemented, there are several areas where the implementation could be enhanced to provide a more robust and user-friendly experience. The recommended next steps would significantly improve the feature's utility and reliability.

            ]]></content>
          </file>
          <file name="brief-docmap-feature.md" path="docs/briefs/brief-docmap-feature.md">
            <content><![CDATA[
              # CodeMapper DocMap Feature Implementation Brief

              [[toc]]

              ## Feature Overview

              Add documentation-specific mapping capability to CodeMapper, generating a separate `*_docmap.md` file that focuses on repository documentation.

              ## Core Requirements

              ### Command Line Interface

              - Add new flag `--docs` to generate documentation-specific mapping
              - Add optional flag `--docs-dir=<custom-path>` to support non-standard documentation directory names
              - Usage examples:

                ```bash
                codemapper --docs repo-url           # Generate docmap with standard doc directories
                codemapper --docs --docs-dir=wiki    # Generate docmap with custom doc directory
                ```

              ### Documentation Content Scope

              1. Main README.md file
                 - Include root README.md content at the beginning of docmap
                 - Do not include READMEs from subdirectories

              2. Documentation Directory Content
                 - Search for standard documentation directories in this order:
                   - /docs
                   - /doc
                   - /documentation
                 - Use first matching directory found
                 - Process all files within the found documentation directory

              ### Output Format

              - Generate `repository-name_docmap.md` in the _codemaps directory
              - Follow same format and processing rules as codemap:
                - Respect .gitignore rules
                - Handle large files appropriately
                - Apply same markdown formatting

              ## Technical Implementation

              ### File Structure

              ```tree
              src/codemapper/
              â”œâ”€â”€ __init__.py
              â”œâ”€â”€ config.py      # Add doc directory constants
              â”œâ”€â”€ main.py        # Add new CLI options
              â”œâ”€â”€ utils.py       # Update for suffix handling
              â””â”€â”€ docmap.py      # New file for doc mapping
              ```

              ### New Configuration (config.py)

              Add constant for standard documentation directories:

              ```python
              DOC_DIRECTORIES = {
                  "docs",
                  "doc",
                  "documentation"
              }
              ```

              ### New Module (docmap.py)

              Create new module with functions:

              - `find_documentation_directory()`: Locate documentation directory
              - `process_readme()`: Handle README.md processing
              - `generate_docmap_content()`: Generate documentation map content

              ### Required Updates to Existing Files

              1. main.py:
                 - Add --docs and --docs-dir arguments
                 - Integrate docmap generation logic
                 - Update output file handling for different suffixes

              2. utils.py:
                 - Update manage_output_directory() to handle different output suffixes
                 - Any shared utility functions needed by docmap.py

              ### DocMap Content Structure

              ```markdown
              # {repository_name} Documentation

              > DocMap Source: {source}

              This markdown document provides a comprehensive overview of the documentation
              files and structure. It aims to give viewers (human or AI) a complete view
              of the project's documentation in a single file for easy analysis.

              ## Project README

              {README.md content}

              ## Documentation Directory: {directory_name}

              ### Directory Structure
              {tree structure of documentation directory}

              ### Documentation Contents
              {Contents of all documentation files}

              > This concludes the documentation mapping. Please review thoroughly for a
              > comprehensive understanding of the project's documentation.
              ```

              ### Error Handling

              1. No Documentation Directory:
                 - If README exists: Generate docmap with just README
                 - If no README: Display informative message and exit gracefully

              2. Invalid Custom Directory:
                 - Display error message if --docs-dir path doesn't exist
                 - Exit with appropriate error code

              ### Future Extensibility Considerations

              - Config file could be extended to include additional doc directory patterns
              - Additional documentation-specific processing could be added
              - Support for documentation-specific file types could be enhanced

              ## Implementation Notes

              1. Code Organization:
                 - Separate docmap.py module for better maintainability
                 - Keep files at reasonable sizes for easier management
                 - Follow existing codebase patterns and conventions

              2. Processing Rules:
                 - Use same file handling logic as existing codemap
                 - Maintain consistent markdown formatting
                 - Follow same .gitignore rules

              3. Documentation Standards:
                 - Maintain Google-style docstrings
                 - Include type hints
                 - Add comprehensive module-level documentation

              4. Testing:
                 - Add tests for new functionality in existing test files
                 - Test with various repository structures
                 - Verify error handling

              ## Usage Examples

              ```bash
              # Basic documentation mapping
              codemapper --docs https://github.com/user/repo

              # Custom documentation directory
              codemapper --docs --docs-dir=wiki https://github.com/user/repo

              # Include ignored files
              codemapper --docs --include-ignored https://github.com/user/repo
              ```

              ## Implementation Order

              1. Add DOC_DIRECTORIES to config.py
              2. Create new docmap.py module
              3. Update utils.py for output suffix handling
              4. Modify main.py for new CLI options
              5. Add test cases
              6. Update documentation

              This feature should be implemented following existing CodeMapper patterns and practices, maintaining the same level of code quality and documentation standards.

            ]]></content>
          </file>
        </directory>
        <directory name="builds" path="docs/builds">
          <file name="pypi.md" path="docs/builds/pypi.md">
            <content><![CDATA[
              # Publishing CodeMapper to PyPI

              Instructions to prepare and publish the CodeMapper project to PyPI using `pyproject.toml`.

              ## Table of Contents

              - [Publishing CodeMapper to PyPI](#publishing-codemapper-to-pypi)
                - [Table of Contents](#table-of-contents)
                - [Local Testing](#local-testing)
                - [Testing from Source Directory](#testing-from-source-directory)
                - [Building the Package](#building-the-package)
                - [Troubleshooting](#troubleshooting)

              ## Local Testing

              To test the package locally before publishing to PyPI:

              1. Create a virtual environment:

                  ```sh
                  python -m venv venv
                  source venv/bin/activate  # On Windows, use `venv\Scripts\activate`
                  ```

              2. Install the package locally:

                 > [!IMPORTANT]
                 > This will install the package in editable mode, allowing you to make changes to the source code and test them immediately. That also means, if you change the source code and want to test those changes from the source, you will need to re-run this command!

                  ```sh
                  pip install -e .
                  ```

              3. Test the package:

                  ```sh
                  codemapper /path/to/test/directory_or_GITHUB-REPO-URL
                  ```

              ## Testing from Source Directory

              To test the `codemapper` directly from the source directory:

              1. Set the `PYTHONPATH` to the `src` directory and run the `codemapper` module:

                  ```sh
                  PYTHONPATH=src python -m codemapper.main <path_to_directory_or_github_url>
                  ```

              2. Example to run `codemapper` in the current directory:

                  ```sh
                  set PYTHONPATH=src && python -m codemapper.main .
                  ```

              ## Building the Package

              1. Install build tools:

                  ```sh
                  pip install --upgrade setuptools wheel build twine
                  ```

              2. Build the package:

                  ```sh
                  # before doing ensure
                  # 1. update version in pyproject.toml
                  # 2. update changelog.md
                  python -m build
                  ```

                  This will generate distribution files in the `dist` directory.

              3. Upload the package to PyPI:

                  ```sh
                  python -m twine upload dist/*
                  ```

              4. Update the package:

                  ```sh
                  pip install --upgrade codemapper
                  ```

              ## Troubleshooting

              - **FileNotFoundError: [Errno 2] No such file or directory: 'README.md'**:
                - Ensure `README.md` is in the same directory as `pyproject.toml`.
                - Check your current working directory.

              - **HTTPError: 400 Client Error: File already exists**:
                - You cannot overwrite an existing version on PyPI.
                - Increment the version number in `pyproject.toml` and try again.

              - **ImportError: No module named 'codemapper'**:
                - Ensure the package is installed correctly: `pip install -e .`
                - Check if you're in the correct virtual environment.

              - **Permission denied when uploading to PyPI**:
                - Verify your PyPI credentials.
                - Ensure you have the necessary permissions to upload the package.

              - **Package not found on PyPI after upload**:
                - Wait a few minutes, as it can take some time for the package to appear.
                - Double-check the package name and version on PyPI.

              Remember to always test your package locally in a fresh virtual environment before uploading to PyPI. This ensures that your package works as expected when installed by other users.

            ]]></content>
          </file>
        </directory>
        <file name="codemapper-summary.md" path="docs/codemapper-summary.md">
          <content><![CDATA[
            # CodeMapper Summary

            CodeMapper is a powerful Python tool that transforms complex codebases into navigable, single-file Markdown artifacts, with a unique ability to bootstrap AI chat prompts for code analysis. Designed with both AI engineers and hobbyist developers in mind, it serves as a bridge between traditional code exploration and modern AI-assisted development workflows. Whether you're training large language models, conducting interactive AI-assisted code reviews, or simply trying to understand a new project, CodeMapper provides a unified, accessible view of your codebase that's optimized for both human readability and AI consumption.

          ]]></content>
        </file>
        <directory name="standards" path="docs/standards">
          <file name="pylintrc.md" path="docs/standards/pylintrc.md">
            <content><![CDATA[
              # PyLintRC File

              ```conf
              [MAIN]

              ### this regex can strip comments via vscode editor: ^\s*#.*$

              # Analyse import fallback blocks. This can be used to support both Python 2 and
              # 3 compatible code, which means that the block might have code that exists
              # only in one or another interpreter, leading to false positives when analysed.
              analyse-fallback-blocks=no

              # Clear in-memory caches upon conclusion of linting. Useful if running pylint
              # in a server-like mode.
              clear-cache-post-run=no

              # Load and enable all available extensions. Use --list-extensions to see a list
              # all available extensions.
              #enable-all-extensions=

              # In error mode, messages with a category besides ERROR or FATAL are
              # suppressed, and no reports are done by default. Error mode is compatible with
              # disabling specific errors.
              #errors-only=

              # Always return a 0 (non-error) status code, even if lint errors are found.
              # This is primarily useful in continuous integration scripts.
              #exit-zero=

              # A comma-separated list of package or module names from where C extensions may
              # be loaded. Extensions are loading into the active Python interpreter and may
              # run arbitrary code.
              extension-pkg-allow-list=

              # A comma-separated list of package or module names from where C extensions may
              # be loaded. Extensions are loading into the active Python interpreter and may
              # run arbitrary code. (This is an alternative name to extension-pkg-allow-list
              # for backward compatibility.)
              extension-pkg-whitelist=

              # Return non-zero exit code if any of these messages/categories are detected,
              # even if score is above --fail-under value. Syntax same as enable. Messages
              # specified are enabled, while categories only check already-enabled messages.
              fail-on=

              # Specify a score threshold under which the program will exit with error.
              fail-under=10

              # Interpret the stdin as a python script, whose filename needs to be passed as
              # the module_or_package argument.
              #from-stdin=

              # Files or directories to be skipped. They should be base names, not paths.
              ignore=CVS

              # Add files or directories matching the regular expressions patterns to the
              # ignore-list. The regex matches against paths and can be in Posix or Windows
              # format. Because '\\' represents the directory delimiter on Windows systems,
              # it can't be used as an escape character.
              ignore-paths=

              # Files or directories matching the regular expression patterns are skipped.
              # The regex matches against base names, not paths. The default value ignores
              # Emacs file locks
              ignore-patterns=^\.#

              # List of module names for which member attributes should not be checked
              # (useful for modules/projects where namespaces are manipulated during runtime
              # and thus existing member attributes cannot be deduced by static analysis). It
              # supports qualified module names, as well as Unix pattern matching.
              ignored-modules=

              # Python code to execute, usually for sys.path manipulation such as
              # pygtk.require().
              #init-hook=

              # Use multiple processes to speed up Pylint. Specifying 0 will auto-detect the
              # number of processors available to use, and will cap the count on Windows to
              # avoid hangs.
              jobs=1

              # Control the amount of potential inferred values when inferring a single
              # object. This can help the performance when dealing with large functions or
              # complex, nested conditions.
              limit-inference-results=100

              # List of plugins (as comma separated values of python module names) to load,
              # usually to register additional checkers.
              load-plugins=

              # Pickle collected data for later comparisons.
              persistent=yes

              # Minimum Python version to use for version dependent checks. Will default to
              # the version used to run pylint.
              py-version=3.10

              # Discover python modules and packages in the file system subtree.
              recursive=no

              # Add paths to the list of the source roots. Supports globbing patterns. The
              # source root is an absolute path or a path relative to the current working
              # directory used to determine a package namespace for modules located under the
              # source root.
              source-roots=

              # When enabled, pylint would attempt to guess common misconfiguration and emit
              # user-friendly hints instead of false-positive error messages.
              suggestion-mode=yes

              # Allow loading of arbitrary C extensions. Extensions are imported into the
              # active Python interpreter and may run arbitrary code.
              unsafe-load-any-extension=no

              # In verbose mode, extra non-checker-related info will be displayed.
              #verbose=

              [BASIC]

              # Naming style matching correct argument names.
              argument-naming-style=snake_case

              # Regular expression matching correct argument names. Overrides argument-
              # naming-style. If left empty, argument names will be checked with the set
              # naming style.
              #argument-rgx=

              # Naming style matching correct attribute names.
              attr-naming-style=snake_case

              # Regular expression matching correct attribute names. Overrides attr-naming-
              # style. If left empty, attribute names will be checked with the set naming
              # style.
              #attr-rgx=

              # Bad variable names which should always be refused, separated by a comma.
              bad-names=foo,
              bar,
              baz,
              toto,
              tutu,
              tata

              # Bad variable names regexes, separated by a comma. If names match any regex,
              # they will always be refused
              bad-names-rgxs=

              # Naming style matching correct class attribute names.
              class-attribute-naming-style=any

              # Regular expression matching correct class attribute names. Overrides class-
              # attribute-naming-style. If left empty, class attribute names will be checked
              # with the set naming style.
              #class-attribute-rgx=

              # Naming style matching correct class constant names.
              class-const-naming-style=UPPER_CASE

              # Regular expression matching correct class constant names. Overrides class-
              # const-naming-style. If left empty, class constant names will be checked with
              # the set naming style.
              #class-const-rgx=

              # Naming style matching correct class names.
              class-naming-style=PascalCase

              # Regular expression matching correct class names. Overrides class-naming-
              # style. If left empty, class names will be checked with the set naming style.
              #class-rgx=

              # Naming style matching correct constant names.
              const-naming-style=UPPER_CASE

              # Regular expression matching correct constant names. Overrides const-naming-
              # style. If left empty, constant names will be checked with the set naming
              # style.
              #const-rgx=

              # Minimum line length for functions/classes that require docstrings, shorter
              # ones are exempt.
              docstring-min-length=-1

              # Naming style matching correct function names.
              function-naming-style=snake_case

              # Regular expression matching correct function names. Overrides function-
              # naming-style. If left empty, function names will be checked with the set
              # naming style.
              #function-rgx=

              # Good variable names which should always be accepted, separated by a comma.
              good-names=i,
              j,
              k,
              ex,
              Run,
              _

              # Good variable names regexes, separated by a comma. If names match any regex,
              # they will always be accepted
              good-names-rgxs=

              # Include a hint for the correct naming format with invalid-name.
              include-naming-hint=no

              # Naming style matching correct inline iteration names.
              inlinevar-naming-style=any

              # Regular expression matching correct inline iteration names. Overrides
              # inlinevar-naming-style. If left empty, inline iteration names will be checked
              # with the set naming style.
              #inlinevar-rgx=

              # Naming style matching correct method names.
              method-naming-style=snake_case

              # Regular expression matching correct method names. Overrides method-naming-
              # style. If left empty, method names will be checked with the set naming style.
              #method-rgx=

              # Naming style matching correct module names.
              module-naming-style=snake_case

              # Regular expression matching correct module names. Overrides module-naming-
              # style. If left empty, module names will be checked with the set naming style.
              #module-rgx=

              # Colon-delimited sets of names that determine each other's naming style when
              # the name regexes allow several styles.
              name-group=

              # Regular expression which should only match function or class names that do
              # not require a docstring.
              no-docstring-rgx=^_

              # List of decorators that produce properties, such as abc.abstractproperty. Add
              # to this list to register other decorators that produce valid properties.
              # These decorators are taken in consideration only for invalid-name.
              property-classes=abc.abstractproperty

              # Regular expression matching correct type alias names. If left empty, type
              # alias names will be checked with the set naming style.
              #typealias-rgx=

              # Regular expression matching correct type variable names. If left empty, type
              # variable names will be checked with the set naming style.
              #typevar-rgx=

              # Naming style matching correct variable names.
              variable-naming-style=snake_case

              # Regular expression matching correct variable names. Overrides variable-
              # naming-style. If left empty, variable names will be checked with the set
              # naming style.
              #variable-rgx=

              [CLASSES]

              # Warn about protected attribute access inside special methods
              check-protected-access-in-special-methods=no

              # List of method names used to declare (i.e. assign) instance attributes.
              defining-attr-methods=__init__,
              __new__,
              setUp,
              asyncSetUp,
              __post_init__

              # List of member names, which should be excluded from the protected access
              # warning.
              exclude-protected=_asdict,_fields,_replace,_source,_make,os._exit

              # List of valid names for the first argument in a class method.
              valid-classmethod-first-arg=cls

              # List of valid names for the first argument in a metaclass class method.
              valid-metaclass-classmethod-first-arg=mcs

              [DESIGN]

              # List of regular expressions of class ancestor names to ignore when counting
              # public methods (see R0903)
              exclude-too-few-public-methods=

              # List of qualified class names to ignore when counting class parents (see
              # R0901)
              ignored-parents=

              # Maximum number of arguments for function / method.
              max-args=5

              # Maximum number of attributes for a class (see R0902).
              max-attributes=7

              # Maximum number of boolean expressions in an if statement (see R0916).
              max-bool-expr=5

              # Maximum number of branch for function / method body.
              max-branches=12

              # Maximum number of locals for function / method body.
              max-locals=15

              # Maximum number of parents for a class (see R0901).
              max-parents=7

              # Maximum number of public methods for a class (see R0904).
              max-public-methods=20

              # Maximum number of return / yield for function / method body.
              max-returns=6

              # Maximum number of statements in function / method body.
              max-statements=50

              # Minimum number of public methods for a class (see R0903).
              min-public-methods=2

              [EXCEPTIONS]

              # Exceptions that will emit a warning when caught.
              overgeneral-exceptions=builtins.BaseException,builtins.Exception

              [FORMAT]

              # Expected format of line ending, e.g. empty (any line ending), LF or CRLF.
              expected-line-ending-format=

              # Regexp for a line that is allowed to be longer than the limit.
              ignore-long-lines=^\s*(# )?<?https?://\S+>?$

              # Number of spaces of indent required inside a hanging or continued line.
              indent-after-paren=4

              # String used as indentation unit. This is usually "    " (4 spaces) or "\t" (1
              # tab).
              indent-string='    '

              # Maximum number of characters on a single line.
              max-line-length=100

              # Maximum number of lines in a module.
              max-module-lines=1000

              # Allow the body of a class to be on the same line as the declaration if body
              # contains single statement.
              single-line-class-stmt=no

              # Allow the body of an if to be on the same line as the test if there is no
              # else.
              single-line-if-stmt=no

              [IMPORTS]

              # List of modules that can be imported at any level, not just the top level
              # one.
              allow-any-import-level=

              # Allow explicit reexports by alias from a package __init__.
              allow-reexport-from-package=no

              # Allow wildcard imports from modules that define __all__.
              allow-wildcard-with-all=no

              # Deprecated modules which should not be used, separated by a comma.
              deprecated-modules=

              # Output a graph (.gv or any supported image format) of external dependencies
              # to the given file (report RP0402 must not be disabled).
              ext-import-graph=

              # Output a graph (.gv or any supported image format) of all (i.e. internal and
              # external) dependencies to the given file (report RP0402 must not be
              # disabled).
              import-graph=

              # Output a graph (.gv or any supported image format) of internal dependencies
              # to the given file (report RP0402 must not be disabled).
              int-import-graph=

              # Force import order to recognize a module as part of the standard
              # compatibility libraries.
              known-standard-library=

              # Force import order to recognize a module as part of a third party library.
              known-third-party=enchant

              # Couples of modules and preferred modules, separated by a comma.
              preferred-modules=

              [LOGGING]

              # The type of string formatting that logging methods do. `old` means using %
              # formatting, `new` is for `{}` formatting.
              logging-format-style=old

              # Logging modules to check that the string format arguments are in logging
              # function parameter format.
              logging-modules=logging

              [MESSAGES CONTROL]

              # Only show warnings with the listed confidence levels. Leave empty to show
              # all. Valid levels: HIGH, CONTROL_FLOW, INFERENCE, INFERENCE_FAILURE,
              # UNDEFINED.
              confidence=HIGH,
              CONTROL_FLOW,
              INFERENCE,
              INFERENCE_FAILURE,
              UNDEFINED

              # Disable the message, report, category or checker with the given id(s). You
              # can either give multiple identifiers separated by comma (,) or put this
              # option multiple times (only on the command line, not in the configuration
              # file where it should appear only once). You can also use "--disable=all" to
              # disable everything first and then re-enable specific checks. For example, if
              # you want to run only the similarities checker, you can use "--disable=all
              # --enable=similarities". If you want to run only the classes checker, but have
              # no Warning level messages displayed, use "--disable=all --enable=classes
              # --disable=W".
              disable=raw-checker-failed,
              bad-inline-option,
              locally-disabled,
              file-ignored,
              suppressed-message,
              useless-suppression,
              deprecated-pragma,
              use-symbolic-message-instead,
              use-implicit-booleaness-not-comparison-to-string,
              use-implicit-booleaness-not-comparison-to-zero

              # Enable the message, report, category or checker with the given id(s). You can
              # either give multiple identifier separated by comma (,) or put this option
              # multiple time (only on the command line, not in the configuration file where
              # it should appear only once). See also the "--disable" option for examples.
              enable=

              [METHOD_ARGS]

              # List of qualified names (i.e., library.method) which require a timeout
              # parameter e.g. 'requests.api.get,requests.api.post'
              timeout-methods=requests.api.delete,requests.api.get,requests.api.head,requests.api.options,requests.api.patch,requests.api.post,requests.api.put,requests.api.request

              [MISCELLANEOUS]

              # List of note tags to take in consideration, separated by a comma.
              notes=FIXME,
              XXX,
              TODO

              # Regular expression of note tags to take in consideration.
              notes-rgx=

              [REFACTORING]

              # Maximum number of nested blocks for function / method body
              max-nested-blocks=5

              # Complete name of functions that never returns. When checking for
              # inconsistent-return-statements if a never returning function is called then
              # it will be considered as an explicit return statement and no message will be
              # printed.
              never-returning-functions=sys.exit,argparse.parse_error

              [REPORTS]

              # Python expression which should return a score less than or equal to 10. You
              # have access to the variables 'fatal', 'error', 'warning', 'refactor',
              # 'convention', and 'info' which contain the number of messages in each
              # category, as well as 'statement' which is the total number of statements
              # analyzed. This score is used by the global evaluation report (RP0004).
              evaluation=max(0, 0 if fatal else 10.0 - ((float(5 * error + warning + refactor + convention) / statement) * 10))

              # Template used to display messages. This is a python new-style format string
              # used to format the message information. See doc for all details.
              msg-template=

              # Set the output format. Available formats are: text, parseable, colorized,
              # json2 (improved json format), json (old json format) and msvs (visual
              # studio). You can also give a reporter class, e.g.
              # mypackage.mymodule.MyReporterClass.
              #output-format=

              # Tells whether to display a full report or only the messages.
              reports=no

              # Activate the evaluation score.
              score=yes

              [SIMILARITIES]

              # Comments are removed from the similarity computation
              ignore-comments=yes

              # Docstrings are removed from the similarity computation
              ignore-docstrings=yes

              # Imports are removed from the similarity computation
              ignore-imports=yes

              # Signatures are removed from the similarity computation
              ignore-signatures=yes

              # Minimum lines number of a similarity.
              min-similarity-lines=4

              [SPELLING]

              # Limits count of emitted suggestions for spelling mistakes.
              max-spelling-suggestions=4

              # Spelling dictionary name. No available dictionaries : You need to install
              # both the python package and the system dependency for enchant to work.
              spelling-dict=

              # List of comma separated words that should be considered directives if they
              # appear at the beginning of a comment and should not be checked.
              spelling-ignore-comment-directives=fmt: on,fmt: off,noqa:,noqa,nosec,isort:skip,mypy:

              # List of comma separated words that should not be checked.
              spelling-ignore-words=

              # A path to a file that contains the private dictionary; one word per line.
              spelling-private-dict-file=

              # Tells whether to store unknown words to the private dictionary (see the
              # --spelling-private-dict-file option) instead of raising a message.
              spelling-store-unknown-words=no

              [STRING]

              # This flag controls whether inconsistent-quotes generates a warning when the
              # character used as a quote delimiter is used inconsistently within a module.
              check-quote-consistency=no

              # This flag controls whether the implicit-str-concat should generate a warning
              # on implicit string concatenation in sequences defined over several lines.
              check-str-concat-over-line-jumps=no

              [TYPECHECK]

              # List of decorators that produce context managers, such as
              # contextlib.contextmanager. Add to this list to register other decorators that
              # produce valid context managers.
              contextmanager-decorators=contextlib.contextmanager

              # List of members which are set dynamically and missed by pylint inference
              # system, and so shouldn't trigger E1101 when accessed. Python regular
              # expressions are accepted.
              generated-members=

              # Tells whether to warn about missing members when the owner of the attribute
              # is inferred to be None.
              ignore-none=yes

              # This flag controls whether pylint should warn about no-member and similar
              # checks whenever an opaque object is returned when inferring. The inference
              # can return multiple potential results while evaluating a Python object, but
              # some branches might not be evaluated, which results in partial inference. In
              # that case, it might be useful to still emit no-member and other checks for
              # the rest of the inferred objects.
              ignore-on-opaque-inference=yes

              # List of symbolic message names to ignore for Mixin members.
              ignored-checks-for-mixins=no-member,
              not-async-context-manager,
              not-context-manager,
              attribute-defined-outside-init

              # List of class names for which member attributes should not be checked (useful
              # for classes with dynamically set attributes). This supports the use of
              # qualified names.
              ignored-classes=optparse.Values,thread._local,_thread._local,argparse.Namespace

              # Show a hint with possible names when a member name was not found. The aspect
              # of finding the hint is based on edit distance.
              missing-member-hint=yes

              # The minimum edit distance a name should have in order to be considered a
              # similar match for a missing member name.
              missing-member-hint-distance=1

              # The total number of similar names that should be taken in consideration when
              # showing a hint for a missing member.
              missing-member-max-choices=1

              # Regex pattern to define which classes are considered mixins.
              mixin-class-rgx=.*[Mm]ixin

              # List of decorators that change the signature of a decorated function.
              signature-mutators=

              [VARIABLES]

              # List of additional names supposed to be defined in builtins. Remember that
              # you should avoid defining new builtins when possible.
              additional-builtins=

              # Tells whether unused global variables should be treated as a violation.
              allow-global-unused-variables=yes

              # List of names allowed to shadow builtins
              allowed-redefined-builtins=

              # List of strings which can identify a callback function by name. A callback
              # name must start or end with one of those strings.
              callbacks=cb_,
              _cb

              # A regular expression matching the name of dummy variables (i.e. expected to
              # not be used).
              dummy-variables-rgx=_+$|(_[a-zA-Z0-9_]*[a-zA-Z0-9]+?$)|dummy|^ignored_|^unused_

              # Argument names that match this expression will be ignored.
              ignored-argument-names=_.*|^ignored_|^unused_

              # Tells whether we should check for unused import in __init__ files.
              init-import=no

              # List of qualified module names which can have objects that can redefine
              # builtins.
              redefining-builtins-modules=six.moves,past.builtins,future.builtins,builtins,io
              ```

            ]]></content>
          </file>
          <file name="python-rules.md" path="docs/standards/python-rules.md">
            <content><![CDATA[
              # System Prompt for Python DevOps AI Assistant in Telecommunications

              ## Your Role and Expertise

              You are a highly skilled senior DevOps engineer specializing in Python development within a telecommunications company. Your areas of expertise include:

              - Python scripting and application development
              - Infrastructure as Code (IaC) using tools like Terraform, Ansible, Nornir
              - Cloud platforms (AWS, Azure, GCP) and their Python SDKs
              - Containerization and orchestration (Docker, Kubernetes)
              - Networking protocols and telecom-specific technologies
              - CI/CD pipelines (Jenkins, GitLab CI, GitHub Actions)
              - Monitoring and logging (Prometheus, Grafana, ELK stack)

              Your primary responsibilities:

              - Assist users with Python-based automation, infrastructure management, and application development in a telecom context
              - Engage proactively by:

                - Asking clarifying questions to fully understand user requirements
                - Identifying potential challenges in user requests
                - Providing comprehensive solutions, including informative explanations and efficient code
              - Educate users on correct terminology, best practices, and Python idioms, even when they seem knowledgeable
              - Correct misused terms or concepts, explaining the correct usage

              ## Your Core Objective

              - Analyze and repair any broken or inefficient Python code provided by the user
              - Enhance scripts based on user-provided notes and requirements
              - Address all TODO comments within the code, ensuring they align with user-provided notes
              - Identify and resolve any discrepancies between code comments, functionality, and user intentions
              - Optimize code for performance and readability, following Python best practices (PEP 8, PEP 20)

              ## Your Workflow

              - Thorough Analysis:

                - Dedicate time to deeply understand the provided code
                - Consider both user goals and script objectives

              - Progress Tracking:

                - Utilize a Kanban board for tracking progress and updates
                - Create a board entry for each distinct challenge or task
                - Regularly update the board with your progress
                - Encourage user review and feedback on your updates

              - Code-Comment Consistency:

                - Ensure perfect alignment between:
                  - Function implementations
                  - Docstrings (using Google or NumPy style)
                  - In-line comments
                  - Module-level documentation

              - Placeholder Handling:

                - Identify all placeholders, whether in comments or non-executing code
                - Never remove existing placeholders
                - Update placeholders with relevant, functional code or information
                - If a placeholder's purpose is unclear, seek clarification from the user

              - Quality Assurance:

                - If you detect any inconsistencies between comments, descriptions, code functionality, and apparent intent:
                  - Halt your coding process immediately
                  - Engage with the user to resolve these discrepancies
                  - Use your expertise to guide users through complex issues

              ## Your Communication Protocol

              - Restate Objective:

                - Begin by clearly articulating your understanding of the core objective

              - Information Gathering:

                - Proactively ask specific, relevant questions to clarify any ambiguities in the user's request or intentions

              - Proposal Presentation:

                - Provide a detailed, bullet-pointed response outlining your proposed improvements and repairs

              - User Approval:

                - Wait for explicit user approval before implementing any code changes

              - Implementation:

                - Once approved, proceed with code modifications without reiterating previously discussed changes

              - Proactive Education:

                - Correct any misused terms or concepts, explaining the proper usage
                - Provide additional relevant information or best practices, even if not explicitly asked

              ## Your Development Environment and Practices

              - Linting and Static Analysis:

                - Use Pylance for linting and static type checking
                - Adhere strictly to Pylance's recommendations and error messages

              - Code Formatting:

                - Utilize Black for consistent code formatting
                - Ensure all code adheres to Black's opinionated style

              - Documentation:

                - Always include a docstring at the top of each module to describe the project or script
                - Use Google or NumPy style for function and class docstrings

              - Naming Conventions:

                - Be extremely careful not to redefine names within the same scope
                - Use clear, descriptive names that follow PEP 8 conventions

              - Logging:

                - Use lazy % formatting in logging functions to comply with Pylint W1203
                - Example: `logging.info("Processing %s", data)` instead of `logging.info(f"Processing {data}")`

              ## Code Artifact Standards

              - Completeness:

                - Every artifact must be a fully functional, complete script or module
                - Partial scripts are strictly prohibited

              - Version Control:

                - Implement a clear versioning system for all code artifacts to facilitate easy tracking of changes

              - Documentation Consistency:

                - Ensure all code changes are accurately reflected in:
                  - Docstrings (function, class, and module level)
                  - In-line comments
                  - README files (for larger projects)

              - Modular Design:

                - Prioritize a modular code structure with:
                  - Well-defined functions and classes
                  - Clear separation of concerns
                  - Use of appropriate design patterns

              - Configuration Management:

                - Use environment variables or configuration files for sensitive or environment-specific information
                - Implement proper error handling and logging

              - Naming Conventions:

                - Follow PEP 8 naming conventions consistently
                - Use clear, descriptive names for variables, functions, and classes

              - Comprehensive Metadata:

                - Include at the top of each script or module:
                  - A clear version number
                  - Author information
                  - Brief description of the script/module purpose
                  - Usage examples
                  - Any required dependencies

              - Project Structure:

                - For larger projects, include a `pyproject.toml` file for Black configuration
                - Include a `.pylintrc` or `setup.cfg` file for Pylance/Pylint configuration

              - Module-Level Docstring:

                - Always start each Python file with a module-level docstring describing the project or module's purpose

              - Logging Setup:

                - Configure logging at the beginning of the script or in a separate logging configuration file
                - Use `%()`-style string formatting in all logging calls

              - Addition and important coding practices:

                  1. Logging Format: Use lazy % formatting in logging functions to comply with Pylint W1203 and improve performance. Example: `logging.info("Processing %s", variable)` instead of `logging.info(f"Processing {variable}")`
                  2. Network Request Timeouts: Always include a timeout parameter in network requests to prevent indefinite hanging. Example: `requests.get(url, timeout=30)`
                  3. Optional Dependency Handling: Implement graceful handling of optional dependencies. Allow the script to function with reduced capabilities if a non-critical package is missing. Example:

                      ```python
                      try:
                          import optional_package
                          OPTIONAL_FEATURE_ENABLED = True

                      except ImportError:
                          OPTIONAL_FEATURE_ENABLED = False
                      ```

                  4. Task-Specific Functions: Create dedicated functions for specific, repeatable tasks to improve code modularity and readability, especially for critical operations. Example: Instead of embedding complex logic in larger functions, break it out into smaller, well-named functions.
                  5. Efficient List Operations: Utilize list comprehensions or generator expressions for creating lists or performing aggregate operations when appropriate. Example: `sum(1 for item in items if condition(item))` instead of a loop with a counter.
                  6. Robust Error Handling for External Operations: When interacting with external services, APIs, or performing I/O operations, implement thorough error checking. Provide meaningful error messages or fallback values to ensure graceful failure handling. Example:

                      ```python
                      try:
                          result = external_api_call()
                      except ExternalAPIError as e:
                          logger.error("API call failed: %s", str(e))
                          result = fallback_value
                      ```

              - Example Code Structure:

              ```python
              """
              Telecom Network Data Processor

              This module provides functionality to process and analyze network data
              from telecommunications equipment. It includes tools for data aggregation,
              performance metric calculation, and anomaly detection.

              Usage:
                  from network_processor import process_network_data

                  data = [...]  # List of network data points
                  results = process_network_data(data)
                  print(results)

              Note: This module requires Python 3.7+ and uses typing features.
              """

              import logging
              from typing import List, Dict

              # Configure logging
              logging.basicConfig(level=logging.INFO)
              logger = logging.getLogger(__name__)

              def process_network_data(data: List[Dict]) -> Dict:
                  """
                  Process network data from telecom equipment.

                  This function takes raw data from network devices and processes it
                  into a summarized format for further analysis.

                  Args:
                      data: A list of dictionaries containing raw network data.
                          Each dictionary should have keys: 'device_id', 'timestamp', 'metrics'.

                  Returns:
                      A dictionary containing processed data with keys:
                      'total_devices', 'average_latency', 'peak_bandwidth'.

                  Raises:
                      ValueError: If the input data is empty or in an invalid format.
                  """
                  if not data:
                      raise ValueError("Input data is empty")

                  try:
                      # Initialize variables for data processing
                      total_devices = len(set(item['device_id'] for item in data))
                      total_latency = 0
                      max_bandwidth = 0

                      # Process each data point
                      for item in data:
                          # Extract and process latency data
                          latency = item['metrics'].get('latency')
                          if latency is not None:
                              total_latency += latency

                          # Track peak bandwidth
                          bandwidth = item['metrics'].get('bandwidth', 0)
                          max_bandwidth = max(max_bandwidth, bandwidth)

                      # Calculate average latency
                      average_latency = total_latency / len(data) if data else 0

                      # Prepare and return processed data
                      return {
                          'total_devices': total_devices,
                          'average_latency': average_latency,
                          'peak_bandwidth': max_bandwidth
                      }

                  except KeyError as missing_key:
                      # Log the error and re-raise with a more informative message
                      logger.error("Invalid data format: missing key %s", missing_key)
                      raise ValueError(f"Invalid data format: missing key {missing_key}") from missing_key

              # Example usage
              if __name__ == "__main__":
                  sample_data = [
                      {'device_id': 'dev1', 'timestamp': 1628097600, 'metrics': {'latency': 20, 'bandwidth': 100}},
                      {'device_id': 'dev2', 'timestamp': 1628097610, 'metrics': {'latency': 25, 'bandwidth': 150}},
                      {'device_id': 'dev1', 'timestamp': 1628097620, 'metrics': {'latency': 22, 'bandwidth': 110}}
                  ]

                  try:
                      result = process_network_data(sample_data)
                      logger.info("Processed data: %s", result)
                  except ValueError as error:
                      logger.error("Error processing data: %s", error)
              ```

              CRITICAL: You must ALWAYS provide complete, fully functional Python scripts or modules in your artifacts. This is essential for user testing and feedback. Partial or incomplete artifacts are unacceptable and render your assistance ineffective.

              IMPORTANT: Do not be obsequiously agreeable. Your role is to educate and guide users, even when they appear knowledgeable. Always correct misused terms or concepts, and provide additional relevant information or best practices, even if not explicitly asked. In the context of Python and DevOps in telecommunications, be particularly attentive to best practices in areas like error handling, type hinting, logging, and security.

            ]]></content>
          </file>
        </directory>
        <file name="todo.md" path="docs/todo.md">
          <content><![CDATA[
            # CodeMapper RoadMap

            - [ ] Add these tasks to the github project board

            ## Core Functionality

            tag is: [`core`](https://github.com/users/shaneholloman/projects/9/views/7)

            - [x] Implement direct repository URL analysis
            - [ ] Expand Git hosting service support (GitLab, Bitbucket)
            - [ ] Develop progress indicator for large repository processing
            - [ ] Enable custom branch selection
            - [ ] Implement intelligent repository categorization
              - [ ] Deterministic analysis
              - [ ] AI-powered analysis
              - [ ] Add option to update original repo with categories (for owners)
            - [ ] Create config yaml file for user preferences
              - [ ] Git pull location
              - [ ] CodeMaps location
              - [ ] DocMaps location
              - [ ] Output format default
              - [ ] api keys for AI integration
              - [ ] Ai services to use

            ## File Outputs

            tag is: [`outputs`](https://github.com/users/shaneholloman/projects/9/views/7)

            - [x] Generate `*_codemap.md` for file contents and directory structure
            - [x] Create separate `*_docmap.md` for documentation directories
            - [x] Add repository source information to output file header
            - [x] Markdown (default)
              - [ ] XML
              - [ ] JSON
              - [ ] YAML
              - [ ] RST
              - [ ] AsciiDoc
            - [ ] Go to top of page link in markdown output next to each heading

            ## CodeMapper Service

            tag is: [`service`](https://github.com/users/shaneholloman/projects/9/views/7)

            - [ ] Create a server version of CodeMapper with API
            - [ ] Implement user authentication and authorization
            - [ ] Ansible playbook for server deployment

            ## Documentation

            tag is: [`docs`](https://github.com/users/shaneholloman/projects/9/views/7)

            - [x] Utilize changelog.md for version tracking

            ## AI Integration

            tag is: [`ai`](https://github.com/users/shaneholloman/projects/9/views/7)

            - [ ] Implement AI-generated alt text for images --option
            - [ ] Implement AI-generated code summarization --option
            - [ ] Explore base64 image encoding and embedding in output --option

            ## User Experience

            tag is: [`ux`](https://github.com/users/shaneholloman/projects/9/views/7)

            - [ ] Create PreFlight checks for CodeMapper
              - [ ] Check for git
              - [ ] Check for Python version
              - [ ] Check for required libraries
            - [ ] Create function for an install advisory for first-time users
              - [ ] Checks for git
              - [ ] Detect your OS
              - [ ] Give advise on how to install git based on OS
            - [ ] Develop comprehensive help command and menu
            - [ ] Add builtin aliases for codemapper [cm, map] can be additionally defined in config
            - [ ] Enhance Table of Contents generation
              - [ ] Consider using `md_toc` library for robustness
            - [ ] Implement a warning and alt method for huge repository processing
            - [ ] EPIC: Introduce Mermaid flow chart option for code execution flow visualization

            ## Developer Experience

            tag is: [`dev`](https://github.com/users/shaneholloman/projects/9/views/7)

            - [x] Implement CI for linting
            - [ ] Implement CI for automated testing
            - [ ] Implement CI for PyPI package deployment

          ]]></content>
        </file>
      </directory>
      <file name="pyproject.toml" path="pyproject.toml">
        <content><![CDATA[
          [build-system]
          requires = ["setuptools>=61.0", "wheel"]
          build-backend = "setuptools.build_meta"

          [tool.black]
          line-length = 100
          target-version = ['py310', 'py311']
          include = '\.pyi?$'
          extend-exclude = '''
          /(
            # directories
            \.eggs
            | \.git
            | \.hg
            | \.mypy_cache
            | \.tox
            | \.venv
            | build
            | dist
          )/
          '''

          [project]
          name = "codemapper"
          version = "4.0.1"
          description = "A tool to generate comprehensive Markdown artifacts of directory structures and file contents"
          readme = "README.md"
          requires-python = ">=3.10"
          license = { file = "LICENSE" }
          authors = [
              { name = "Shane Holloman", email = "shaneholloman@gmail.com" }
          ]
          classifiers = [
              "Development Status :: 4 - Beta",
              "Intended Audience :: Developers",
              "License :: OSI Approved :: MIT License",
              "Operating System :: OS Independent",
              "Programming Language :: Python :: 3.10",
              "Programming Language :: Python :: 3.11",
              "Programming Language :: Python :: 3.12"
          ]
          keywords = ["codemapper", "markdown", "directory", "file contents"]
          dependencies = [
              "chardet",
              "pathspec"
          ]
          [project.urls]
          "Homepage" = "https://github.com/shaneholloman/codemapper"

          [project.scripts]
          codemapper = "codemapper.main:main"

        ]]></content>
      </file>
      <file name="requirements.txt" path="requirements.txt">
        <content><![CDATA[
          pathspec>=0.9.0
          chardet>=4.0.0

        ]]></content>
      </file>
      <directory name="src" path="src">
        <directory name="codemapper" path="src/codemapper">
          <file name="__init__.py" path="src/codemapper/__init__.py">
            <content><![CDATA[
              """
              CodeMapper: A tool for generating comprehensive Markdown artifacts of
              directory structures and file contents.

              Package Structure:
              - main.py: Entry point and CLI handling
              - utils.py: Core functionality and helper functions
              - config.py: Configuration constants and settings

              This package provides functionality to analyze local directories or GitHub repositories,
              creating detailed Markdown documentation of their structure and contents.
              """

              __version__ = "4.0.1"

              # Any other necessary imports or package-level code can go here

            ]]></content>
          </file>
          <file name="config.py" path="src/codemapper/config.py">
            <content><![CDATA[
              """Configuration and constants for CodeMapper."""

              # Standard documentation directory names to check
              DOC_DIRECTORIES = {
                  "docs",
                  "wiki",
                  "documentation",
              }

              # Output file suffixes
              CODEMAP_SUFFIX = "_codemap.md"
              DOCMAP_SUFFIX = "_docmap.md"

              ARCHIVE_EXTENSIONS = {
                  ".zip", ".tar", ".gz", ".rar", ".7z", ".bz2", ".xz",
                  ".tgz", ".tbz2", ".tar.gz", ".tar.bz2",
              }

              CODE_FENCE_MAP = {
                  ".bat": "batch",
                  ".c": "c",
                  ".cfg": "ini",
                  ".conf": "ini",
                  ".cpp": "cpp",
                  ".cs": "csharp",
                  ".css": "css",
                  ".csv": "csv",
                  ".dart": "dart",
                  ".dockerfile": "dockerfile",
                  ".go": "go",
                  ".groovy": "groovy",
                  ".h": "cpp",
                  ".hbs": "handlebars",
                  ".hcl": "hcl",
                  ".hpp": "cpp",
                  ".html": "html",
                  ".ini": "ini",
                  ".j2": "jinja2",
                  ".java": "java",
                  ".js": "javascript",
                  ".json": "json",
                  ".jsx": "jsx",
                  ".kt": "kotlin",
                  ".lua": "lua",
                  ".log": "log",
                  ".md": "markdown",
                  ".php": "php",
                  ".pl": "perl",
                  ".pkl": "pickle",
                  ".proto": "protobuf",
                  ".ps1": "powershell",
                  ".py": "python",
                  ".r": "r",
                  ".rb": "ruby",
                  ".rs": "rust",
                  ".scala": "scala",
                  ".sh": "bash",
                  ".sql": "sql",
                  ".swift": "swift",
                  ".tf": "hcl",
                  ".ts": "typescript",
                  ".tsx": "tsx",
                  ".txt": "text",
                  ".vue": "vue",
                  ".xml": "xml",
                  ".yaml": "yml",
                  ".yml": "yml",
                  ".gitignore": "ini",
                  "requirements.txt": "ini",
                  "requirements.yml": "yml",
                  "Dockerfile": "dockerfile",
                  "Makefile": "makefile",
                  "": "txt",  # Default fallback
              }

              LARGE_FILE_EXTENSIONS = {
                  # Database files
                  ".db", ".sqlite", ".sqlite3", ".dbf", ".mdb", ".accdb",
                  ".sql", ".psql", ".dmp",
                  # Image files
                  ".jpg", ".jpeg", ".png", ".gif", ".bmp", ".tiff", ".ico",
                  ".svg", ".webp", ".eps", ".raw", ".cr2", ".nef",
                  # Video files
                  ".mp4", ".avi", ".mov", ".wmv", ".flv", ".mkv", ".webm",
                  ".vob", ".ogv",
                  # Audio files
                  ".mp3", ".wav", ".ogg", ".flac",
                  # Document files
                  ".pdf", ".doc", ".docx", ".xls", ".xlsx", ".ppt", ".pptx",
                  ".odt",
                  # Archive files
                  ".zip", ".tar", ".gz", ".rar", ".7z", ".bz2", ".xz",
                  ".tgz", ".tbz2",
                  # Executable files
                  ".exe", ".msi", ".apk", ".app", ".dmg", ".iso", ".jar",
                  ".deb", ".rpm",
                  # Font files
                  ".ttf", ".otf", ".woff", ".woff2", ".eot", ".fon",
                  # Binary files
                  ".bin", ".dll", ".so", ".dylib", ".dat", ".sav",
              }

            ]]></content>
          </file>
          <file name="docmap.py" path="src/codemapper/docmap.py">
            <content><![CDATA[
              """
              Documentation Mapping Module for CodeMapper.

              This module provides functionality to generate comprehensive documentation maps
              from repositories, focusing on README files and documentation directories. It works
              in conjunction with the main CodeMapper functionality but specifically targets
              documentation content.

              The module supports scanning for common documentation directories and processing
              README.md files to create a complete documentation overview.
              """

              import os
              import logging
              # Add dataclasses import for our new DocMapConfig
              from dataclasses import dataclass
              from typing import Optional

              # Need pathspec for type hinting
              import pathspec

              from .config import DOC_DIRECTORIES
              from .utils import (
                  read_file_content,
                  generate_file_tree,
                  collect_file_paths,
              )

              logger = logging.getLogger(__name__)

              # This is our new configuration class
              # @dataclass is a decorator that automatically adds special methods to the class
              # This makes it easier to create and use classes that are mainly used to hold data
              @dataclass
              class DocMapConfig:
                  """Configuration class for document mapping generation.

                  This class holds all the parameters needed for generating a documentation map.
                  Using a class like this helps us organize related data and makes the code
                  easier to maintain.

                  Attributes:
                      directory_path (str): The path to the directory being mapped
                      gitignore_spec (pathspec.PathSpec): Gitignore specifications to follow
                      include_ignored (bool): Whether to include ignored files, defaults to False
                      source (str): Source information string, defaults to empty string
                      base_name (str): Base name for documentation, defaults to empty string
                      doc_dir (Optional[str]): Custom documentation directory, defaults to None
                  """
                  directory_path: str
                  gitignore_spec: pathspec.PathSpec
                  include_ignored: bool = False
                  source: str = ""
                  base_name: str = ""
                  doc_dir: Optional[str] = None

              def find_documentation_directory(base_path: str, custom_dir: Optional[str] = None) -> Optional[str]:
                  """
                  Find the documentation directory in the given base path.

                  Args:
                      base_path (str): Base directory path to search in
                      custom_dir (Optional[str]): Custom documentation directory path if specified

                  Returns:
                      Optional[str]: Path to documentation directory if found, None otherwise
                  """
                  if custom_dir:
                      custom_path = os.path.join(base_path, custom_dir)
                      return custom_path if os.path.isdir(custom_path) else None

                  for doc_dir in DOC_DIRECTORIES:
                      doc_path = os.path.join(base_path, doc_dir)
                      if os.path.isdir(doc_path):
                          logger.info("Found documentation directory: %s", doc_path)
                          return doc_path

                  logger.info("No standard documentation directory found")
                  return None

              def process_readme(base_path: str) -> Optional[str]:
                  """
                  Process the root README.md file.

                  Args:
                      base_path (str): Base directory path containing the README

                  Returns:
                      Optional[str]: Content of README.md if found, None otherwise
                  """
                  readme_path = os.path.join(base_path, "README.md")
                  if os.path.isfile(readme_path):
                      logger.info("Found README.md file")
                      return read_file_content(readme_path)

                  logger.info("No README.md file found")
                  return None

              # Updated to use the new DocMapConfig class
              def generate_docmap_content(config: DocMapConfig) -> str:
                  """
                  Generate documentation mapping markdown content.

                  Instead of taking multiple parameters, this function now takes a single
                  DocMapConfig object that contains all the necessary configuration values.
                  This makes the function cleaner and easier to maintain.

                  Args:
                      config (DocMapConfig): Configuration object containing all parameters

                  Returns:
                      str: Generated markdown content for documentation mapping
                  """
                  # Start building the markdown content
                  md_content = [f"# {config.base_name} Documentation", ""]
                  md_content.append(f"> DocMap Source: {config.source}\n")
                  md_content.append(
                      "This markdown document provides a comprehensive overview of the documentation "
                      "files and structure. It aims to give viewers (human or AI) a complete view "
                      "of the project's documentation in a single file for easy analysis.\n"
                  )

                  # Process README first
                  readme_content = process_readme(config.directory_path)
                  if readme_content:
                      md_content.extend([
                          "## Project README\n",
                          "The following section contains the main project README content:\n",
                          "````markdown",
                          readme_content,
                          "````\n"
                      ])

                  # Find and process documentation directory
                  doc_path = find_documentation_directory(config.directory_path, config.doc_dir)
                  if doc_path:
                      relative_doc_path = os.path.relpath(doc_path, config.directory_path)
                      md_content.extend([
                          f"## Documentation Directory: {relative_doc_path}\n",
                          "### Directory Structure\n",
                          "```tree"
                      ])

                      tree_content = generate_file_tree(doc_path, config.gitignore_spec, config.include_ignored)
                      md_content.extend([tree_content, "```\n"])

                      # Collect and process documentation files
                      file_paths = collect_file_paths(doc_path, config.gitignore_spec, config.include_ignored)
                      if file_paths:
                          md_content.append("### Documentation Contents\n")
                          for path in file_paths:
                              full_path = os.path.join(doc_path, path)
                              content = read_file_content(full_path)
                              is_markdown = path.endswith('.md')
                              md_content.extend([
                                  f"#### {path}\n",
                                  "````markdown" if is_markdown else "```",
                                  content,
                                  "````\n" if is_markdown else "```\n"
                              ])

                  # If neither README nor doc directory found, include a note
                  if not readme_content and not doc_path:
                      md_content.append(
                          "> Note: No README.md or standard documentation directory found in this repository.\n"
                      )

                  md_content.append(
                      "> This concludes the documentation mapping. Please review thoroughly for a "
                      "comprehensive understanding of the project's documentation.\n"
                  )

                  return "\n".join(md_content)

            ]]></content>
          </file>
          <file name="main.py" path="src/codemapper/main.py">
            <content><![CDATA[
              """
              CodeMapper: Comprehensive Codebase Visualization for Humans and AI

              Main execution module for CodeMapper tool.
              """

              import argparse
              import os
              import subprocess
              import sys

              from . import __version__
              from .config import CODEMAP_SUFFIX, DOCMAP_SUFFIX
              from .utils import (
                  load_gitignore_specs,
                  generate_markdown_document,
                  detect_input_type,
                  clone_github_repo,
                  manage_output_directory,
                  capture_source,
              )
              # Import both the function and the configuration class
              from .docmap import generate_docmap_content, DocMapConfig

              def main():
                  """Main function to orchestrate the markdown document generation process."""
                  parser = argparse.ArgumentParser(
                      description="Generate markdown document from directory structure or GitHub repository."
                  )
                  parser.add_argument(
                      "input_path",
                      nargs="?",
                      help="Path to the directory to process or GitHub repository URL",
                  )
                  parser.add_argument(
                      "--include-ignored",
                      action="store_true",
                      help="Include files normally ignored by .gitignore",
                  )
                  parser.add_argument(
                      "--version",
                      action="version",
                      version=f"CodeMapper version {__version__}",
                      help="Show the version number and exit",
                  )
                  parser.add_argument(
                      "--docs",
                      action="store_true",
                      help="Generate documentation map instead of code map",
                  )
                  parser.add_argument(
                      "--docs-dir",
                      help="Specify custom documentation directory path",
                  )
                  args = parser.parse_args()

                  if not args.input_path:
                      parser.print_help()
                      sys.exit(1)

                  try:
                      input_type, path = detect_input_type(args.input_path)
                  except ValueError as e:
                      print(f"Error: {e}")
                      sys.exit(1)

                  source = capture_source(args.input_path)

                  if input_type == "github":
                      try:
                          directory_path = clone_github_repo(path)
                      except subprocess.CalledProcessError as e:
                          print(f"Error cloning repository: {e}")
                          sys.exit(1)
                      except OSError as e:
                          print(f"Error creating directory: {e}")
                          sys.exit(1)
                  else:
                      directory_path = path

                  if not os.path.isabs(args.input_path) and not args.input_path.startswith(
                      ("http://", "https://")
                  ):
                      base_name = os.path.basename(os.path.abspath(args.input_path))
                  else:
                      base_name = os.path.basename(directory_path)

                  gitignore_spec = load_gitignore_specs(directory_path)

                  if args.docs:
                      # Create a DocMapConfig object with all our parameters
                      doc_config = DocMapConfig(
                          directory_path=directory_path,
                          gitignore_spec=gitignore_spec,
                          include_ignored=args.include_ignored,
                          source=source,
                          base_name=base_name,
                          doc_dir=args.docs_dir
                      )
                      # Pass the config object to generate_docmap_content
                      markdown_content = generate_docmap_content(doc_config)
                      output_file_path = manage_output_directory(base_name, args.input_path, DOCMAP_SUFFIX)
                  else:
                      markdown_content = generate_markdown_document(
                          directory_path, gitignore_spec, args.include_ignored, source, base_name
                      )
                      output_file_path = manage_output_directory(base_name, args.input_path, CODEMAP_SUFFIX)

                  with open(output_file_path, "w", encoding="utf-8") as md_file:
                      md_file.write(markdown_content)

                  print(f"Markdown file has been created: {output_file_path}")

              if __name__ == "__main__":
                  main()

            ]]></content>
          </file>
          <file name="utils.py" path="src/codemapper/utils.py">
            <content><![CDATA[
              """Utility functions for CodeMapper."""

              import os
              import re
              import subprocess
              import mimetypes
              from typing import List, Tuple  # Add back Tuple, remove unused Optional

              import chardet
              import pathspec

              from .config import CODEMAP_SUFFIX  # At top level import

              from .config import (
                  ARCHIVE_EXTENSIONS,
                  CODE_FENCE_MAP,
                  LARGE_FILE_EXTENSIONS,
              )

              # Move all functions from codemapper.py
              def should_exclude_directory(dir_name: str, include_ignored: bool = False) -> bool:
                  """Determine if a directory should be excluded from processing."""
                  if include_ignored:
                      return dir_name == ".git"
                  return dir_name in {".git", ".gitignore"}


              def determine_code_fence(file_path: str) -> str:
                  """Determine the appropriate code fence language based on the file path."""
                  _, ext = os.path.splitext(file_path)
                  file_name = os.path.basename(file_path)

                  # Check for specific file names first
                  if file_name in CODE_FENCE_MAP:  # Removed unnecessary parentheses
                      return CODE_FENCE_MAP[file_name]

                  # Then check for extensions
                  return CODE_FENCE_MAP.get(ext.lower(), "txt")


              def load_gitignore_specs(base_path: str) -> pathspec.PathSpec:
                  """Load .gitignore specifications from the given base path."""
                  gitignore_path = os.path.join(base_path, ".gitignore")
                  if os.path.isfile(gitignore_path):
                      with open(gitignore_path, "r", encoding="utf-8") as file:
                          return pathspec.PathSpec.from_lines("gitwildmatch", file)
                  return pathspec.PathSpec.from_lines("gitwildmatch", [])


              def collect_file_paths(
                  directory_path: str,
                  gitignore_spec: pathspec.PathSpec,
                  include_ignored: bool = False
              ) -> List[str]:
                  """Collect file paths, respecting .gitignore rules unless include_ignored is True."""
                  file_paths = []

                  for root, dirs, files in os.walk(directory_path):
                      dirs[:] = [
                          d for d in dirs
                          if not should_exclude_directory(d, include_ignored)
                          and (include_ignored or not gitignore_spec.match_file(
                              os.path.join(root, d)))
                      ]

                      for filename in files:
                          _, ext = os.path.splitext(filename)
                          if ext.lower() in ARCHIVE_EXTENSIONS:
                              continue
                          file_path = os.path.join(root, filename)
                          if include_ignored or not gitignore_spec.match_file(file_path):
                              rel_path = os.path.relpath(file_path, start=directory_path)
                              # Normalize path separators to forward slashes
                              normalized_path = rel_path.replace(os.sep, "/")
                              file_paths.append(normalized_path)

                  return file_paths


              def generate_toc(file_paths: List[str], base_name: str) -> str:
                  """Generate a table of contents based on heading levels."""
                  toc = ["<!-- TOC -->", ""]
                  toc.append(f"- [{base_name}](#{base_name.lower().replace(' ', '-')})")
                  toc.append("  - [Document Table of Contents](#document-table-of-contents)")
                  toc.append("  - [Repo File Tree](#repo-file-tree)")
                  toc.append("  - [Repo File Contents](#repo-file-contents)")

                  # Sort file paths, placing README.md first and .gitignore second
                  sorted_paths = sorted(
                      file_paths, key=lambda x: (x != "README.md", x != ".gitignore", x.lower())
                  )

                  for path in sorted_paths:
                      # Normalize path separators to forward slashes
                      normalized_path = path.replace(os.sep, "/")

                      # Handle __init__.py files specially
                      if normalized_path.endswith("__init__.py"):
                          link = (
                              normalized_path.lower()
                              .replace("__init__.py", "init.py")
                              .replace(".", "")
                              .replace("/", "")
                          )
                          heading = normalized_path.replace("__init__.py", "init.py")
                      else:
                          # Create the link by removing dots and slashes, but keep underscores
                          link = normalized_path.lower().replace(".", "").replace("/", "")

                          # Replace double underscores with a single underscore in the link
                          while "__" in link:
                              link = link.replace("__", "_")

                          # Escape underscores in the heading
                          heading = normalized_path.replace("_", "\\_")

                      toc.append(f"    - [{heading}](#{link})")

                  toc.append("")
                  toc.append("<!-- /TOC -->")

                  return "\n".join(toc)


              def generate_file_tree(
                  directory_path: str,
                  gitignore_spec: pathspec.PathSpec,
                  include_ignored: bool = False
              ) -> str:
                  """Generate an accurate file tree representation of the given directory."""
                  def walk_directory(dir_path: str, prefix: str = "") -> List[str]:
                      files = []
                      contents = sorted(os.listdir(dir_path))
                      dirs = [
                          d
                          for d in contents
                          if os.path.isdir(os.path.join(dir_path, d))
                          and not should_exclude_directory(d, include_ignored)
                      ]
                      regular_files = [f for f in contents if os.path.isfile(os.path.join(dir_path, f))]

                      for idx, name in enumerate(dirs + regular_files):
                          full_path = os.path.join(dir_path, name)
                          rel_path = os.path.relpath(full_path, directory_path)

                          if not include_ignored and gitignore_spec.match_file(rel_path):
                              continue

                          is_last = idx == len(dirs + regular_files) - 1
                          current_prefix = "â””â”€â”€ " if is_last else "â”œâ”€â”€ "

                          if os.path.isdir(full_path):
                              files.append(
                                  f"{prefix}{current_prefix}{name}/"
                              )  # Add trailing slash for directories
                              extension = "    " if is_last else "â”‚   "
                              files.extend(walk_directory(full_path, prefix + extension))
                          else:
                              files.append(f"{prefix}{current_prefix}{name}")

                      return files

                  tree = [".", *walk_directory(directory_path)]

                  dir_count = sum(1 for line in tree if line.endswith("/"))
                  file_count = len(tree) - dir_count - 1  # -1 for the root '.'

                  tree.append(f"\n{dir_count} directories, {file_count} files")
                  return "\n".join(tree)


              def is_large_file(file_path: str) -> bool:
                  """Determine if a file is considered a large binary file."""
                  _, ext = os.path.splitext(file_path.lower())
                  file_name = os.path.basename(file_path)

                  # Check if it's in LARGE_FILE_EXTENSIONS
                  if ext in LARGE_FILE_EXTENSIONS:
                      return True

                  # Check if it's in CODE_FENCE_MAP
                  if ext in CODE_FENCE_MAP or file_name in CODE_FENCE_MAP:
                      return False

                  # Fallback to MIME type check
                  mime_type, _ = mimetypes.guess_type(file_path)
                  if mime_type:
                      # List of MIME types that are considered text-based
                      text_mime_types = [
                          "text/",
                          "application/json",
                          "application/javascript",
                          "application/xml",
                          "application/x-httpd-php",
                          "application/x-sh",
                          "application/x-csh",
                      ]
                      return not any(mime_type.startswith(text_type) for text_type in text_mime_types)

                  # If MIME type couldn't be determined, assume it's not a large file
                  return False


              def get_file_info(file_path: str) -> str:
                  """Get information about a file without reading its contents."""
                  size = os.path.getsize(file_path)
                  mime_type, _ = mimetypes.guess_type(file_path)
                  return f"File Type: {mime_type or 'Unknown'}, Size: {size} bytes"


              def read_file_content(file_path: str) -> str:
                  """Read file content with encoding detection and large file handling."""
                  if is_large_file(file_path):
                      return f"[Large or binary file detected. {get_file_info(file_path)}]"

                  try:
                      with open(file_path, "rb") as file:
                          raw_data = file.read(1024)  # Read only the first 1024 bytes for detection
                      detect_result = chardet.detect(raw_data)
                      detected_encoding = detect_result["encoding"] if detect_result else None

                      encodings_to_try = [detected_encoding, "utf-8", "latin-1"]

                      for encoding in encodings_to_try:
                          if encoding:
                              try:
                                  with open(file_path, "r", encoding=encoding) as file:
                                      return file.read().rstrip()
                              except UnicodeDecodeError:
                                  continue

                      return (
                          f"[Error: Unable to decode file with detected encoding "
                          f"({detected_encoding}), UTF-8, or Latin-1]"
                      )
                  except IOError as e:
                      return f"[Error reading file: {str(e)}]"


              def capture_source(input_path: str) -> str:
                  """Capture the source of the pulled repo."""
                  if os.path.isdir(input_path):
                      return f"Local directory: `{os.path.abspath(input_path)}`"
                  return f"GitHub repository: <{input_path}>"


              def generate_markdown_document(
                  directory_path: str,
                  gitignore_spec: pathspec.PathSpec,
                  include_ignored: bool = False,
                  source: str = "",
                  base_name: str = ""
              ) -> str:
                  """Generate a markdown document from the directory structure."""
                  md_content = f"# {base_name}\n\n"
                  md_content += f"> CodeMap Source: {source}\n\n"
                  md_content += (
                      "This markdown document provides a comprehensive overview of the "
                      "directory structure and file contents. It aims to give viewers "
                      "(human or AI) a complete view of the codebase in a single file "
                      "for easy analysis.\n\n"
                  )
                  md_content += "## Document Table of Contents\n\n"
                  md_content += (
                      "The table of contents below is for navigational convenience and "
                      "reflects this document's structure, not the actual file structure "
                      "of the repository.\n\n"
                  )

                  file_paths = collect_file_paths(directory_path, gitignore_spec, include_ignored)

                  # Generate TOC
                  toc = generate_toc(file_paths, base_name)
                  md_content += toc + "\n\n"

                  md_content += "## Repo File Tree\n\n"
                  md_content += (
                      "This file tree represents the actual structure of the repository. "
                      "It's crucial for understanding the organization of the codebase.\n\n"
                  )
                  md_content += "```tree\n"
                  md_content += generate_file_tree(directory_path, gitignore_spec, include_ignored)
                  md_content += "\n```\n\n"

                  md_content += "## Repo File Contents\n\n"
                  md_content += (
                      "The following sections present the content of each file in the repository. "
                      "Large and binary files are acknowledged but their contents are not displayed.\n\n"
                  )

                  # Generate code blocks for each file
                  for i, path in enumerate(file_paths):
                      md_content += f"### {path}\n\n"
                      full_path = os.path.join(directory_path, path)

                      code_fence_lang = determine_code_fence(path)
                      fence = "````" if path.endswith(".md") else "```"
                      md_content += f"{fence}{code_fence_lang}\n"
                      file_content = read_file_content(full_path)
                      md_content += file_content + "\n"
                      md_content += f"{fence}\n"

                      if i < len(file_paths) - 1:
                          md_content += "\n"
                      else:
                          md_content += (
                              "\n> This concludes the repository's file contents. "
                              "Please review thoroughly for a comprehensive "
                              "understanding of the codebase.\n"
                          )

                  return md_content


              def detect_input_type(input_path: str) -> Tuple[str, str]:
                  """Detect whether the input is a local directory or a GitHub URL."""
                  # Check if it's a valid local directory
                  if os.path.isdir(input_path):
                      return "local", input_path

                  # Check if it's a valid GitHub URL
                  github_pattern = r"^https?://github\.com/[\w-]+/[\w.-]+(?:\.git)?$"
                  if re.match(github_pattern, input_path):
                      return "github", input_path

                  raise ValueError("Invalid input. Please provide a valid local directory path or GitHub URL.")


              def clone_github_repo(repo_url: str) -> str:
                  """Clone a GitHub repository into a '_github' directory."""
                  github_dir = os.path.join(".", "_github")
                  os.makedirs(github_dir, exist_ok=True)

                  repo_name = repo_url.split("/")[-1].replace(".git", "")
                  repo_path = os.path.join(github_dir, repo_name)

                  if os.path.exists(repo_path):
                      print(f"Repository '{repo_name}' already exists. Updating...")
                      subprocess.run(["git", "-C", repo_path, "pull"], check=True)
                      return repo_path

                  print(f"Cloning repository '{repo_name}'...")
                  subprocess.run(["git", "clone", repo_url, repo_path], check=True)
                  return repo_path


              def manage_output_directory(base_name: str, input_path: str, suffix: str = CODEMAP_SUFFIX) -> str:
                  """
                  Manage the output directory for the markdown output.

                  Args:
                      base_name (str): Base name for the output file
                      input_path (str): Original input path (used for relative path handling)
                      suffix (str): Suffix for the output file. Defaults to CODEMAP_SUFFIX.

                  Returns:
                      str: Path to the output file
                  """
                  output_dir = os.path.join(".", "_codemaps")
                  os.makedirs(output_dir, exist_ok=True)

                  # If input_path is a relative path, use its basename
                  if not os.path.isabs(input_path) and not input_path.startswith(("http://", "https://")):
                      base_name = os.path.basename(os.path.abspath(input_path))

                  file_name = f"{base_name}{suffix}"
                  return os.path.join(output_dir, file_name)

            ]]></content>
          </file>
        </directory>
      </directory>
      <directory name="tests" path="tests">
        <file name="test_github_codemapper.py" path="tests/test_github_codemapper.py">
          <content><![CDATA[
            """Unit tests for CodeMapper functionality."""

            import tempfile
            from pathlib import Path

            import pytest
            import pathspec

            from codemapper.utils import (
                should_exclude_directory,
                determine_code_fence,
                load_gitignore_specs,
                collect_file_paths,
                is_large_file,
                read_file_content,
            )

            @pytest.fixture(name='test_dir')
            def create_temp_dir():
                """Create a temporary directory for testing."""
                with tempfile.TemporaryDirectory() as temp_dir_name:
                    yield temp_dir_name

            @pytest.fixture(name='test_repo')
            def create_sample_repo(test_dir):
                """Create a sample repository structure for testing."""
                # Create test files
                Path(test_dir, "README.md").write_text("# Test Repository", encoding='utf-8')
                Path(test_dir, ".gitignore").write_text("*.log\n__pycache__/", encoding='utf-8')
                Path(test_dir, "main.py").write_text("print('Hello, World!')", encoding='utf-8')
                Path(test_dir, "test.log").write_text("test log content", encoding='utf-8')

                # Create nested directory structure
                src_dir = Path(test_dir, "src")
                src_dir.mkdir()
                Path(src_dir, "__init__.py").write_text("", encoding='utf-8')
                Path(src_dir, "app.py").write_text("# App code", encoding='utf-8')

                return test_dir

            # Test cases for utility functions
            def test_should_exclude_directory():
                """Test directory exclusion logic."""
                assert should_exclude_directory(".git") is True
                assert should_exclude_directory("src") is False
                assert should_exclude_directory(".git", include_ignored=True) is True
                assert should_exclude_directory(".gitignore", include_ignored=True) is False

            def test_determine_code_fence():
                """Test code fence language determination."""
                assert determine_code_fence("test.py") == "python"
                assert determine_code_fence("README.md") == "markdown"
                assert determine_code_fence("Dockerfile") == "dockerfile"
                assert determine_code_fence("unknown.xyz") == "txt"

            def test_load_gitignore_specs(test_repo):
                """Test gitignore specifications loading."""
                specs = load_gitignore_specs(test_repo)
                assert isinstance(specs, pathspec.PathSpec)
                assert specs.match_file("test.log") is True
                assert specs.match_file("main.py") is False

            def test_collect_file_paths(test_repo):
                """Test file path collection."""
                specs = load_gitignore_specs(test_repo)
                paths = collect_file_paths(test_repo, specs)

                assert "README.md" in paths
                assert "main.py" in paths
                assert "src/app.py" in paths
                assert "test.log" not in paths  # Should be ignored per .gitignore

                # Test with include_ignored=True
                all_paths = collect_file_paths(test_repo, specs, include_ignored=True)
                assert "test.log" in all_paths

            def test_is_large_file(test_dir):
                """Test large file detection."""
                # Create a test file larger than typical text files
                large_file = Path(test_dir, "large.bin")
                large_file.write_bytes(b'\0' * 1024 * 1024)  # 1MB file

                small_file = Path(test_dir, "small.txt")
                small_file.write_text("Hello World", encoding='utf-8')

                assert is_large_file(str(large_file)) is True
                assert is_large_file(str(small_file)) is False

            def test_read_file_content(test_dir):
                """Test file content reading with different encodings."""
                # Test UTF-8 file
                utf8_file = Path(test_dir, "utf8.txt")
                utf8_content = "Hello, ä¸–ç•Œ!"
                utf8_file.write_text(utf8_content, encoding='utf-8')
                assert read_file_content(str(utf8_file)) == utf8_content

                # Test binary file
                bin_file = Path(test_dir, "test.bin")
                bin_file.write_bytes(b'\x00\x01\x02\x03')
                assert "Large or binary file detected" in read_file_content(str(bin_file))

            def test_integration_sample_repo(test_repo):
                """Integration test using a sample repository."""
                specs = load_gitignore_specs(test_repo)
                paths = collect_file_paths(test_repo, specs)

                assert len(paths) >= 3  # README.md, main.py, src/app.py

                allowed_extensions = ('.md', '.py')
                allowed_files = {'.gitignore'}
                assert all(
                    p.endswith(allowed_extensions) or p in allowed_files
                    for p in paths
                )

            # Add test for handling non-existent files
            def test_read_nonexistent_file():
                """Test reading a non-existent file."""
                content = read_file_content("nonexistent_file.txt")
                assert "Error reading file" in content

            if __name__ == "__main__":
                pytest.main([__file__])

          ]]></content>
        </file>
      </directory>
      <directory name="wip" path="wip">
        <directory name="packages" path="wip/packages">
          <file name="package_categories.json" path="wip/packages/package_categories.json">
            <content><![CDATA[
              {
                "Archiving :: Packaging": [
                  "wheel"
                ],
                "Artificial Intelligence": [
                  "huggingface-hub",
                  "llama-index",
                  "llama-index-core",
                  "llama-index-legacy",
                  "tokenizers"
                ],
                "Build Tools": [
                  "pip",
                  "poetry",
                  "poetry-core"
                ],
                "Code Generators": [
                  "astor",
                  "llvmlite",
                  "proto-plus"
                ],
                "Communications": [
                  "coloredlogs",
                  "humanfriendly",
                  "pyasn1",
                  "pyasn1_modules"
                ],
                "Compilers": [
                  "numba",
                  "pyparsing",
                  "tree-sitter"
                ],
                "Cryptography": [
                  "cryptography",
                  "pyOpenSSL",
                  "rsa"
                ],
                "Database": [
                  "peewee"
                ],
                "Debuggers": [
                  "debugpy",
                  "stack-data"
                ],
                "Desktop Environment": [
                  "tqdm"
                ],
                "Documentation": [
                  "docutils",
                  "mkdocs",
                  "mkdocs-callouts"
                ],
                "Email :: Filters": [
                  "Markdown"
                ],
                "Filters": [
                  "Pygments"
                ],
                "Financial": [
                  "yfinance"
                ],
                "Fonts": [
                  "fonttools"
                ],
                "Front-Ends": [
                  "alembic",
                  "SQLAlchemy"
                ],
                "GIS": [
                  "shapely"
                ],
                "Graphics": [
                  "pikepdf",
                  "Pillow",
                  "psgfiglet",
                  "pypdfium2",
                  "PySimpleGUI"
                ],
                "Graphics :: Graphics Conversion": [
                  "html2image"
                ],
                "Information Analysis": [
                  "contourpy",
                  "regex"
                ],
                "Installation/Setup": [
                  "ansible-core"
                ],
                "Internet": [
                  "fastapi",
                  "filelock",
                  "google-api-core",
                  "google-cloud-aiplatform",
                  "google-cloud-bigquery",
                  "google-cloud-core",
                  "google-cloud-resource-manager",
                  "google-cloud-storage",
                  "google-resumable-media",
                  "grpc-google-iam-v1",
                  "paramiko",
                  "pydantic",
                  "pytube",
                  "scp",
                  "websocket-client"
                ],
                "JSON": [
                  "jiter",
                  "jsonschema",
                  "referencing"
                ],
                "JSON :: JSON Schema": [
                  "jsonschema-specifications"
                ],
                "Libraries": [
                  "aiohappyeyeballs",
                  "aiosqlite",
                  "async-timeout",
                  "blessed",
                  "Brotli",
                  "click-option-group",
                  "decorator",
                  "diff-match-patch",
                  "frozendict",
                  "iniconfig",
                  "intel-openmp",
                  "jsonpatch",
                  "jsonpointer",
                  "mkl",
                  "mmh3",
                  "more-itertools",
                  "multitasking",
                  "pathlib",
                  "pytest",
                  "python-dateutil",
                  "python-decouple",
                  "python-editor",
                  "questionary",
                  "scipy",
                  "six",
                  "tabulate",
                  "tbb",
                  "virtualenv",
                  "watchdog",
                  "wcwidth"
                ],
                "Libraries :: Application Frameworks": [
                  "fastapi-cli",
                  "inquirer",
                  "typer"
                ],
                "Libraries :: Python Modules": [
                  "aiohttp-client-cache",
                  "annotated-types",
                  "appdirs",
                  "asttokens",
                  "beautifulsoup4",
                  "bracex",
                  "cachetools",
                  "chardet",
                  "charset-normalizer",
                  "cohere",
                  "Deprecated",
                  "dirtyjson",
                  "distro",
                  "email_validator",
                  "fastavro",
                  "fastjsonschema",
                  "flatbuffers",
                  "greenlet",
                  "html5lib",
                  "jedi",
                  "jsonpickle",
                  "markdown-it-py",
                  "mdurl",
                  "monotonic",
                  "networkx",
                  "oauthlib",
                  "openai",
                  "parso",
                  "pathspec",
                  "pkginfo",
                  "platformdirs",
                  "psutil",
                  "pypdf",
                  "PyPDF2",
                  "PyPika",
                  "python-multipart",
                  "pytz",
                  "PyYAML",
                  "pyyaml_env_tag",
                  "requests-cache",
                  "resolvelib",
                  "ruamel.yaml",
                  "ruff",
                  "setuptools",
                  "shellingham",
                  "tomli",
                  "watchfiles",
                  "wcmatch",
                  "wget"
                ],
                "Markup :: HTML": [
                  "lxml"
                ],
                "Markup :: XML": [
                  "defusedxml",
                  "xmltodict"
                ],
                "Mathematics": [
                  "mpmath"
                ],
                "Multimedia": [
                  "moviepy"
                ],
                "Name Service (DNS)": [
                  "dnspython",
                  "idna"
                ],
                "Networking": [
                  "pyzmq"
                ],
                "Scientific/Engineering": [
                  "joblib",
                  "nltk",
                  "onnxruntime",
                  "pandas",
                  "pysbd",
                  "sympy",
                  "torch"
                ],
                "Security": [
                  "semgrep"
                ],
                "Shells": [
                  "ipython"
                ],
                "Software Development": [
                  "argcomplete",
                  "cleo",
                  "distlib",
                  "docker",
                  "mypy",
                  "mypy-extensions",
                  "numpy",
                  "pexpect",
                  "prompt-toolkit",
                  "PyGithub",
                  "readchar",
                  "typing_extensions",
                  "typing-inspect",
                  "yamllint",
                  "yaspin"
                ],
                "Sound/Audio": [
                  "pydub",
                  "sounddevice",
                  "soundfile"
                ],
                "Sphinx": [
                  "docstring_parser"
                ],
                "Systems Administration": [
                  "ansible-compat",
                  "enrich",
                  "molecule",
                  "python-dotenv",
                  "subprocess-tee"
                ],
                "Terminals": [
                  "colorama",
                  "jinxed",
                  "ptyprocess",
                  "termcolor"
                ],
                "Testing": [
                  "pluggy",
                  "portalocker"
                ],
                "Text Processing": [
                  "pdfminer.six",
                  "pyfiglet"
                ],
                "Uncategorized": [
                  "agentops",
                  "aiosignal",
                  "anyio",
                  "archspec",
                  "attrs",
                  "auth0-python",
                  "backports.tarfile",
                  "bcrypt",
                  "boto3",
                  "botocore",
                  "bs4",
                  "build",
                  "cattrs",
                  "certifi",
                  "cffi",
                  "chromadb",
                  "chroma-hnswlib",
                  "click",
                  "click-completion",
                  "click-help-colors",
                  "codemapper",
                  "comm",
                  "conda",
                  "conda-content-trust",
                  "conda-libmamba-solver",
                  "conda-package-handling",
                  "conda_package_streaming",
                  "ConfigArgParse",
                  "crashtest",
                  "crontab",
                  "cycler",
                  "dataclasses-json",
                  "diskcache",
                  "docker-shell",
                  "durationpy",
                  "embedchain",
                  "executing",
                  "ffmpeg",
                  "frozenlist",
                  "fsspec",
                  "ghp-import",
                  "gitdb",
                  "GitPython",
                  "git-filter-repo",
                  "git-python",
                  "googleapis-common-protos",
                  "google-crc32c",
                  "gptcache",
                  "grep-ast",
                  "grpcio",
                  "grpcio-status",
                  "grpcio-tools",
                  "h2",
                  "hpack",
                  "httptools",
                  "httpx-sse",
                  "hyperframe",
                  "imageio",
                  "imageio-ffmpeg",
                  "importlib-metadata",
                  "importlib_resources",
                  "installer",
                  "instructor",
                  "ipykernel",
                  "itsdangerous",
                  "jaraco.classes",
                  "jaraco.context",
                  "jaraco.functools",
                  "jmespath",
                  "jsonref",
                  "json_repair",
                  "jupyter_client",
                  "jupyter_core",
                  "keyring",
                  "kiwisolver",
                  "langchain",
                  "langchain-cohere",
                  "langchain-community",
                  "langchain-core",
                  "langchain-experimental",
                  "langchain-openai",
                  "langchain-text-splitters",
                  "langsmith",
                  "libmambapy",
                  "litellm",
                  "llama-cloud",
                  "llama-index-agent-openai",
                  "llama-index-cli",
                  "llama-index-embeddings-openai",
                  "llama-index-indices-managed-llama-cloud",
                  "llama-index-llms-openai",
                  "llama-index-multi-modal-llms-openai",
                  "llama-index-program-openai",
                  "llama-index-question-gen-openai",
                  "llama-index-readers-file",
                  "llama-index-readers-llama-parse",
                  "llama-parse",
                  "marshmallow",
                  "matplotlib-inline",
                  "mem0ai",
                  "menuinst",
                  "mergedeep",
                  "msgpack",
                  "multidict",
                  "nest-asyncio",
                  "nh3",
                  "ollama",
                  "openai-whisper",
                  "openbb",
                  "openbb-benzinga",
                  "openbb-bls",
                  "openbb-cftc",
                  "openbb-commodity",
                  "openbb-core",
                  "openbb-crypto",
                  "openbb-currency",
                  "openbb-derivatives",
                  "openbb-econdb",
                  "openbb-economy",
                  "openbb-equity",
                  "openbb-etf",
                  "openbb-federal-reserve",
                  "openbb-fixedincome",
                  "openbb-fmp",
                  "openbb-fred",
                  "openbb-index",
                  "openbb-intrinio",
                  "openbb-news",
                  "openbb-oecd",
                  "openbb-polygon",
                  "openbb-regulators",
                  "openbb-sec",
                  "openbb-tiingo",
                  "openbb-tradingeconomics",
                  "openbb-yfinance",
                  "opentelemetry-api",
                  "opentelemetry-exporter-otlp-proto-common",
                  "opentelemetry-exporter-otlp-proto-grpc",
                  "opentelemetry-exporter-otlp-proto-http",
                  "opentelemetry-instrumentation",
                  "opentelemetry-instrumentation-asgi",
                  "opentelemetry-instrumentation-fastapi",
                  "opentelemetry-proto",
                  "opentelemetry-sdk",
                  "opentelemetry-semantic-conventions",
                  "opentelemetry-util-http",
                  "orjson",
                  "overrides",
                  "packaging",
                  "parameterized",
                  "pdf2image",
                  "pdfplumber",
                  "pipx",
                  "poetry-plugin-export",
                  "posthog",
                  "proglog",
                  "protobuf",
                  "pulsar-client",
                  "pure-eval",
                  "pycparser",
                  "pydantic_core",
                  "PyNaCl",
                  "pyproject_hooks",
                  "pyreadline3",
                  "PySocks",
                  "pytesseract",
                  "python-crontab",
                  "python-lsp-jsonrpc",
                  "pyvis",
                  "pywin32",
                  "pywin32-ctypes",
                  "qdrant-client",
                  "rapidfuzz",
                  "readme_renderer",
                  "requests-oauthlib",
                  "requests-toolbelt",
                  "rfc3986",
                  "rich",
                  "rpds-py",
                  "s3transfer",
                  "smmap",
                  "sniffio",
                  "spellcaster",
                  "sseclient-py",
                  "stashapp-tools",
                  "striprtf",
                  "tiktoken",
                  "tokencost",
                  "tokentrim",
                  "toml",
                  "tomlkit",
                  "tornado",
                  "traitlets",
                  "tree-sitter-languages",
                  "trove-classifiers",
                  "truststore",
                  "twine",
                  "types-requests",
                  "tzdata",
                  "ujson",
                  "url-normalize",
                  "userpath",
                  "uuid7",
                  "websockets",
                  "whisper",
                  "wrapt",
                  "zipp",
                  "zstandard"
                ],
                "Utilities": [
                  "ansicon",
                  "boltons",
                  "face",
                  "glom",
                  "kubernetes",
                  "pycosat",
                  "PyJWT",
                  "PyMuPDF",
                  "PyMuPDFb",
                  "schema",
                  "tenacity",
                  "win-inet-pton"
                ],
                "Version Control": [
                  "dulwich"
                ],
                "Visualization": [
                  "matplotlib"
                ],
                "WWW/HTTP": [
                  "aiohttp",
                  "asgiref",
                  "backoff",
                  "CacheControl",
                  "google-auth",
                  "h11",
                  "httpcore",
                  "httpx",
                  "requests",
                  "starlette",
                  "urllib3",
                  "uvicorn",
                  "webencodings",
                  "yarl"
                ],
                "WWW/HTTP :: Dynamic Content": [
                  "Jinja2",
                  "Mako",
                  "MarkupSafe",
                  "soupsieve"
                ]
              }
            ]]></content>
          </file>
          <file name="package_info.json" path="wip/packages/package_info.json">
            <content><![CDATA[
              {
                "Brotli": {
                  "category": "Libraries",
                  "description": "Python bindings for the Brotli compression library",
                  "version": "1.0.9"
                },
                "CacheControl": {
                  "category": "WWW/HTTP",
                  "description": "httplib2 caching for requests",
                  "version": "0.14.0"
                },
                "ConfigArgParse": {
                  "category": "Uncategorized",
                  "description": "A drop-in replacement for argparse that allows options to also be set via config files and/or environment variables.",
                  "version": "1.7"
                },
                "Deprecated": {
                  "category": "Libraries :: Python Modules",
                  "description": "Python @deprecated decorator to deprecate old python classes, functions or methods.",
                  "version": "1.2.14"
                },
                "GitPython": {
                  "category": "Uncategorized",
                  "description": "GitPython is a Python library used to interact with Git repositories",
                  "version": "3.1.40"
                },
                "Jinja2": {
                  "category": "WWW/HTTP :: Dynamic Content",
                  "description": "A very fast and expressive template engine.",
                  "version": "3.1.2"
                },
                "Mako": {
                  "category": "WWW/HTTP :: Dynamic Content",
                  "description": "A super-fast templating language that borrows the best ideas from the existing templating languages.",
                  "version": "1.3.5"
                },
                "Markdown": {
                  "category": "Email :: Filters",
                  "description": "Python implementation of John Gruber's Markdown.",
                  "version": "3.5.2"
                },
                "MarkupSafe": {
                  "category": "WWW/HTTP :: Dynamic Content",
                  "description": "Safely add untrusted strings to HTML/XML markup.",
                  "version": "2.1.3"
                },
                "Pillow": {
                  "category": "Graphics",
                  "description": "Python Imaging Library (Fork)",
                  "version": "10.1.0"
                },
                "PyGithub": {
                  "category": "Software Development",
                  "description": "Use the full Github API v3",
                  "version": "2.3.0"
                },
                "PyJWT": {
                  "category": "Utilities",
                  "description": "JSON Web Token implementation in Python",
                  "version": "2.8.0"
                },
                "PyMuPDF": {
                  "category": "Utilities",
                  "description": "A high performance Python library for data extraction, analysis, conversion & manipulation of PDF (and other) documents.",
                  "version": "1.24.10"
                },
                "PyMuPDFb": {
                  "category": "Utilities",
                  "description": "MuPDF shared libraries for PyMuPDF.",
                  "version": "1.24.10"
                },
                "PyNaCl": {
                  "category": "Uncategorized",
                  "description": "Python binding to the Networking and Cryptography (NaCl) library",
                  "version": "1.5.0"
                },
                "PyPDF2": {
                  "category": "Libraries :: Python Modules",
                  "description": "A pure-python PDF library capable of splitting, merging, cropping, and transforming PDF files",
                  "version": "3.0.1"
                },
                "PyPika": {
                  "category": "Libraries :: Python Modules",
                  "description": "A SQL query builder API for Python",
                  "version": "0.48.9"
                },
                "PySimpleGUI": {
                  "category": "Graphics",
                  "description": "Python GUIs for Humans! PySimpleGUI is the top-rated Python application development environment. Launched in 2018 and actively developed, maintained, and supported in 2024. Transforms tkinter, Qt, WxPython, and Remi into a simple, intuitive, and fun experience for both hobbyists and expert users.",
                  "version": "5.0.6"
                },
                "PySocks": {
                  "category": "Uncategorized",
                  "description": "A Python SOCKS client module. See https://github.com/Anorov/PySocks for more information.",
                  "version": "1.7.1"
                },
                "PyYAML": {
                  "category": "Libraries :: Python Modules",
                  "description": "YAML parser and emitter for Python",
                  "version": "6.0.1"
                },
                "Pygments": {
                  "category": "Filters",
                  "description": "Pygments is a syntax highlighting package written in Python.",
                  "version": "2.17.2"
                },
                "SQLAlchemy": {
                  "category": "Front-Ends",
                  "description": "Database Abstraction Library",
                  "version": "2.0.31"
                },
                "agentops": {
                  "category": "Uncategorized",
                  "description": "Observability and DevTool Platform for AI Agents",
                  "version": "0.3.12"
                },
                "aiohappyeyeballs": {
                  "category": "Libraries",
                  "description": "Happy Eyeballs for asyncio",
                  "version": "2.4.0"
                },
                "aiohttp": {
                  "category": "WWW/HTTP",
                  "description": "Async http client/server framework (asyncio)",
                  "version": "3.10.5"
                },
                "aiohttp-client-cache": {
                  "category": "Libraries :: Python Modules",
                  "description": "Persistent cache for aiohttp requests",
                  "version": "0.11.1"
                },
                "aiosignal": {
                  "category": "Uncategorized",
                  "description": "aiosignal: a list of registered asynchronous callbacks",
                  "version": "1.3.1"
                },
                "aiosqlite": {
                  "category": "Libraries",
                  "description": "asyncio bridge to the standard sqlite3 module",
                  "version": "0.20.0"
                },
                "alembic": {
                  "category": "Front-Ends",
                  "description": "A database migration tool for SQLAlchemy.",
                  "version": "1.13.2"
                },
                "annotated-types": {
                  "category": "Libraries :: Python Modules",
                  "description": "Reusable constraint types to use with typing.Annotated",
                  "version": "0.6.0"
                },
                "ansible-compat": {
                  "category": "Systems Administration",
                  "description": "Ansible compatibility goodies",
                  "version": "4.1.10"
                },
                "ansible-core": {
                  "category": "Installation/Setup",
                  "description": "Radically simple IT automation",
                  "version": "2.15.5"
                },
                "ansicon": {
                  "category": "Utilities",
                  "description": "Python wrapper for loading Jason Hood's ANSICON",
                  "version": "1.89.0"
                },
                "anyio": {
                  "category": "Uncategorized",
                  "description": "High level compatibility layer for multiple asynchronous event loop implementations",
                  "version": "3.7.1"
                },
                "appdirs": {
                  "category": "Libraries :: Python Modules",
                  "description": "A small Python module for determining appropriate platform-specific dirs, e.g. a \"user data dir\".",
                  "version": "1.4.4"
                },
                "archspec": {
                  "category": "Uncategorized",
                  "description": "A library to query system architecture",
                  "version": "0.2.1"
                },
                "argcomplete": {
                  "category": "Software Development",
                  "description": "Bash tab completion for argparse",
                  "version": "3.2.2"
                },
                "asgiref": {
                  "category": "WWW/HTTP",
                  "description": "ASGI specs, helper code, and adapters",
                  "version": "3.8.1"
                },
                "astor": {
                  "category": "Code Generators",
                  "description": "Read/rewrite/write Python ASTs",
                  "version": "0.8.1"
                },
                "asttokens": {
                  "category": "Libraries :: Python Modules",
                  "description": "Annotate AST trees with source code positions",
                  "version": "2.4.1"
                },
                "async-timeout": {
                  "category": "Libraries",
                  "description": "Timeout context manager for asyncio programs",
                  "version": "4.0.3"
                },
                "attrs": {
                  "category": "Uncategorized",
                  "description": "Classes Without Boilerplate",
                  "version": "23.1.0"
                },
                "auth0-python": {
                  "category": "Uncategorized",
                  "description": "",
                  "version": "4.7.2"
                },
                "backoff": {
                  "category": "WWW/HTTP",
                  "description": "Function decoration for backoff and retry",
                  "version": "2.2.1"
                },
                "backports.tarfile": {
                  "category": "Uncategorized",
                  "description": "Backport of CPython tarfile module",
                  "version": "1.2.0"
                },
                "bcrypt": {
                  "category": "Uncategorized",
                  "description": "Modern password hashing for your software and your servers",
                  "version": "4.2.0"
                },
                "beautifulsoup4": {
                  "category": "Libraries :: Python Modules",
                  "description": "Screen-scraping library",
                  "version": "4.12.3"
                },
                "blessed": {
                  "category": "Libraries",
                  "description": "Easy, practical library for making terminal apps, by providing an elegant, well-documented interface to Colors, Keyboard input, and screen Positioning capabilities.",
                  "version": "1.20.0"
                },
                "boltons": {
                  "category": "Utilities",
                  "description": "When they're not builtins, they're boltons.",
                  "version": "23.0.0"
                },
                "boto3": {
                  "category": "Uncategorized",
                  "description": "The AWS SDK for Python",
                  "version": "1.35.24"
                },
                "botocore": {
                  "category": "Uncategorized",
                  "description": "Low-level, data-driven core of boto 3.",
                  "version": "1.35.24"
                },
                "bracex": {
                  "category": "Libraries :: Python Modules",
                  "description": "Bash style brace expander.",
                  "version": "2.4"
                },
                "bs4": {
                  "category": "Uncategorized",
                  "description": "Dummy package for Beautiful Soup (beautifulsoup4)",
                  "version": "0.0.2"
                },
                "build": {
                  "category": "Uncategorized",
                  "description": "A simple, correct Python build frontend",
                  "version": "1.2.2"
                },
                "cachetools": {
                  "category": "Libraries :: Python Modules",
                  "description": "Extensible memoizing collections and decorators",
                  "version": "5.5.0"
                },
                "cattrs": {
                  "category": "Uncategorized",
                  "description": "Composable complex class support for attrs and dataclasses.",
                  "version": "24.1.2"
                },
                "certifi": {
                  "category": "Uncategorized",
                  "description": "Python package for providing Mozilla's CA Bundle.",
                  "version": "2024.2.2"
                },
                "cffi": {
                  "category": "Uncategorized",
                  "description": "Foreign Function Interface for Python calling C code.",
                  "version": "1.16.0"
                },
                "chardet": {
                  "category": "Libraries :: Python Modules",
                  "description": "Universal encoding detector for Python 3",
                  "version": "5.2.0"
                },
                "charset-normalizer": {
                  "category": "Libraries :: Python Modules",
                  "description": "The Real First Universal Charset Detector. Open, modern and actively maintained alternative to Chardet.",
                  "version": "3.3.2"
                },
                "chroma-hnswlib": {
                  "category": "Uncategorized",
                  "description": "Chromas fork of hnswlib",
                  "version": "0.7.3"
                },
                "chromadb": {
                  "category": "Uncategorized",
                  "description": "Chroma.",
                  "version": "0.4.24"
                },
                "cleo": {
                  "category": "Software Development",
                  "description": "Cleo allows you to create beautiful and testable command-line interfaces.",
                  "version": "2.1.0"
                },
                "click": {
                  "category": "Uncategorized",
                  "description": "Composable command line interface toolkit",
                  "version": "8.1.7"
                },
                "click-completion": {
                  "category": "Uncategorized",
                  "description": "Fish, Bash, Zsh and PowerShell completion for Click",
                  "version": "0.5.2"
                },
                "click-help-colors": {
                  "category": "Uncategorized",
                  "description": "Colorization of help messages in Click",
                  "version": "0.9.2"
                },
                "click-option-group": {
                  "category": "Libraries",
                  "description": "Option groups missing in Click",
                  "version": "0.5.6"
                },
                "codemapper": {
                  "category": "Uncategorized",
                  "description": "A tool to generate comprehensive Markdown artifacts of directory structures and file contents",
                  "version": "3.2.2"
                },
                "cohere": {
                  "category": "Libraries :: Python Modules",
                  "description": "",
                  "version": "5.9.4"
                },
                "colorama": {
                  "category": "Terminals",
                  "description": "Cross-platform colored terminal text.",
                  "version": "0.4.6"
                },
                "coloredlogs": {
                  "category": "Communications",
                  "description": "Colored terminal output for Python's logging module",
                  "version": "15.0.1"
                },
                "comm": {
                  "category": "Uncategorized",
                  "description": "Jupyter Python Comm implementation, for usage in ipykernel, xeus-python etc.",
                  "version": "0.2.0"
                },
                "conda": {
                  "category": "Uncategorized",
                  "description": "OS-agnostic, system-level binary package manager.",
                  "version": "23.11.0"
                },
                "conda-content-trust": {
                  "category": "Uncategorized",
                  "description": "Signing and verification tools for the conda ecosystem",
                  "version": "0.1.3"
                },
                "conda-libmamba-solver": {
                  "category": "Uncategorized",
                  "description": "The fast mamba solver, now in conda",
                  "version": "23.12.0"
                },
                "conda-package-handling": {
                  "category": "Uncategorized",
                  "description": "Create and extract conda packages of various formats.",
                  "version": "2.2.0"
                },
                "conda_package_streaming": {
                  "category": "Uncategorized",
                  "description": "An efficient library to read from new and old format .conda and .tar.bz2 conda packages.",
                  "version": "0.9.0"
                },
                "contourpy": {
                  "category": "Information Analysis",
                  "description": "Python library for calculating contours of 2D quadrilateral grids",
                  "version": "1.2.0"
                },
                "crashtest": {
                  "category": "Uncategorized",
                  "description": "Manage Python errors with ease",
                  "version": "0.4.1"
                },
                "crontab": {
                  "category": "Uncategorized",
                  "description": "Parse and use crontab schedules in Python",
                  "version": "1.0.1"
                },
                "cryptography": {
                  "category": "Cryptography",
                  "description": "cryptography is a package which provides cryptographic recipes and primitives to Python developers.",
                  "version": "43.0.1"
                },
                "cycler": {
                  "category": "Uncategorized",
                  "description": "Composable style cycles",
                  "version": "0.12.1"
                },
                "dataclasses-json": {
                  "category": "Uncategorized",
                  "description": "Easily serialize dataclasses to and from JSON.",
                  "version": "0.6.7"
                },
                "debugpy": {
                  "category": "Debuggers",
                  "description": "An implementation of the Debug Adapter Protocol for Python",
                  "version": "1.8.0"
                },
                "decorator": {
                  "category": "Libraries",
                  "description": "Decorators for Humans",
                  "version": "4.4.2"
                },
                "defusedxml": {
                  "category": "Markup :: XML",
                  "description": "XML bomb protection for Python stdlib modules",
                  "version": "0.8.0rc2"
                },
                "diff-match-patch": {
                  "category": "Libraries",
                  "description": "Diff Match and Patch",
                  "version": "20230430"
                },
                "dirtyjson": {
                  "category": "Libraries :: Python Modules",
                  "description": "JSON decoder for Python that can extract data from the muck",
                  "version": "1.0.8"
                },
                "diskcache": {
                  "category": "Uncategorized",
                  "description": "Disk Cache -- Disk and file backed persistent cache.",
                  "version": "5.6.3"
                },
                "distlib": {
                  "category": "Software Development",
                  "description": "Distribution utilities",
                  "version": "0.3.8"
                },
                "distro": {
                  "category": "Libraries :: Python Modules",
                  "description": "Distro - an OS platform information API",
                  "version": "1.8.0"
                },
                "dnspython": {
                  "category": "Name Service (DNS)",
                  "description": "DNS toolkit",
                  "version": "2.6.1"
                },
                "docker": {
                  "category": "Software Development",
                  "description": "A Python library for the Docker Engine API.",
                  "version": "7.1.0"
                },
                "docker-shell": {
                  "category": "Uncategorized",
                  "description": "Docker Interactive Shell Runner",
                  "version": "0.0.9"
                },
                "docstring_parser": {
                  "category": "Sphinx",
                  "description": "Parse Python docstrings in reST, Google and Numpydoc format",
                  "version": "0.16"
                },
                "docutils": {
                  "category": "Documentation",
                  "description": "Docutils -- Python Documentation Utilities",
                  "version": "0.21.2"
                },
                "dulwich": {
                  "category": "Version Control",
                  "description": "Python Git Library",
                  "version": "0.21.7"
                },
                "durationpy": {
                  "category": "Uncategorized",
                  "description": "Module for converting between datetime.timedelta and Go's Duration strings.",
                  "version": "0.7"
                },
                "email_validator": {
                  "category": "Libraries :: Python Modules",
                  "description": "A robust email address syntax and deliverability validation library.",
                  "version": "2.2.0"
                },
                "embedchain": {
                  "category": "Uncategorized",
                  "description": "Simplest open source retrieval (RAG) framework",
                  "version": "0.1.121"
                },
                "enrich": {
                  "category": "Systems Administration",
                  "description": "enrich",
                  "version": "1.2.7"
                },
                "executing": {
                  "category": "Uncategorized",
                  "description": "Get the currently executing AST node of a frame, and other information",
                  "version": "2.0.1"
                },
                "face": {
                  "category": "Utilities",
                  "description": "A command-line application framework (and CLI parser). Friendly for users, full-featured for developers.",
                  "version": "22.0.0"
                },
                "fastapi": {
                  "category": "Internet",
                  "description": "FastAPI framework, high performance, easy to learn, fast to code, ready for production",
                  "version": "0.111.1"
                },
                "fastapi-cli": {
                  "category": "Libraries :: Application Frameworks",
                  "description": "Run and manage FastAPI apps from the command line with FastAPI CLI. \ud83d\ude80",
                  "version": "0.0.5"
                },
                "fastavro": {
                  "category": "Libraries :: Python Modules",
                  "description": "Fast read/write of AVRO files",
                  "version": "1.9.7"
                },
                "fastjsonschema": {
                  "category": "Libraries :: Python Modules",
                  "description": "Fastest Python implementation of JSON schema",
                  "version": "2.20.0"
                },
                "ffmpeg": {
                  "category": "Uncategorized",
                  "description": "ffmpeg python package url [https://github.com/jiashaokun/ffmpeg]",
                  "version": "1.4"
                },
                "filelock": {
                  "category": "Internet",
                  "description": "A platform independent file lock.",
                  "version": "3.12.4"
                },
                "flatbuffers": {
                  "category": "Libraries :: Python Modules",
                  "description": "The FlatBuffers serialization format for Python",
                  "version": "24.3.25"
                },
                "fonttools": {
                  "category": "Fonts",
                  "description": "Tools to manipulate font files",
                  "version": "4.47.0"
                },
                "frozendict": {
                  "category": "Libraries",
                  "description": "A simple immutable dictionary",
                  "version": "2.4.4"
                },
                "frozenlist": {
                  "category": "Uncategorized",
                  "description": "A list-like structure which implements collections.abc.MutableSequence",
                  "version": "1.4.0"
                },
                "fsspec": {
                  "category": "Uncategorized",
                  "description": "File-system specification",
                  "version": "2023.9.2"
                },
                "ghp-import": {
                  "category": "Uncategorized",
                  "description": "Copy your docs directly to the gh-pages branch.",
                  "version": "2.1.0"
                },
                "git-filter-repo": {
                  "category": "Uncategorized",
                  "description": "Quickly rewrite git repository history",
                  "version": "2.45.0"
                },
                "git-python": {
                  "category": "Uncategorized",
                  "description": "combination and simplification of some useful git commands",
                  "version": "1.0.3"
                },
                "gitdb": {
                  "category": "Uncategorized",
                  "description": "Git Object Database",
                  "version": "4.0.11"
                },
                "glom": {
                  "category": "Utilities",
                  "description": "A declarative object transformer and formatter, for conglomerating nested data.",
                  "version": "22.1.0"
                },
                "google-api-core": {
                  "category": "Internet",
                  "description": "Google API client core library",
                  "version": "2.20.0"
                },
                "google-auth": {
                  "category": "WWW/HTTP",
                  "description": "Google Authentication Library",
                  "version": "2.35.0"
                },
                "google-cloud-aiplatform": {
                  "category": "Internet",
                  "description": "Vertex AI API client library",
                  "version": "1.67.1"
                },
                "google-cloud-bigquery": {
                  "category": "Internet",
                  "description": "Google BigQuery API client library",
                  "version": "3.25.0"
                },
                "google-cloud-core": {
                  "category": "Internet",
                  "description": "Google Cloud API client core library",
                  "version": "2.4.1"
                },
                "google-cloud-resource-manager": {
                  "category": "Internet",
                  "description": "Google Cloud Resource Manager API client library",
                  "version": "1.12.5"
                },
                "google-cloud-storage": {
                  "category": "Internet",
                  "description": "Google Cloud Storage API client library",
                  "version": "2.18.2"
                },
                "google-crc32c": {
                  "category": "Uncategorized",
                  "description": "A python wrapper of the C library 'Google CRC32C'",
                  "version": "1.6.0"
                },
                "google-resumable-media": {
                  "category": "Internet",
                  "description": "Utilities for Google Media Downloads and Resumable Uploads",
                  "version": "2.7.2"
                },
                "googleapis-common-protos": {
                  "category": "Uncategorized",
                  "description": "Common protobufs used in Google APIs",
                  "version": "1.65.0"
                },
                "gptcache": {
                  "category": "Uncategorized",
                  "description": "GPTCache, a powerful caching library that can be used to speed up and lower the cost of chat applications that rely on the LLM service. GPTCache works as a memcache for AIGC applications, similar to how Redis works for traditional applications.",
                  "version": "0.1.44"
                },
                "greenlet": {
                  "category": "Libraries :: Python Modules",
                  "description": "Lightweight in-process concurrent programming",
                  "version": "3.0.3"
                },
                "grep-ast": {
                  "category": "Uncategorized",
                  "description": "A tool to grep through the AST of a source file",
                  "version": "0.2.4"
                },
                "grpc-google-iam-v1": {
                  "category": "Internet",
                  "description": "IAM API client library",
                  "version": "0.13.1"
                },
                "grpcio": {
                  "category": "Uncategorized",
                  "description": "HTTP/2-based RPC framework",
                  "version": "1.65.1"
                },
                "grpcio-status": {
                  "category": "Uncategorized",
                  "description": "Status proto mapping for gRPC",
                  "version": "1.62.3"
                },
                "grpcio-tools": {
                  "category": "Uncategorized",
                  "description": "Protobuf code generator for gRPC",
                  "version": "1.62.3"
                },
                "h11": {
                  "category": "WWW/HTTP",
                  "description": "A pure-Python, bring-your-own-I/O implementation of HTTP/1.1",
                  "version": "0.14.0"
                },
                "h2": {
                  "category": "Uncategorized",
                  "description": "HTTP/2 State-Machine based protocol implementation",
                  "version": "4.1.0"
                },
                "hpack": {
                  "category": "Uncategorized",
                  "description": "Pure-Python HPACK header compression",
                  "version": "4.0.0"
                },
                "html2image": {
                  "category": "Graphics :: Graphics Conversion",
                  "description": "Package acting as a wrapper around the headless mode of existing web browsers to generate images from URLs and from HTML+CSS strings or files.",
                  "version": "2.0.4.3"
                },
                "html5lib": {
                  "category": "Libraries :: Python Modules",
                  "description": "HTML parser based on the WHATWG HTML specification",
                  "version": "1.1"
                },
                "httpcore": {
                  "category": "WWW/HTTP",
                  "description": "A minimal low-level HTTP client.",
                  "version": "1.0.2"
                },
                "httptools": {
                  "category": "Uncategorized",
                  "description": "A collection of framework independent HTTP protocol utils.",
                  "version": "0.6.1"
                },
                "httpx": {
                  "category": "WWW/HTTP",
                  "description": "The next generation HTTP client.",
                  "version": "0.27.2"
                },
                "httpx-sse": {
                  "category": "Uncategorized",
                  "description": "Consume Server-Sent Event (SSE) messages with HTTPX.",
                  "version": "0.4.0"
                },
                "huggingface-hub": {
                  "category": "Artificial Intelligence",
                  "description": "Client library to download and publish models, datasets and other repos on the huggingface.co hub",
                  "version": "0.16.4"
                },
                "humanfriendly": {
                  "category": "Communications",
                  "description": "Human friendly output for text interfaces using Python",
                  "version": "10.0"
                },
                "hyperframe": {
                  "category": "Uncategorized",
                  "description": "HTTP/2 framing layer for Python",
                  "version": "6.0.1"
                },
                "idna": {
                  "category": "Name Service (DNS)",
                  "description": "Internationalized Domain Names in Applications (IDNA)",
                  "version": "3.6"
                },
                "imageio": {
                  "category": "Uncategorized",
                  "description": "Library for reading and writing a wide range of image, video, scientific, and volumetric data formats.",
                  "version": "2.34.2"
                },
                "imageio-ffmpeg": {
                  "category": "Uncategorized",
                  "description": "FFMPEG wrapper for Python",
                  "version": "0.5.1"
                },
                "importlib-metadata": {
                  "category": "Uncategorized",
                  "description": "Read metadata from Python packages",
                  "version": "6.8.0"
                },
                "importlib_resources": {
                  "category": "Uncategorized",
                  "description": "Read resources from Python packages",
                  "version": "6.4.5"
                },
                "iniconfig": {
                  "category": "Libraries",
                  "description": "brain-dead simple config-ini parsing",
                  "version": "2.0.0"
                },
                "inquirer": {
                  "category": "Libraries :: Application Frameworks",
                  "description": "Collection of common interactive command line user interfaces, based on Inquirer.js",
                  "version": "3.1.3"
                },
                "installer": {
                  "category": "Uncategorized",
                  "description": "A library for installing Python wheels.",
                  "version": "0.7.0"
                },
                "instructor": {
                  "category": "Uncategorized",
                  "description": "structured outputs for llm",
                  "version": "1.3.3"
                },
                "intel-openmp": {
                  "category": "Libraries",
                  "description": "Intel OpenMP* Runtime Library",
                  "version": "2021.4.0"
                },
                "ipykernel": {
                  "category": "Uncategorized",
                  "description": "IPython Kernel for Jupyter",
                  "version": "6.26.0"
                },
                "ipython": {
                  "category": "Shells",
                  "description": "IPython: Productive Interactive Computing",
                  "version": "8.18.0"
                },
                "itsdangerous": {
                  "category": "Uncategorized",
                  "description": "Safely pass data to untrusted environments and back.",
                  "version": "2.2.0"
                },
                "jaraco.classes": {
                  "category": "Uncategorized",
                  "description": "Utility functions for Python class constructs",
                  "version": "3.4.0"
                },
                "jaraco.context": {
                  "category": "Uncategorized",
                  "description": "Useful decorators and context managers",
                  "version": "5.3.0"
                },
                "jaraco.functools": {
                  "category": "Uncategorized",
                  "description": "Functools like those found in stdlib",
                  "version": "4.0.2"
                },
                "jedi": {
                  "category": "Libraries :: Python Modules",
                  "description": "An autocompletion tool for Python that can be used for text editors.",
                  "version": "0.19.1"
                },
                "jinxed": {
                  "category": "Terminals",
                  "description": "Jinxed Terminal Library",
                  "version": "1.2.0"
                },
                "jiter": {
                  "category": "JSON",
                  "description": "Fast iterable JSON parser.",
                  "version": "0.4.2"
                },
                "jmespath": {
                  "category": "Uncategorized",
                  "description": "JSON Matching Expressions",
                  "version": "1.0.1"
                },
                "joblib": {
                  "category": "Scientific/Engineering",
                  "description": "Lightweight pipelining with Python functions",
                  "version": "1.4.2"
                },
                "json_repair": {
                  "category": "Uncategorized",
                  "description": "A package to repair broken json strings",
                  "version": "0.25.3"
                },
                "jsonpatch": {
                  "category": "Libraries",
                  "description": "Apply JSON-Patches (RFC 6902) ",
                  "version": "1.33"
                },
                "jsonpickle": {
                  "category": "Libraries :: Python Modules",
                  "description": "Python library for serializing any arbitrary object graph into JSON",
                  "version": "3.0.2"
                },
                "jsonpointer": {
                  "category": "Libraries",
                  "description": "Identify specific nodes in a JSON document (RFC 6901) ",
                  "version": "2.1"
                },
                "jsonref": {
                  "category": "Uncategorized",
                  "description": "jsonref is a library for automatic dereferencing of JSON Reference objects for Python.",
                  "version": "1.1.0"
                },
                "jsonschema": {
                  "category": "JSON",
                  "description": "An implementation of JSON Schema validation for Python",
                  "version": "4.23.0"
                },
                "jsonschema-specifications": {
                  "category": "JSON :: JSON Schema",
                  "description": "The JSON Schema meta-schemas and vocabularies, exposed as a Registry",
                  "version": "2023.11.2"
                },
                "jupyter_client": {
                  "category": "Uncategorized",
                  "description": "Jupyter protocol implementation and client libraries",
                  "version": "8.6.0"
                },
                "jupyter_core": {
                  "category": "Uncategorized",
                  "description": "Jupyter core package. A base package on which Jupyter projects rely.",
                  "version": "5.5.0"
                },
                "keyring": {
                  "category": "Uncategorized",
                  "description": "Store and access your passwords safely.",
                  "version": "24.3.1"
                },
                "kiwisolver": {
                  "category": "Uncategorized",
                  "description": "A fast implementation of the Cassowary constraint solver",
                  "version": "1.4.5"
                },
                "kubernetes": {
                  "category": "Utilities",
                  "description": "Kubernetes python client",
                  "version": "31.0.0"
                },
                "langchain": {
                  "category": "Uncategorized",
                  "description": "Building applications with LLMs through composability",
                  "version": "0.2.16"
                },
                "langchain-cohere": {
                  "category": "Uncategorized",
                  "description": "An integration package connecting Cohere and LangChain",
                  "version": "0.1.9"
                },
                "langchain-community": {
                  "category": "Uncategorized",
                  "description": "Community contributed LangChain integrations.",
                  "version": "0.2.17"
                },
                "langchain-core": {
                  "category": "Uncategorized",
                  "description": "Building applications with LLMs through composability",
                  "version": "0.2.41"
                },
                "langchain-experimental": {
                  "category": "Uncategorized",
                  "description": "Building applications with LLMs through composability",
                  "version": "0.0.65"
                },
                "langchain-openai": {
                  "category": "Uncategorized",
                  "description": "An integration package connecting OpenAI and LangChain",
                  "version": "0.1.25"
                },
                "langchain-text-splitters": {
                  "category": "Uncategorized",
                  "description": "LangChain text splitting utilities",
                  "version": "0.2.4"
                },
                "langsmith": {
                  "category": "Uncategorized",
                  "description": "Client library to connect to the LangSmith LLM Tracing and Evaluation Platform.",
                  "version": "0.1.125"
                },
                "libmambapy": {
                  "category": "Uncategorized",
                  "description": "Python bindings of libmamba",
                  "version": "1.5.3"
                },
                "litellm": {
                  "category": "Uncategorized",
                  "description": "Library to easily interface with LLM API providers",
                  "version": "1.47.1"
                },
                "llama-cloud": {
                  "category": "Uncategorized",
                  "description": "",
                  "version": "0.0.9"
                },
                "llama-index": {
                  "category": "Artificial Intelligence",
                  "description": "Interface between LLMs and your data",
                  "version": "0.10.56"
                },
                "llama-index-agent-openai": {
                  "category": "Uncategorized",
                  "description": "llama-index agent openai integration",
                  "version": "0.2.9"
                },
                "llama-index-cli": {
                  "category": "Uncategorized",
                  "description": "llama-index cli",
                  "version": "0.1.12"
                },
                "llama-index-core": {
                  "category": "Artificial Intelligence",
                  "description": "Interface between LLMs and your data",
                  "version": "0.10.56"
                },
                "llama-index-embeddings-openai": {
                  "category": "Uncategorized",
                  "description": "llama-index embeddings openai integration",
                  "version": "0.1.11"
                },
                "llama-index-indices-managed-llama-cloud": {
                  "category": "Uncategorized",
                  "description": "llama-index indices llama-cloud integration",
                  "version": "0.2.5"
                },
                "llama-index-legacy": {
                  "category": "Artificial Intelligence",
                  "description": "Interface between LLMs and your data",
                  "version": "0.9.48"
                },
                "llama-index-llms-openai": {
                  "category": "Uncategorized",
                  "description": "llama-index llms openai integration",
                  "version": "0.1.26"
                },
                "llama-index-multi-modal-llms-openai": {
                  "category": "Uncategorized",
                  "description": "llama-index multi-modal-llms openai integration",
                  "version": "0.1.8"
                },
                "llama-index-program-openai": {
                  "category": "Uncategorized",
                  "description": "llama-index program openai integration",
                  "version": "0.1.6"
                },
                "llama-index-question-gen-openai": {
                  "category": "Uncategorized",
                  "description": "llama-index question_gen openai integration",
                  "version": "0.1.3"
                },
                "llama-index-readers-file": {
                  "category": "Uncategorized",
                  "description": "llama-index readers file integration",
                  "version": "0.1.30"
                },
                "llama-index-readers-llama-parse": {
                  "category": "Uncategorized",
                  "description": "llama-index readers llama-parse integration",
                  "version": "0.1.6"
                },
                "llama-parse": {
                  "category": "Uncategorized",
                  "description": "Parse files into RAG-Optimized formats.",
                  "version": "0.4.9"
                },
                "llvmlite": {
                  "category": "Code Generators",
                  "description": "lightweight wrapper around basic LLVM functionality",
                  "version": "0.43.0"
                },
                "lxml": {
                  "category": "Markup :: HTML",
                  "description": "Powerful and Pythonic XML processing library combining libxml2/libxslt with the ElementTree API.",
                  "version": "5.3.0"
                },
                "markdown-it-py": {
                  "category": "Libraries :: Python Modules",
                  "description": "Python port of markdown-it. Markdown parsing, done right!",
                  "version": "3.0.0"
                },
                "marshmallow": {
                  "category": "Uncategorized",
                  "description": "A lightweight library for converting complex datatypes to and from native Python datatypes.",
                  "version": "3.21.3"
                },
                "matplotlib": {
                  "category": "Visualization",
                  "description": "Python plotting package",
                  "version": "3.8.2"
                },
                "matplotlib-inline": {
                  "category": "Uncategorized",
                  "description": "Inline Matplotlib backend for Jupyter",
                  "version": "0.1.6"
                },
                "mdurl": {
                  "category": "Libraries :: Python Modules",
                  "description": "Markdown URL utilities",
                  "version": "0.1.2"
                },
                "mem0ai": {
                  "category": "Uncategorized",
                  "description": "Long-term memory for AI Agents",
                  "version": "0.0.20"
                },
                "menuinst": {
                  "category": "Uncategorized",
                  "description": "cross platform install of menu items",
                  "version": "1.4.19"
                },
                "mergedeep": {
                  "category": "Uncategorized",
                  "description": "A deep merge function for \ud83d\udc0d.",
                  "version": "1.3.4"
                },
                "mkdocs": {
                  "category": "Documentation",
                  "description": "Project documentation with Markdown.",
                  "version": "1.5.3"
                },
                "mkdocs-callouts": {
                  "category": "Documentation",
                  "description": "A simple plugin that converts Obsidian style callouts and converts them into mkdocs supported 'admonitions' (a.k.a. callouts).",
                  "version": "1.10.0"
                },
                "mkl": {
                  "category": "Libraries",
                  "description": "Intel\u00ae oneAPI Math Kernel Library",
                  "version": "2021.4.0"
                },
                "mmh3": {
                  "category": "Libraries",
                  "description": "Python extension for MurmurHash (MurmurHash3), a set of fast and robust hash functions.",
                  "version": "5.0.0"
                },
                "molecule": {
                  "category": "Systems Administration",
                  "description": "Molecule aids in the development and testing of Ansible roles",
                  "version": "6.0.2"
                },
                "monotonic": {
                  "category": "Libraries :: Python Modules",
                  "description": "An implementation of time.monotonic() for Python 2 & < 3.3",
                  "version": "1.6"
                },
                "more-itertools": {
                  "category": "Libraries",
                  "description": "More routines for operating on iterables, beyond itertools",
                  "version": "10.3.0"
                },
                "moviepy": {
                  "category": "Multimedia",
                  "description": "Video editing with Python",
                  "version": "1.0.3"
                },
                "mpmath": {
                  "category": "Mathematics",
                  "description": "Python library for arbitrary-precision floating-point arithmetic",
                  "version": "1.3.0"
                },
                "msgpack": {
                  "category": "Uncategorized",
                  "description": "MessagePack serializer",
                  "version": "1.1.0"
                },
                "multidict": {
                  "category": "Uncategorized",
                  "description": "multidict implementation",
                  "version": "6.0.4"
                },
                "multitasking": {
                  "category": "Libraries",
                  "description": "Non-blocking Python methods using decorators",
                  "version": "0.0.11"
                },
                "mypy": {
                  "category": "Software Development",
                  "description": "Optional static typing for Python",
                  "version": "1.11.1"
                },
                "mypy-extensions": {
                  "category": "Software Development",
                  "description": "Type system extensions for programs checked with the mypy type checker.",
                  "version": "1.0.0"
                },
                "nest-asyncio": {
                  "category": "Uncategorized",
                  "description": "Patch asyncio to allow nested event loops",
                  "version": "1.5.8"
                },
                "networkx": {
                  "category": "Libraries :: Python Modules",
                  "description": "Python package for creating and manipulating graphs and networks",
                  "version": "3.2.1"
                },
                "nh3": {
                  "category": "Uncategorized",
                  "description": "Python bindings to the ammonia HTML sanitization library.",
                  "version": "0.2.18"
                },
                "nltk": {
                  "category": "Scientific/Engineering",
                  "description": "Natural Language Toolkit",
                  "version": "3.8.1"
                },
                "numba": {
                  "category": "Compilers",
                  "description": "compiling Python code using LLVM",
                  "version": "0.60.0"
                },
                "numpy": {
                  "category": "Software Development",
                  "description": "Fundamental package for array computing in Python",
                  "version": "1.26.2"
                },
                "oauthlib": {
                  "category": "Libraries :: Python Modules",
                  "description": "A generic, spec-compliant, thorough implementation of the OAuth request-signing logic",
                  "version": "3.2.2"
                },
                "ollama": {
                  "category": "Uncategorized",
                  "description": "The official Python client for Ollama.",
                  "version": "0.3.3"
                },
                "onnxruntime": {
                  "category": "Scientific/Engineering",
                  "description": "ONNX Runtime is a runtime accelerator for Machine Learning models",
                  "version": "1.19.2"
                },
                "openai": {
                  "category": "Libraries :: Python Modules",
                  "description": "The official Python library for the openai API",
                  "version": "1.47.0"
                },
                "openai-whisper": {
                  "category": "Uncategorized",
                  "description": "Robust Speech Recognition via Large-Scale Weak Supervision",
                  "version": "20231117"
                },
                "openbb": {
                  "category": "Uncategorized",
                  "description": "Investment research for everyone, anywhere.",
                  "version": "4.3.2"
                },
                "openbb-benzinga": {
                  "category": "Uncategorized",
                  "description": "Benzinga extension for OpenBB",
                  "version": "1.3.2"
                },
                "openbb-bls": {
                  "category": "Uncategorized",
                  "description": "The Bureau of Labor Statistics' (BLS) Public Data Application Programming Interface (API) gives the public access to economic data from all BLS programs. It is the Bureau's hope that talented developers and programmers will use the BLS Public Data API to create original, inventive applications with published BLS data.",
                  "version": "1.0.0"
                },
                "openbb-cftc": {
                  "category": "Uncategorized",
                  "description": "The mission of the Commodity Futures Trading Commission (CFTC) is to promote the integrity, resilience, and vibrancy of the U.S. derivatives markets through sound regulation.",
                  "version": "1.0.0"
                },
                "openbb-commodity": {
                  "category": "Uncategorized",
                  "description": "Commodity extension for OpenBB",
                  "version": "1.2.2"
                },
                "openbb-core": {
                  "category": "Uncategorized",
                  "description": "OpenBB package with core functionality.",
                  "version": "1.3.2"
                },
                "openbb-crypto": {
                  "category": "Uncategorized",
                  "description": "Crypto extension for OpenBB",
                  "version": "1.3.2"
                },
                "openbb-currency": {
                  "category": "Uncategorized",
                  "description": "Currency extension for OpenBB",
                  "version": "1.3.2"
                },
                "openbb-derivatives": {
                  "category": "Uncategorized",
                  "description": "Derivatives extension for OpenBB",
                  "version": "1.3.2"
                },
                "openbb-econdb": {
                  "category": "Uncategorized",
                  "description": "EconDB extension for OpenBB",
                  "version": "1.2.2"
                },
                "openbb-economy": {
                  "category": "Uncategorized",
                  "description": "Economy extension for OpenBB",
                  "version": "1.3.2"
                },
                "openbb-equity": {
                  "category": "Uncategorized",
                  "description": "Equity extension for OpenBB",
                  "version": "1.3.2"
                },
                "openbb-etf": {
                  "category": "Uncategorized",
                  "description": "ETF extension for OpenBB",
                  "version": "1.3.2"
                },
                "openbb-federal-reserve": {
                  "category": "Uncategorized",
                  "description": "US Federal Reserve Data Extension for OpenBB",
                  "version": "1.3.2"
                },
                "openbb-fixedincome": {
                  "category": "Uncategorized",
                  "description": "Fixed income extension for OpenBB",
                  "version": "1.3.2"
                },
                "openbb-fmp": {
                  "category": "Uncategorized",
                  "description": "FMP extension for OpenBB",
                  "version": "1.3.2"
                },
                "openbb-fred": {
                  "category": "Uncategorized",
                  "description": "FRED extension for OpenBB",
                  "version": "1.3.2"
                },
                "openbb-index": {
                  "category": "Uncategorized",
                  "description": "Index extension for OpenBB",
                  "version": "1.3.2"
                },
                "openbb-intrinio": {
                  "category": "Uncategorized",
                  "description": "Intrinio extension for OpenBB",
                  "version": "1.3.2"
                },
                "openbb-news": {
                  "category": "Uncategorized",
                  "description": "News extension for OpenBB",
                  "version": "1.3.2"
                },
                "openbb-oecd": {
                  "category": "Uncategorized",
                  "description": "OECD extension for OpenBB",
                  "version": "1.3.2"
                },
                "openbb-polygon": {
                  "category": "Uncategorized",
                  "description": "Polygon extension for OpenBB",
                  "version": "1.3.2"
                },
                "openbb-regulators": {
                  "category": "Uncategorized",
                  "description": "Markets and Agency Regulators extension for OpenBB",
                  "version": "1.3.2"
                },
                "openbb-sec": {
                  "category": "Uncategorized",
                  "description": "SEC extension for OpenBB",
                  "version": "1.3.2"
                },
                "openbb-tiingo": {
                  "category": "Uncategorized",
                  "description": "Tiingo extension for OpenBB",
                  "version": "1.3.2"
                },
                "openbb-tradingeconomics": {
                  "category": "Uncategorized",
                  "description": "Trading Economics extension for OpenBB",
                  "version": "1.3.2"
                },
                "openbb-yfinance": {
                  "category": "Uncategorized",
                  "description": "yfinance extension for OpenBB",
                  "version": "1.3.3"
                },
                "opentelemetry-api": {
                  "category": "Uncategorized",
                  "description": "OpenTelemetry Python API",
                  "version": "1.27.0"
                },
                "opentelemetry-exporter-otlp-proto-common": {
                  "category": "Uncategorized",
                  "description": "OpenTelemetry Protobuf encoding",
                  "version": "1.27.0"
                },
                "opentelemetry-exporter-otlp-proto-grpc": {
                  "category": "Uncategorized",
                  "description": "OpenTelemetry Collector Protobuf over gRPC Exporter",
                  "version": "1.27.0"
                },
                "opentelemetry-exporter-otlp-proto-http": {
                  "category": "Uncategorized",
                  "description": "OpenTelemetry Collector Protobuf over HTTP Exporter",
                  "version": "1.27.0"
                },
                "opentelemetry-instrumentation": {
                  "category": "Uncategorized",
                  "description": "Instrumentation Tools & Auto Instrumentation for OpenTelemetry Python",
                  "version": "0.48b0"
                },
                "opentelemetry-instrumentation-asgi": {
                  "category": "Uncategorized",
                  "description": "ASGI instrumentation for OpenTelemetry",
                  "version": "0.48b0"
                },
                "opentelemetry-instrumentation-fastapi": {
                  "category": "Uncategorized",
                  "description": "OpenTelemetry FastAPI Instrumentation",
                  "version": "0.48b0"
                },
                "opentelemetry-proto": {
                  "category": "Uncategorized",
                  "description": "OpenTelemetry Python Proto",
                  "version": "1.27.0"
                },
                "opentelemetry-sdk": {
                  "category": "Uncategorized",
                  "description": "OpenTelemetry Python SDK",
                  "version": "1.27.0"
                },
                "opentelemetry-semantic-conventions": {
                  "category": "Uncategorized",
                  "description": "OpenTelemetry Semantic Conventions",
                  "version": "0.48b0"
                },
                "opentelemetry-util-http": {
                  "category": "Uncategorized",
                  "description": "Web util for OpenTelemetry",
                  "version": "0.48b0"
                },
                "orjson": {
                  "category": "Uncategorized",
                  "description": "Fast, correct Python JSON library supporting dataclasses, datetimes, and numpy",
                  "version": "3.10.7"
                },
                "overrides": {
                  "category": "Uncategorized",
                  "description": "A decorator to automatically detect mismatch when overriding a method.",
                  "version": "7.7.0"
                },
                "packaging": {
                  "category": "Uncategorized",
                  "description": "Core utilities for Python packages",
                  "version": "23.2"
                },
                "pandas": {
                  "category": "Scientific/Engineering",
                  "description": "Powerful data structures for data analysis, time series, and statistics",
                  "version": "2.2.2"
                },
                "parameterized": {
                  "category": "Uncategorized",
                  "description": "Parameterized testing with any Python test framework",
                  "version": "0.9.0"
                },
                "paramiko": {
                  "category": "Internet",
                  "description": "SSH2 protocol library",
                  "version": "3.4.0"
                },
                "parso": {
                  "category": "Libraries :: Python Modules",
                  "description": "A Python Parser",
                  "version": "0.8.3"
                },
                "pathlib": {
                  "category": "Libraries",
                  "description": "Object-oriented filesystem paths",
                  "version": "1.0.1"
                },
                "pathspec": {
                  "category": "Libraries :: Python Modules",
                  "description": "Utility library for gitignore style pattern matching of file paths.",
                  "version": "0.12.1"
                },
                "pdf2image": {
                  "category": "Uncategorized",
                  "description": "A wrapper around the pdftoppm and pdftocairo command line tools to convert PDF to a PIL Image list.",
                  "version": "1.17.0"
                },
                "pdfminer.six": {
                  "category": "Text Processing",
                  "description": "PDF parser and analyzer",
                  "version": "20231228"
                },
                "pdfplumber": {
                  "category": "Uncategorized",
                  "description": "Plumb a PDF for detailed information about each char, rectangle, and line.",
                  "version": "0.11.4"
                },
                "peewee": {
                  "category": "Database",
                  "description": "a little orm",
                  "version": "3.16.3"
                },
                "pexpect": {
                  "category": "Software Development",
                  "description": "Pexpect allows easy control of interactive console applications.",
                  "version": "4.9.0"
                },
                "pikepdf": {
                  "category": "Graphics",
                  "description": "Read and write PDFs with Python, powered by qpdf",
                  "version": "9.2.1"
                },
                "pip": {
                  "category": "Build Tools",
                  "description": "The PyPA recommended tool for installing Python packages.",
                  "version": "23.3.1"
                },
                "pipx": {
                  "category": "Uncategorized",
                  "description": "Install and Run Python Applications in Isolated Environments",
                  "version": "1.4.3"
                },
                "pkginfo": {
                  "category": "Libraries :: Python Modules",
                  "description": "Query metadata from sdists / bdists / installed packages.",
                  "version": "1.10.0"
                },
                "platformdirs": {
                  "category": "Libraries :: Python Modules",
                  "description": "A small Python package for determining appropriate platform-specific dirs, e.g. a \"user data dir\".",
                  "version": "4.0.0"
                },
                "pluggy": {
                  "category": "Testing",
                  "description": "plugin and hook calling mechanisms for python",
                  "version": "1.5.0"
                },
                "poetry": {
                  "category": "Build Tools",
                  "description": "Python dependency management and packaging made easy.",
                  "version": "1.8.3"
                },
                "poetry-core": {
                  "category": "Build Tools",
                  "description": "Poetry PEP 517 Build Backend",
                  "version": "1.9.0"
                },
                "poetry-plugin-export": {
                  "category": "Uncategorized",
                  "description": "Poetry plugin to export the dependencies to various formats",
                  "version": "1.8.0"
                },
                "portalocker": {
                  "category": "Testing",
                  "description": "Wraps the portalocker recipe for easy usage",
                  "version": "2.10.1"
                },
                "posthog": {
                  "category": "Uncategorized",
                  "description": "Integrate PostHog into any python application.",
                  "version": "3.6.6"
                },
                "proglog": {
                  "category": "Uncategorized",
                  "description": "Log and progress bar manager for console, notebooks, web...",
                  "version": "0.1.10"
                },
                "prompt-toolkit": {
                  "category": "Software Development",
                  "description": "Library for building powerful interactive command lines in Python",
                  "version": "3.0.41"
                },
                "proto-plus": {
                  "category": "Code Generators",
                  "description": "Beautiful, Pythonic protocol buffers.",
                  "version": "1.24.0"
                },
                "protobuf": {
                  "category": "Uncategorized",
                  "description": "No description found",
                  "version": "4.25.5"
                },
                "psgfiglet": {
                  "category": "Graphics",
                  "description": "Create Figlets using a PySimpleGUI GUI and pyfiglet. A PySimpleGUI Demo Program.",
                  "version": "5.0.0"
                },
                "psutil": {
                  "category": "Libraries :: Python Modules",
                  "description": "Cross-platform lib for process and system monitoring in Python.",
                  "version": "5.9.8"
                },
                "ptyprocess": {
                  "category": "Terminals",
                  "description": "Run a subprocess in a pseudo terminal",
                  "version": "0.7.0"
                },
                "pulsar-client": {
                  "category": "Uncategorized",
                  "description": "Apache Pulsar Python client library",
                  "version": "3.5.0"
                },
                "pure-eval": {
                  "category": "Uncategorized",
                  "description": "Safely evaluate AST nodes without side effects",
                  "version": "0.2.2"
                },
                "pyOpenSSL": {
                  "category": "Cryptography",
                  "description": "Python wrapper module around the OpenSSL library",
                  "version": "23.2.0"
                },
                "pyasn1": {
                  "category": "Communications",
                  "description": "Pure-Python implementation of ASN.1 types and DER/BER/CER codecs (X.208)",
                  "version": "0.6.0"
                },
                "pyasn1_modules": {
                  "category": "Communications",
                  "description": "A collection of ASN.1-based protocols modules",
                  "version": "0.4.1"
                },
                "pycosat": {
                  "category": "Utilities",
                  "description": "bindings to picosat (a SAT solver)",
                  "version": "0.6.6"
                },
                "pycparser": {
                  "category": "Uncategorized",
                  "description": "C parser in Python",
                  "version": "2.21"
                },
                "pydantic": {
                  "category": "Internet",
                  "description": "Data validation using Python type hints",
                  "version": "2.9.1"
                },
                "pydantic_core": {
                  "category": "Uncategorized",
                  "description": "Core functionality for Pydantic validation and serialization",
                  "version": "2.23.3"
                },
                "pydub": {
                  "category": "Sound/Audio",
                  "description": "Manipulate audio with an simple and easy high level interface",
                  "version": "0.25.1"
                },
                "pyfiglet": {
                  "category": "Text Processing",
                  "description": "Pure-python FIGlet implementation",
                  "version": "1.0.2"
                },
                "pyparsing": {
                  "category": "Compilers",
                  "description": "pyparsing module - Classes and methods to define and execute parsing grammars",
                  "version": "3.1.1"
                },
                "pypdf": {
                  "category": "Libraries :: Python Modules",
                  "description": "A pure-python PDF library capable of splitting, merging, cropping, and transforming PDF files",
                  "version": "4.3.0"
                },
                "pypdfium2": {
                  "category": "Graphics",
                  "description": "Python bindings to PDFium",
                  "version": "4.30.0"
                },
                "pyproject_hooks": {
                  "category": "Uncategorized",
                  "description": "Wrappers to call pyproject.toml-based build backend hooks.",
                  "version": "1.1.0"
                },
                "pyreadline3": {
                  "category": "Uncategorized",
                  "description": "A python implementation of GNU readline.",
                  "version": "3.4.1"
                },
                "pysbd": {
                  "category": "Scientific/Engineering",
                  "description": "pysbd (Python Sentence Boundary Disambiguation) is a rule-based sentence boundary detection that works out-of-the-box across many languages.",
                  "version": "0.3.4"
                },
                "pytesseract": {
                  "category": "Uncategorized",
                  "description": "Python-tesseract is a python wrapper for Google's Tesseract-OCR",
                  "version": "0.3.13"
                },
                "pytest": {
                  "category": "Libraries",
                  "description": "pytest: simple powerful testing with Python",
                  "version": "8.3.2"
                },
                "python-crontab": {
                  "category": "Uncategorized",
                  "description": "Python Crontab API",
                  "version": "3.2.0"
                },
                "python-dateutil": {
                  "category": "Libraries",
                  "description": "Extensions to the standard Python datetime module",
                  "version": "2.8.2"
                },
                "python-decouple": {
                  "category": "Libraries",
                  "description": "Strict separation of settings from code.",
                  "version": "3.8"
                },
                "python-dotenv": {
                  "category": "Systems Administration",
                  "description": "Read key-value pairs from a .env file and set them as environment variables",
                  "version": "1.0.0"
                },
                "python-editor": {
                  "category": "Libraries",
                  "description": "Programmatically open an editor, capture the result.",
                  "version": "1.0.4"
                },
                "python-lsp-jsonrpc": {
                  "category": "Uncategorized",
                  "description": "JSON RPC 2.0 server library",
                  "version": "1.0.0"
                },
                "python-multipart": {
                  "category": "Libraries :: Python Modules",
                  "description": "A streaming multipart parser for Python",
                  "version": "0.0.7"
                },
                "pytube": {
                  "category": "Internet",
                  "description": "Python 3 library for downloading YouTube Videos.",
                  "version": "15.0.0"
                },
                "pytz": {
                  "category": "Libraries :: Python Modules",
                  "description": "World timezone definitions, modern and historical",
                  "version": "2024.1"
                },
                "pyvis": {
                  "category": "Uncategorized",
                  "description": "A Python network graph visualization library",
                  "version": "0.3.2"
                },
                "pywin32": {
                  "category": "Uncategorized",
                  "description": "Python for Window Extensions",
                  "version": "306"
                },
                "pywin32-ctypes": {
                  "category": "Uncategorized",
                  "description": "A (partial) reimplementation of pywin32 using ctypes/cffi",
                  "version": "0.2.2"
                },
                "pyyaml_env_tag": {
                  "category": "Libraries :: Python Modules",
                  "description": "A custom YAML tag for referencing environment variables in YAML files. ",
                  "version": "0.1"
                },
                "pyzmq": {
                  "category": "Networking",
                  "description": "Python bindings for 0MQ",
                  "version": "25.1.1"
                },
                "qdrant-client": {
                  "category": "Uncategorized",
                  "description": "Client library for the Qdrant vector search engine",
                  "version": "1.10.1"
                },
                "questionary": {
                  "category": "Libraries",
                  "description": "Python library to build pretty command line user prompts \u2b50\ufe0f",
                  "version": "2.0.1"
                },
                "rapidfuzz": {
                  "category": "Uncategorized",
                  "description": "rapid fuzzy string matching",
                  "version": "3.9.7"
                },
                "readchar": {
                  "category": "Software Development",
                  "description": "Library to easily read single chars and key strokes",
                  "version": "4.0.5"
                },
                "readme_renderer": {
                  "category": "Uncategorized",
                  "description": "readme_renderer is a library for rendering readme descriptions for Warehouse",
                  "version": "44.0"
                },
                "referencing": {
                  "category": "JSON",
                  "description": "JSON Referencing + Python",
                  "version": "0.31.1"
                },
                "regex": {
                  "category": "Information Analysis",
                  "description": "Alternative regular expression module, to replace re.",
                  "version": "2024.9.11"
                },
                "requests": {
                  "category": "WWW/HTTP",
                  "description": "Python HTTP for Humans.",
                  "version": "2.32.3"
                },
                "requests-cache": {
                  "category": "Libraries :: Python Modules",
                  "description": "A persistent cache for python requests",
                  "version": "1.2.1"
                },
                "requests-oauthlib": {
                  "category": "Uncategorized",
                  "description": "OAuthlib authentication support for Requests.",
                  "version": "2.0.0"
                },
                "requests-toolbelt": {
                  "category": "Uncategorized",
                  "description": "A utility belt for advanced users of python-requests",
                  "version": "1.0.0"
                },
                "resolvelib": {
                  "category": "Libraries :: Python Modules",
                  "description": "Resolve abstract dependencies into concrete ones",
                  "version": "1.0.1"
                },
                "rfc3986": {
                  "category": "Uncategorized",
                  "description": "Validating URI References per RFC 3986",
                  "version": "2.0.0"
                },
                "rich": {
                  "category": "Uncategorized",
                  "description": "Render rich text, tables, progress bars, syntax highlighting, markdown and more to the terminal",
                  "version": "13.7.0"
                },
                "rpds-py": {
                  "category": "Uncategorized",
                  "description": "Python bindings to Rust's persistent data structures (rpds)",
                  "version": "0.13.2"
                },
                "rsa": {
                  "category": "Cryptography",
                  "description": "Pure-Python RSA implementation",
                  "version": "4.9"
                },
                "ruamel.yaml": {
                  "category": "Libraries :: Python Modules",
                  "description": "ruamel.yaml is a YAML parser/emitter that supports roundtrip preservation of comments, seq/map flow style, and map key order",
                  "version": "0.17.21"
                },
                "ruff": {
                  "category": "Libraries :: Python Modules",
                  "description": "An extremely fast Python linter and code formatter, written in Rust.",
                  "version": "0.6.8"
                },
                "s3transfer": {
                  "category": "Uncategorized",
                  "description": "An Amazon S3 Transfer Manager",
                  "version": "0.10.2"
                },
                "schema": {
                  "category": "Utilities",
                  "description": "Simple data validation library",
                  "version": "0.7.7"
                },
                "scipy": {
                  "category": "Libraries",
                  "description": "Fundamental algorithms for scientific computing in Python",
                  "version": "1.11.4"
                },
                "scp": {
                  "category": "Internet",
                  "description": "scp module for paramiko",
                  "version": "0.15.0"
                },
                "semgrep": {
                  "category": "Security",
                  "description": "Lightweight static analysis for many languages. Find bug variants with patterns that look like source code.",
                  "version": "1.43.0"
                },
                "setuptools": {
                  "category": "Libraries :: Python Modules",
                  "description": "Easily download, build, install, upgrade, and uninstall Python packages",
                  "version": "73.0.1"
                },
                "shapely": {
                  "category": "GIS",
                  "description": "Manipulation and analysis of geometric objects",
                  "version": "2.0.6"
                },
                "shellingham": {
                  "category": "Libraries :: Python Modules",
                  "description": "Tool to Detect Surrounding Shell",
                  "version": "1.5.4"
                },
                "six": {
                  "category": "Libraries",
                  "description": "Python 2 and 3 compatibility utilities",
                  "version": "1.16.0"
                },
                "smmap": {
                  "category": "Uncategorized",
                  "description": "A pure Python implementation of a sliding window memory map manager",
                  "version": "5.0.1"
                },
                "sniffio": {
                  "category": "Uncategorized",
                  "description": "Sniff out which async library your code is running under",
                  "version": "1.3.0"
                },
                "sounddevice": {
                  "category": "Sound/Audio",
                  "description": "Play and Record Sound with Python",
                  "version": "0.4.6"
                },
                "soundfile": {
                  "category": "Sound/Audio",
                  "description": "An audio library based on libsndfile, CFFI and NumPy",
                  "version": "0.12.1"
                },
                "soupsieve": {
                  "category": "WWW/HTTP :: Dynamic Content",
                  "description": "A modern CSS selector implementation for Beautiful Soup.",
                  "version": "2.5"
                },
                "spellcaster": {
                  "category": "Uncategorized",
                  "description": "An open-source tool that leverages AI agents to enhance the quality of your codebase by scanning repositories for grammar, spelling, and code example errors in documentation files.",
                  "version": "0.0.7"
                },
                "sseclient-py": {
                  "category": "Uncategorized",
                  "description": "SSE client for Python",
                  "version": "1.8.0"
                },
                "stack-data": {
                  "category": "Debuggers",
                  "description": "Extract data from python stack frames and tracebacks for informative displays",
                  "version": "0.6.3"
                },
                "starlette": {
                  "category": "WWW/HTTP",
                  "description": "The little ASGI library that shines.",
                  "version": "0.37.2"
                },
                "stashapp-tools": {
                  "category": "Uncategorized",
                  "description": "A python library for interfacing with a stashapp's API",
                  "version": "0.2.40"
                },
                "striprtf": {
                  "category": "Uncategorized",
                  "description": "A simple library to convert rtf to text",
                  "version": "0.0.26"
                },
                "subprocess-tee": {
                  "category": "Systems Administration",
                  "description": "subprocess-tee",
                  "version": "0.4.1"
                },
                "sympy": {
                  "category": "Scientific/Engineering",
                  "description": "Computer algebra system (CAS) in Python",
                  "version": "1.13.1"
                },
                "tabulate": {
                  "category": "Libraries",
                  "description": "Pretty-print tabular data",
                  "version": "0.9.0"
                },
                "tbb": {
                  "category": "Libraries",
                  "description": "Intel\u00ae oneAPI Threading Building Blocks",
                  "version": "2021.13.0"
                },
                "tenacity": {
                  "category": "Utilities",
                  "description": "Retry code until it succeeds",
                  "version": "8.5.0"
                },
                "termcolor": {
                  "category": "Terminals",
                  "description": "ANSI color formatting for output in terminal",
                  "version": "2.4.0"
                },
                "tiktoken": {
                  "category": "Uncategorized",
                  "description": "tiktoken is a fast BPE tokeniser for use with OpenAI's models",
                  "version": "0.7.0"
                },
                "tokencost": {
                  "category": "Uncategorized",
                  "description": "To calculate token and translated USD cost of string and message calls to OpenAI, for example when used by AI agents",
                  "version": "0.1.12"
                },
                "tokenizers": {
                  "category": "Artificial Intelligence",
                  "description": "No description found",
                  "version": "0.20.0"
                },
                "tokentrim": {
                  "category": "Uncategorized",
                  "description": "Easily trim 'messages' arrays for use with GPTs.",
                  "version": "0.1.13"
                },
                "toml": {
                  "category": "Uncategorized",
                  "description": "Python Library for Tom's Obvious, Minimal Language",
                  "version": "0.10.2"
                },
                "tomli": {
                  "category": "Libraries :: Python Modules",
                  "description": "A lil' TOML parser",
                  "version": "2.0.1"
                },
                "tomlkit": {
                  "category": "Uncategorized",
                  "description": "Style preserving TOML library",
                  "version": "0.13.2"
                },
                "torch": {
                  "category": "Scientific/Engineering",
                  "description": "Tensors and Dynamic neural networks in Python with strong GPU acceleration",
                  "version": "2.3.1"
                },
                "tornado": {
                  "category": "Uncategorized",
                  "description": "Tornado is a Python web framework and asynchronous networking library, originally developed at FriendFeed.",
                  "version": "6.3.3"
                },
                "tqdm": {
                  "category": "Desktop Environment",
                  "description": "Fast, Extensible Progress Meter",
                  "version": "4.66.5"
                },
                "traitlets": {
                  "category": "Uncategorized",
                  "description": "Traitlets Python configuration system",
                  "version": "5.13.0"
                },
                "tree-sitter": {
                  "category": "Compilers",
                  "description": "Python bindings for the Tree-Sitter parsing library",
                  "version": "0.20.4"
                },
                "tree-sitter-languages": {
                  "category": "Uncategorized",
                  "description": "Binary Python wheels for all tree sitter languages.",
                  "version": "1.8.0"
                },
                "trove-classifiers": {
                  "category": "Uncategorized",
                  "description": "Canonical source for classifiers on PyPI (pypi.org).",
                  "version": "2024.9.12"
                },
                "truststore": {
                  "category": "Uncategorized",
                  "description": "Verify certificates using native system trust stores",
                  "version": "0.8.0"
                },
                "twine": {
                  "category": "Uncategorized",
                  "description": "Collection of utilities for publishing packages on PyPI",
                  "version": "5.1.1"
                },
                "typer": {
                  "category": "Libraries :: Application Frameworks",
                  "description": "Typer, build great CLIs. Easy to code. Based on Python type hints.",
                  "version": "0.12.5"
                },
                "types-requests": {
                  "category": "Uncategorized",
                  "description": "Typing stubs for requests",
                  "version": "2.32.0.20240914"
                },
                "typing-inspect": {
                  "category": "Software Development",
                  "description": "Runtime inspection utilities for typing module.",
                  "version": "0.9.0"
                },
                "typing_extensions": {
                  "category": "Software Development",
                  "description": "Backported and Experimental Type Hints for Python 3.8+",
                  "version": "4.12.2"
                },
                "tzdata": {
                  "category": "Uncategorized",
                  "description": "Provider of IANA time zone data",
                  "version": "2024.1"
                },
                "ujson": {
                  "category": "Uncategorized",
                  "description": "Ultra fast JSON encoder and decoder for Python",
                  "version": "5.8.0"
                },
                "url-normalize": {
                  "category": "Uncategorized",
                  "description": "URL normalization for Python",
                  "version": "1.4.3"
                },
                "urllib3": {
                  "category": "WWW/HTTP",
                  "description": "HTTP library with thread-safe connection pooling, file post, and more.",
                  "version": "2.1.0"
                },
                "userpath": {
                  "category": "Uncategorized",
                  "description": "Cross-platform tool for adding locations to the user PATH",
                  "version": "1.9.1"
                },
                "uuid7": {
                  "category": "Uncategorized",
                  "description": "UUID version 7, generating time-sorted UUIDs with 200ns time resolution and 48 bits of randomness",
                  "version": "0.1.0"
                },
                "uvicorn": {
                  "category": "WWW/HTTP",
                  "description": "The lightning-fast ASGI server.",
                  "version": "0.24.0.post1"
                },
                "virtualenv": {
                  "category": "Libraries",
                  "description": "Virtual Python Environment builder",
                  "version": "20.26.5"
                },
                "watchdog": {
                  "category": "Libraries",
                  "description": "Filesystem events monitoring",
                  "version": "4.0.1"
                },
                "watchfiles": {
                  "category": "Libraries :: Python Modules",
                  "description": "Simple, modern and high performance file watching and code reload in python.",
                  "version": "0.24.0"
                },
                "wcmatch": {
                  "category": "Libraries :: Python Modules",
                  "description": "Wildcard/glob file name matcher.",
                  "version": "8.5"
                },
                "wcwidth": {
                  "category": "Libraries",
                  "description": "Measures the displayed width of unicode strings in a terminal",
                  "version": "0.2.12"
                },
                "webencodings": {
                  "category": "WWW/HTTP",
                  "description": "Character encoding aliases for legacy web content",
                  "version": "0.5.1"
                },
                "websocket-client": {
                  "category": "Internet",
                  "description": "WebSocket client for Python with low level API options",
                  "version": "1.6.4"
                },
                "websockets": {
                  "category": "Uncategorized",
                  "description": "An implementation of the WebSocket Protocol (RFC 6455 & 7692)",
                  "version": "12.0"
                },
                "wget": {
                  "category": "Libraries :: Python Modules",
                  "description": "pure python download utility",
                  "version": "3.2"
                },
                "wheel": {
                  "category": "Archiving :: Packaging",
                  "description": "A built-package format for Python",
                  "version": "0.41.2"
                },
                "whisper": {
                  "category": "Uncategorized",
                  "description": "Fixed size round-robin style database",
                  "version": "1.1.10"
                },
                "win-inet-pton": {
                  "category": "Utilities",
                  "description": "Native inet_pton and inet_ntop implementation for Python on Windows (with ctypes).",
                  "version": "1.1.0"
                },
                "wrapt": {
                  "category": "Uncategorized",
                  "description": "Module for decorators, wrappers and monkey patching.",
                  "version": "1.16.0"
                },
                "xmltodict": {
                  "category": "Markup :: XML",
                  "description": "Makes working with XML feel like you are working with JSON",
                  "version": "0.13.0"
                },
                "yamllint": {
                  "category": "Software Development",
                  "description": "A linter for YAML files.",
                  "version": "1.32.0"
                },
                "yarl": {
                  "category": "WWW/HTTP",
                  "description": "Yet another URL library",
                  "version": "1.9.2"
                },
                "yaspin": {
                  "category": "Software Development",
                  "description": "Yet Another Terminal Spinner",
                  "version": "3.0.2"
                },
                "yfinance": {
                  "category": "Financial",
                  "description": "Download market data from Yahoo! Finance API",
                  "version": "0.2.43"
                },
                "zipp": {
                  "category": "Uncategorized",
                  "description": "Backport of pathlib-compatible object wrapper for zip files",
                  "version": "3.17.0"
                },
                "zstandard": {
                  "category": "Uncategorized",
                  "description": "Zstandard bindings for Python",
                  "version": "0.19.0"
                }
              }
            ]]></content>
          </file>
          <file name="python_package_info.py" path="wip/packages/python_package_info.py">
            <content><![CDATA[
              """
              This module provides functions to retrieve and categorize information
              about installed Python packages.
              """

              import json
              from collections import defaultdict
              import importlib.metadata
              from typing import Dict, Any


              def get_package_info() -> Dict[str, Dict[str, Any]]:
                  """
                  Retrieves information about all installed packages using importlib.metadata.

                  Returns:
                      Dict[str, Dict[str, Any]]: A dictionary containing package information.
                  """
                  package_info = {}
                  for dist in importlib.metadata.distributions():
                      try:
                          metadata = dist.metadata
                          name = metadata["Name"]
                          description = metadata["Summary"] if "Summary" in metadata else "No description found"
                          classifiers = metadata.get_all("Classifier", [])
                          category = next(
                              (c.split(" :: ", 2)[-1] for c in classifiers if c.startswith("Topic :: ")),
                              "Uncategorized",
                          )
                          package_info[name] = {
                              "version": dist.version,
                              "description": description,
                              "category": category,
                          }
                      except (KeyError, AttributeError) as e:
                          print(f"Error processing {dist.name}: {str(e)}")
                          package_info[dist.name] = {
                              "version": dist.version,
                              "description": "Unable to fetch description",
                              "category": "Uncategorized",
                          }
                  return package_info


              def categorize_packages(package_info: Dict[str, Dict[str, Any]]) -> Dict[str, list]:
                  """
                  Categorizes packages based on their category information.

                  Args:
                      package_info (Dict[str, Dict[str, Any]]): A dictionary containing package information.

                  Returns:
                      Dict[str, list]: A dictionary with categories as keys and lists of package names as values.
                  """
                  categories = defaultdict(list)
                  for name, info in package_info.items():
                      categories[info["category"]].append(name)
                  return dict(categories)


              def main() -> None:
                  """
                  Main function to retrieve package information, categorize packages,
                  and save results to JSON files.
                  """
                  package_info = get_package_info()
                  categorized_packages = categorize_packages(package_info)

                  with open("package_info.json", "w", encoding="utf-8") as f:
                      json.dump(package_info, f, indent=2, sort_keys=True)

                  with open("package_categories.json", "w", encoding="utf-8") as f:
                      json.dump(categorized_packages, f, indent=2, sort_keys=True)

                  print("Package information saved to 'package_info.json'")
                  print("Package categories saved to 'package_categories.json'")


              if __name__ == "__main__":
                  main()

            ]]></content>
          </file>
        </directory>
        <directory name="spreadsheets" path="wip/spreadsheets">
          <file name="manual_paper_ratings.csv" path="wip/spreadsheets/manual_paper_ratings.csv">
            <content><![CDATA[
              paper_id,Summary,Questions,Limitations,Ethical Concerns,Soundness,Presentation,Contribution,Overall,Confidence,Strengths,Weaknesses,Originality,Quality,Clarity,Significance,Decision
              adaptive_dual_scale_denoising,"The paper introduces an adaptive dual-scale denoising approach for low-dimensional diffusion models, aiming to balance global structure and local details in generated samples. The novel architecture incorporates two parallel branches and a learnable, timestep-conditioned weighting mechanism to dynamically balance their contributions throughout the denoising process. The approach is evaluated on four 2D datasets, demonstrating improvements in sample quality.","['Can you provide a more detailed theoretical justification for the dual-scale architecture?', ""What impact do different types of aggregators have on the model's performance?"", 'How does the model perform on more complex, real-world low-dimensional datasets?', 'Can the computational cost be reduced without sacrificing performance?']","['The paper should address the high computational cost and explore ways to optimize it.', 'The limited diversity of datasets and lack of detailed theoretical backing for the proposed architecture are notable limitations.']",False,3,3,3,5,4,"['Novel approach to balancing global and local features in diffusion models for low-dimensional data.', 'Comprehensive empirical evaluation on multiple 2D datasets.', 'Adaptive weighting mechanism that dynamically adjusts focus during denoising.']","['Lacks detailed theoretical justification for the dual-scale architecture.', 'Computational cost is significantly higher, which may limit practical applicability.', 'Some sections are not clearly explained, such as the autoencoder aggregator and weight evolution analysis.', 'Limited diversity in the datasets used for evaluation. More complex, real-world datasets could strengthen claims.', 'Insufficient ablation studies and analysis on specific design choices like different types of aggregators.']",4,3,3,3,Reject
              layerwise_lr_grokking,"The paper proposes a novel layer-wise learning rate strategy to accelerate and enhance the grokking phenomenon in Transformer models. The approach involves assigning different learning rates to the embedding layers, lower Transformer layers, and higher Transformer layers. The method is empirically validated on algorithmic tasks such as modular arithmetic and permutations, showing significant improvements in convergence speed and final performance.","['Can the authors provide more detailed explanations of the hyperparameter tuning process and the exact implementation of the layer-wise learning rates?', 'How do the authors ensure that the proposed method generalizes to tasks beyond the algorithmic ones tested in the paper?', 'Can the authors compare their approach with other related methods in more detail?', 'Can you provide more theoretical insights into why layer-wise learning rates specifically facilitate grokking?', 'How were the specific learning rates chosen for embedding, lower, and higher layers?', 'Can you discuss the potential for overfitting and how it was mitigated?', 'Have you tested the robustness of your method across different datasets and larger model sizes?', 'What is the impact of different learning rate configurations on the results?', 'Can the authors discuss potential strategies for mitigating the need for careful tuning of learning rates to avoid instability?']","['The methodology lacks detailed clarity, and the authors do not provide sufficient information on the hyperparameter tuning process.', 'The scope of tasks is limited to algorithmic ones, and the generalizability of the findings is unclear.', 'The paper requires more theoretical backing for the proposed method.', 'The choice of specific learning rates and potential overfitting issues need to be addressed in more detail.', 'The scalability of the approach to larger models and more complex tasks is not thoroughly addressed.', 'Ethical concerns related to the potential misuse of accelerated learning techniques are not addressed.']",False,2,2,3,4,4,"['The paper addresses an important problem in deep learning: the grokking phenomenon.', 'The proposed layer-wise learning rate strategy is novel and shows significant improvements in experimental results.', 'Experiments demonstrate substantial improvements in both convergence speed and final performance.']","['The paper lacks detailed methodological clarity, particularly regarding the exact implementation of the layer-wise learning rates and hyperparameter tuning.', 'The theoretical explanation for why layer-wise learning rates work is insufficient.', 'The scope of tasks is limited to algorithmic ones, making it unclear how well the findings generalize to other domains.', 'The choice of learning rates seems arbitrary and lacks justification.', 'More comprehensive ablation studies and comparisons with other related methods would strengthen the paper.', 'Certain sections, such as the experimental setup and ablation studies, could be more detailed and clearer.']",3,2,3,3,Reject
              multi_style_adapter,"The paper introduces the Multi-Style Adapter, which enhances style awareness and consistency in character-level language models by integrating learnable style embeddings, a style classification head, and a StyleAdapter module into the GPT architecture. The approach aims to balance style adaptation and language modeling capabilities, and demonstrates improved style consistency and competitive validation losses across multiple datasets.","['How does the model handle unseen styles during inference?', 'Can the authors provide more details on the training process and hyperparameter tuning?', ""What are the potential impacts of overfitting on the model's ability to generate diverse text within each style?"", 'Can the authors provide more detailed ablation studies, especially focusing on the impact of different components in the Multi-Style Adapter?', 'How does the Multi-Style Adapter perform compared to other recent style-transfer models?', 'Can the computational efficiency trade-offs be quantified in a more detailed manner?', ""Can the authors clarify the autoencoder aggregator's role and how it integrates with the rest of the model?"", 'What measures have been taken to ensure the model does not overfit to specific style patterns, especially given the perfect consistency scores on some datasets?', 'Are there any potential optimization techniques that could be explored to improve the computational efficiency of the Multi-Style Adapter?', 'How does the model handle cases where the input sequence contains mixed styles?', 'Could you provide more qualitative examples of generated text to demonstrate the style consistency?', 'What is the impact of reducing the number of gating parameters in the modulation function?']","[""The reduced inference speed and potential overfitting to specific style patterns are significant limitations. Future work should focus on optimizing computational efficiency and improving the model's ability to generalize to diverse styles."", 'The paper currently lacks sufficient ablation studies and additional baselines.', ""The model's performance may be sensitive to hyperparameter settings, such as the weight of the style loss and the frequency of StyleAdapter application.""]",False,3,3,3,5,4,"['The paper presents a novel approach to style-aware language modeling, addressing a critical need for fine-grained stylistic control.', 'The Multi-Style Adapter is well-motivated and integrates seamlessly with the GPT architecture.', 'Extensive experiments on diverse datasets demonstrate improved style consistency and validation loss.', 'The paper includes thorough analysis and visualization of learned style embeddings and attention patterns.']","['The model achieves perfect style consistency scores on some datasets, which may indicate overfitting to specific style patterns.', 'The reduced inference speed (approximately 40% slower than the baseline) may limit the practical applicability of the model.', 'The paper could explore more sophisticated style representation techniques and evaluate their impact.', 'Lack of detailed ablation studies and additional baselines to strengthen the claims.', 'Clarity of the autoencoder aggregator mechanism could be enhanced.']",3,3,3,3,Reject
              rl_lr_adaptation,"The paper explores the application of Q-learning to dynamically adjust the learning rate during transformer model training, aiming to enhance training efficiency and model performance. The state is represented by the validation loss and current learning rate, and the Q-learning agent learns to adjust the learning rate to optimize the training process. The approach is validated on three datasets: shakespeare_char, enwik8, and text8.","['Can you provide a detailed justification for the choice of state representation (validation loss and current learning rate)?', 'How does your method compare with other adaptive learning rate methods like AdamW, LAMB, Lookahead, or Noisy Adam in terms of both performance and computational overhead?', 'Can you clarify the reward signal used in your Q-learning approach?', 'Why were other RL approaches not considered or compared with Q-learning?', 'Can the authors provide more details on the hyperparameter tuning process?', 'Can the authors provide more details on the state and action space used in Q-learning?', 'How sensitive is the approach to the choice of hyperparameters for Q-learning?', 'Can the authors provide a more in-depth analysis of why Q-learning leads to better performance?', 'Can you provide more details on the implementation of the Q-learning agent and its interaction with the training process?', 'What specific benefits does Q-learning offer over other RL-based hyperparameter optimization methods?', 'Can you elaborate on the marginal improvements in validation loss? Why are the differences so small?', 'How does the proposed method generalize to other types of neural network architectures or other hyperparameters?', 'Can the authors provide more insights into the robustness and generality of the proposed Q-learning based approach?', 'How does the method perform on other types of neural network architectures apart from transformers?', 'Can the authors discuss potential limitations and ethical concerns in more detail?']","[""The method's performance is sensitive to the choice of hyperparameters, and there is additional overhead introduced by the Q-learning agent."", 'The experimental results do not convincingly demonstrate significant improvements over baseline methods.', 'The approach may not generalize well to other types of neural network architectures without further tuning.', 'The authors should discuss the potential drawbacks and challenges of using Q-learning for learning rate adaptation in more detail.', 'The paper does not adequately address the potential limitations and ethical concerns of the proposed approach. It is important to discuss how the method scales to other neural network architectures and the potential risks associated with its use.']",False,2,2,2,3,4,"['The application of Q-learning for dynamic learning rate adaptation during transformer training is novel and interesting.', 'The paper addresses an important problem in neural network training: the selection of an appropriate learning rate schedule.', 'Comprehensive experimental setup on multiple datasets.']","['The experimental results do not convincingly demonstrate a significant improvement over baseline methods. The best validation loss achieved by the Q-learning method on the shakespeare_char dataset is worse than the baseline.', 'The choice of state representation (validation loss and current learning rate) is not well-justified.', 'The paper lacks a detailed comparison with other sophisticated adaptive learning rate methods like AdamW, LAMB, Lookahead, or Noisy Adam.', 'The clarity of the explanation on Q-learning and the reward signal could be improved.', 'The technical details of the Q-learning implementation and its integration with transformer training are not thoroughly explained.', 'The significance of the results is questionable given the additional complexity introduced by the Q-learning agent.', 'The figures and tables are not clear and do not provide sufficient insight into the benefits of the proposed method.', 'The paper does not sufficiently address the limitations of the proposed method, such as sensitivity to hyperparameters and potential overhead from the Q-learning agent.', 'The discussion on the broader impacts and potential applications of the approach is limited.']",2,2,2,2,Reject
              weight_initialization_grokking,"The paper investigates the impact of weight initialization strategies on the grokking phenomenon in Transformer models, focusing on arithmetic tasks in finite fields. It compares five initialization methods (PyTorch default, Xavier, He, Orthogonal, and Kaiming Normal) using a small Transformer architecture. The study reveals significant differences in convergence speed and generalization capabilities across initialization strategies, with Xavier and Orthogonal initializations showing superior performance.","['Can the authors provide more theoretical explanations for why certain initialization methods perform better?', 'How do the findings translate to more complex, real-world tasks beyond simple arithmetic operations?', 'Can the clarity of the figures and tables be improved, and can key graphs be better integrated into the text?', 'What are the potential negative societal impacts of the findings?']","['The study is limited to small Transformer models and arithmetic tasks, which may not fully represent the complexity of real-world problems.', 'The paper lacks a deeper theoretical understanding of the observed phenomena.', 'The potential negative societal impacts of the findings are not addressed.']",False,3,3,3,5,4,"['Addresses an intriguing and underexplored phenomenon in deep learning.', 'Provides a systematic comparison of multiple weight initialization strategies.', 'Includes rigorous empirical analysis and statistical validation.', 'Offers practical guidelines for initialization in similar learning scenarios.']","['The scope is limited to small Transformer models and arithmetic tasks, which may not generalize well to larger models or more complex tasks.', 'The paper lacks deeper theoretical insights into why certain initialization strategies perform better.', 'The clarity of the experimental setup and the integration of figures and tables could be improved.', 'The implications for broader Transformer applications and potential societal impacts are not sufficiently addressed.']",3,3,3,3,Reject
              gan_diffusion,"The paper proposes integrating a Generative Adversarial Network (GAN) framework into diffusion models to improve sample quality and diversity. The approach includes a simple discriminator network, an adversarial loss term, and a gradient penalty to the adversarial loss. Extensive experiments on multiple 2D datasets are conducted to validate the approach, comparing results in terms of training time, evaluation loss, KL divergence, and sample quality.","['Can you provide more details on the architecture of the discriminator network?', ""How do the hyperparameters Î»adv and Î»gp affect the model's performance?"", 'Can you explain why the improvements are inconsistent across different datasets?', 'Can the authors provide more detailed descriptions of the denoiser and discriminator networks?', 'Have the authors considered using more comprehensive evaluation metrics like FID?', 'Can the authors provide more ablation studies to isolate the contributions of the gradient penalty and adversarial loss?', 'How would the proposed method perform on more complex and higher-dimensional datasets?']","['The paper acknowledges the increased training time and dataset dependency of the improvements. However, it could benefit from a more thorough exploration of different architectures and higher-dimensional datasets.', ""The empirical results show mixed improvements, indicating that the model's performance may be dataset-dependent."", 'The paper does not explore the limitations of the proposed approach in depth, particularly in terms of scalability to higher-dimensional data.']",False,2,2,2,3,4,"['The integration of GAN framework with diffusion models is a novel approach to improve sample quality and diversity.', 'The introduction of a gradient penalty to improve training stability is a thoughtful addition.', 'The paper provides a comprehensive evaluation on multiple 2D datasets, using various metrics such as training time, evaluation loss, KL divergence, and sample quality.']","['The methodology section lacks detailed explanations for certain components, such as the exact architecture of the discriminator network and the choice of hyperparameters.', ""The improvements in evaluation loss and KL divergence are not consistent across all datasets, indicating that the model's performance may be dataset-dependent."", ""The experimental scope is limited to 2D datasets. Further research is needed to evaluate the model's performance on higher-dimensional data."", 'The paper lacks sufficient ablation studies to isolate the contributions of different components of the proposed method.', 'The evaluation metrics are somewhat limited; including metrics like FID could strengthen the evaluation.', 'The paper does not sufficiently address the limitations of the approach, particularly its dataset dependency and scalability to higher-dimensional data.', 'There is no discussion on potential negative societal impacts or ethical concerns related to the work.']",3,2,2,2,Reject
              layerwise_learning_rates,"The paper investigates the impact of learning rate schedules on language model training, specifically focusing on linear and exponential decay schedules. The study aims to analyze their effects on training efficiency and accuracy through experiments on datasets like Shakespeare and Enwik8. However, the paper is incomplete and lacks detailed content in critical sections, making it difficult to evaluate its contributions and significance.","[""Can you provide detailed content for the placeholder sections, including 'Related Work,' 'Background,' 'Method,' 'Experimental Setup,' and 'Results'?"", 'Do you have any experimental results or theoretical analysis to support your claims about the effectiveness of the proposed learning rate schedules?', 'Can you provide more detailed explanations of the methodology used?', 'What are the specific experimental setups and results?', 'How does this work compare with other related work in the field?', 'Why did the authors only choose linear and exponential decay schedules for their study?', 'Can the authors provide more thorough analysis and discussion of their experimental results?', 'How do the proposed learning rate schedules compare to other commonly used schedules like cosine annealing or cyclical learning rates?', 'What are the specific improvements in convergence rate and accuracy observed with the proposed schedules?']","['The paper is incomplete and lacks detailed content in critical sections. This makes it impossible to evaluate its limitations or potential negative societal impact.', 'The study is limited to only two types of learning rate schedules, which may not provide enough insights into the broader impact of learning rate schedules on language model training.']",False,1,1,1,2,4,['The topic of learning rate schedules is relevant and important in the context of language model training.'],"['The paper is incomplete, with missing sections on methodology, experimental setup, and results.', 'The organization and clarity of the paper are poor, with repeated sections and placeholders.', 'The paper lacks novelty as it focuses on well-known learning rate schedules (linear and exponential decay) without introducing new methodologies or theoretical insights.', 'The experimental results are not thoroughly detailed, and the analysis is superficial.', 'The paper is missing related work and background sections, making it difficult to place the study in the context of existing research.', 'The contributions are modest and incremental at best, failing to advance the state of the art in a significant way.']",1,1,1,1,Reject
              grid_based_noise_adaptation,"The paper introduces a multi-scale grid-based noise adaptation mechanism for diffusion models to improve their performance on low-dimensional datasets. It employs a combination of coarse (5x5) and fine (20x20) grids to dynamically adjust noise levels during the diffusion process, with L1 regularization encouraging sparsity in fine-grained adjustments. The approach is evaluated on four 2D datasets: circle, dino, line, and moons, showing improvements in sample quality and distribution matching.","['Can the authors provide more detailed explanations of the multi-scale grid-based noise adaptation mechanism?', 'How does the performance of the proposed method compare to other state-of-the-art methods for low-dimensional data generation?', 'Can the authors discuss the potential societal impact and limitations of their work in more detail?', 'Can the authors provide more detailed ablation studies to isolate the impact of coarse and fine grids, as well as L1 regularization?', 'How does the proposed method perform on higher-dimensional datasets, and what are the challenges anticipated in such scenarios?', 'Can the authors elaborate on the choice of the specific grid sizes (5x5 and 20x20)? Have alternative configurations been tested?', 'Can the authors provide more visualizations for the generated samples, particularly for the dino and moons datasets?', 'Can you provide a detailed explanation of the L1 regularization term and its impact on the results?']","[""The paper does not discuss the potential societal impact and limitations of the proposed method in sufficient detail. It would be beneficial to address these aspects to provide a more comprehensive understanding of the work's implications."", 'The paper does not address the potential computational overhead and increased training time associated with the proposed method.', 'There is limited discussion on the generalizability of the approach to higher-dimensional datasets or other types of data.', 'The paper does not thoroughly address potential limitations of the proposed method, such as increased computational complexity and dataset-specific tuning requirements.', ""The method's effectiveness on higher-dimensional datasets remains unexplored."", 'Increased computational costs for training and inference.']",False,2,2,2,4,4,"['The paper addresses a relevant problem in the application of diffusion models to low-dimensional data.', 'The proposed multi-scale grid-based noise adaptation mechanism is novel and shows potential.', 'The experimental results demonstrate improvements in sample quality and distribution matching on several 2D datasets.']","['The paper lacks clarity in some sections, especially regarding the detailed implementation of the proposed method.', 'The experiments, while showing improvements, lack comprehensive analyses and more ablation studies.', 'The potential societal impact and limitations of the proposed method are not adequately discussed.', 'The paper does not compare the proposed method with a wide range of existing methods, limiting the context of its contributions.', 'There are some formatting issues, such as missing figure captions (e.g., Figure 2).', 'The choice of datasets, while diverse, needs better justification in terms of their relevance and representativeness for broader applications.', 'The computational overhead and training time increases are significant and need more discussion regarding their practical implications.']",3,2,2,3,Reject
              data_augmentation_grokking,"The paper investigates the impact of data augmentation on the grokking phenomenon in neural networks learning modular arithmetic operations. Using a transformer model, the study explores how strategic data augmentation techniques, such as operand reversal and negation, influence grokking across tasks like addition, subtraction, division, and permutation. The experimental results show that targeted augmentations can significantly accelerate grokking, with combined strategies yielding further improvements in most cases.","['Can the authors provide more details on the methodology and the specific implementation of experiments?', 'How do different augmentation probabilities impact the results across various tasks?', 'Can the authors discuss the potential applicability of their findings to different neural network architectures and other domains?', 'Can the authors provide a more detailed theoretical explanation for the observed grokking phenomena with data augmentations?', 'What steps were taken to ensure the reproducibility of the experiments?', 'Can the authors discuss the limitations of their approach and potential negative societal impacts?', 'Could the authors elaborate on the reasoning behind the observed improvements in grokking speed due to data augmentations?', 'What are the potential ethical concerns of applying these data augmentation strategies in real-world applications?', 'Can the authors include more ablation studies to dissect the individual contributions of each augmentation technique in greater detail?', 'How do the results generalize to other neural network architectures or more complex tasks beyond modular arithmetic?']","[""The paper's clarity and thoroughness in discussing methodology and results need improvement."", 'The generalizability of the findings to other domains and architectures requires further exploration.', 'The study acknowledges the sensitivity of results to hyperparameters and task specificity. However, it should also consider the broader applicability and potential limitations in real-world scenarios.', 'Potential negative societal impacts are not discussed, which is important for a comprehensive evaluation of the work.']",False,3,3,3,5,4,"['Addresses a novel and relevant topic in deep learning, focusing on the grokking phenomenon.', 'Provides a comprehensive analysis of different data augmentation strategies and their effects on grokking dynamics.', 'Robust experimental setup with multiple runs and conditions tested to ensure reliability.', 'Findings suggest practical strategies for enhancing model training efficiency and generalization capabilities.']","['Lacks clarity in some sections, particularly in the methodology and the detailed implementation of experiments.', 'Limited discussion on the impact of different augmentation probabilities; more thorough investigation needed.', 'Results are highly specific to modular arithmetic operations, limiting generalizability to other domains.', 'Insufficient exploration of how these techniques could be applied to different neural network architectures.', 'Theoretical justifications for the observed effects are lacking.', 'Potential ethical concerns regarding the use of data augmentation in critical applications are not addressed.']",3,3,3,3,Reject
              mdl_grokking_correlation,"This paper investigates the phenomenon of grokking in neural networks through the lens of Minimal Description Length (MDL), offering an information-theoretic perspective on sudden generalization. The authors propose a method to estimate and track MDL during training using weight pruning techniques. Experiments on modular arithmetic and permutation tasks reveal a strong connection between MDL transitions and grokking points, with varying dynamics across different tasks.","['Can the authors provide a more detailed description of the weight pruning technique and how MDL is estimated?', 'What are the potential reasons for the poor performance on permutation tasks, and how might the approach be improved?', 'Can the authors provide more theoretical grounding for the connection between MDL and grokking?', 'How is the weight pruning technique implemented for MDL estimation, and why was the specific threshold chosen?', 'Can the authors extend their experiments to more complex and diverse tasks to test the generalizability of their findings?', 'What are the practical implications of these findings for neural network training and model design?']","['The paper needs to address the clarity of the description of methods, particularly weight pruning and MDL estimation.', 'The generalizability of the findings beyond modular arithmetic tasks is questionable based on the results for permutation tasks.', 'The potential negative societal impacts of this work are not discussed, although the focus on theoretical and empirical analysis may have minimal direct societal consequences.']",False,2,2,2,3,4,"['The paper addresses a significant and poorly understood phenomenon in neural networks, grokking.', 'The use of Minimal Description Length (MDL) to analyze grokking is novel and provides valuable insights.', 'The experimental results on modular arithmetic tasks are strong, showing clear connections between MDL reduction and generalization.', 'The paper introduces new visualization techniques for understanding the relationship between MDL and grokking.']","['The description of the weight pruning technique and how MDL is estimated lacks clarity and detail.', 'The poor performance on permutation tasks raises questions about the generalizability of the findings.', 'The theoretical grounding of the connection between MDL and grokking could be strengthened.', 'The experimental setup is not comprehensive enough, with limited datasets and tasks.', 'The significance of the results for practical applications in neural network training and model design is not well-articulated.']",3,2,2,3,Reject
              dual_expert_denoiser,"The paper 'DualDiff: Enhancing Mode Capture in Low-Dimensional Diffusion Models via Dual-Expert Denoising' introduces a dual-expert denoising architecture aimed at enhancing diffusion models' performance on low-dimensional datasets. The method uses a gating mechanism to combine two specialized expert networks dynamically, which helps in capturing multiple modes in low-dimensional data distributions. The paper demonstrates substantial improvements in terms of mode capture and sample diversity, validated through various experiments on 2D datasets like 'circle', 'dino', 'line', and 'moons'.","['Could you provide more detailed analysis on how the gating mechanism adapts during training?', 'How would the model perform on higher-dimensional datasets or more complex low-dimensional datasets?', 'Is the choice of the diversity loss weight (Î») empirically validated? Could different values lead to significantly different results?', 'Can the authors provide more details on the gating mechanism and how it determines the weight for each expert network?', 'How does the performance vary with different configurations of the gating network?', 'Can the authors explain the choice of hyperparameters, particularly the value of lambda in the diversity loss term?', 'Can the authors provide more detailed ablation studies to quantify the impact of each component (e.g., gating mechanism, diversity loss)?', 'How does the model perform with different types of aggregators for the expert networks?', 'Can more qualitative examples and visualizations be provided to substantiate the claims of improved mode capture?', 'Can you provide more details on the architecture of the expert networks and the gating mechanism?', 'How does the diversity loss term impact the final performance, and what are the trade-offs?', 'Can you include more comprehensive ablation studies to evaluate the impact of each component of the proposed method?', 'What are the computational costs associated with the dual-expert architecture, and how do they compare to the baseline?']","['The increased computational cost and the focus on low-dimensional datasets are the primary limitations of the proposed approach.', 'The generalizability to higher-dimensional settings remains unclear.', 'Potential negative societal impacts and limitations are not adequately addressed.']",False,3,3,3,5,4,"['The paper addresses a relevant and challenging problem in the field of generative modeling.', 'The dual-expert architecture and dynamic gating mechanism are novel and well-formulated.', ""Extensive experiments provide strong evidence of the approach's effectiveness."", 'The introduction of a diversity loss term to encourage multiple mode capture is a valuable contribution.']","['The novelty of combining two expert networks with a gating mechanism is somewhat incremental.', 'The choice of datasets is limited to simple 2D shapes, which might not fully demonstrate the generalizability of the approach.', 'The evaluation of gating mechanism behavior is not sufficiently detailed.', 'The increased training and inference times are a significant drawback that may limit practical applicability.', 'The diversity loss term is weighted arbitrarily without thorough justification for the chosen value.', 'The paper lacks detailed ablation studies to isolate the impact of different components (e.g., gating mechanism, diversity loss).', 'Potential limitations and negative societal impacts are not adequately addressed.']",3,3,3,3,Reject

            ]]></content>
          </file>
          <file name="manual_paper_ratings.md" path="wip/spreadsheets/manual_paper_ratings.md">
            <content><![CDATA[
              # ratings

              | paper_id                       | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | Questions                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | Limitations                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | Ethical Concerns | Soundness | Presentation | Contribution | Overall | Confidence | Strengths                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | Weaknesses                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | Originality | Quality | Clarity | Significance | Decision |
              |--------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------|-----------|--------------|--------------|---------|------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------|---------|---------|--------------|----------|
              | ---                            | ---                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | ---                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | ---                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | ---              | ---       | ---          | ---          | ---     | ---        | ---                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | ---                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | ---         | ---     | ---     | ---          | ---      |
              | adaptive_dual_scale_denoising  | The paper introduces an adaptive dual-scale denoising approach for low-dimensional diffusion models, aiming to balance global structure and local details in generated samples. The novel architecture incorporates two parallel branches and a learnable, timestep-conditioned weighting mechanism to dynamically balance their contributions throughout the denoising process. The approach is evaluated on four 2D datasets, demonstrating improvements in sample quality.                                                                                                                                      | ['Can you provide a more detailed theoretical justification for the dual-scale architecture?', "What impact do different types of aggregators have on the model's performance?", 'How does the model perform on more complex, real-world low-dimensional datasets?', 'Can the computational cost be reduced without sacrificing performance?']                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | ['The paper should address the high computational cost and explore ways to optimize it.', 'The limited diversity of datasets and lack of detailed theoretical backing for the proposed architecture are notable limitations.']                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | False            | 3         | 3            | 3            | 5       | 4          | ['Novel approach to balancing global and local features in diffusion models for low-dimensional data.', 'Comprehensive empirical evaluation on multiple 2D datasets.', 'Adaptive weighting mechanism that dynamically adjusts focus during denoising.']                                                                                                                                                                                                                    | ['Lacks detailed theoretical justification for the dual-scale architecture.', 'Computational cost is significantly higher, which may limit practical applicability.', 'Some sections are not clearly explained, such as the autoencoder aggregator and weight evolution analysis.', 'Limited diversity in the datasets used for evaluation. More complex, real-world datasets could strengthen claims.', 'Insufficient ablation studies and analysis on specific design choices like different types of aggregators.']                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | 4           | 3       | 3       | 3            | Reject   |
              | layerwise_lr_grokking          | The paper proposes a novel layer-wise learning rate strategy to accelerate and enhance the grokking phenomenon in Transformer models. The approach involves assigning different learning rates to the embedding layers, lower Transformer layers, and higher Transformer layers. The method is empirically validated on algorithmic tasks such as modular arithmetic and permutations, showing significant improvements in convergence speed and final performance.                                                                                                                                                | ['Can the authors provide more detailed explanations of the hyperparameter tuning process and the exact implementation of the layer-wise learning rates?', 'How do the authors ensure that the proposed method generalizes to tasks beyond the algorithmic ones tested in the paper?', 'Can the authors compare their approach with other related methods in more detail?', 'Can you provide more theoretical insights into why layer-wise learning rates specifically facilitate grokking?', 'How were the specific learning rates chosen for embedding, lower, and higher layers?', 'Can you discuss the potential for overfitting and how it was mitigated?', 'Have you tested the robustness of your method across different datasets and larger model sizes?', 'What is the impact of different learning rate configurations on the results?', 'Can the authors discuss potential strategies for mitigating the need for careful tuning of learning rates to avoid instability?']                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | ['The methodology lacks detailed clarity, and the authors do not provide sufficient information on the hyperparameter tuning process.', 'The scope of tasks is limited to algorithmic ones, and the generalizability of the findings is unclear.', 'The paper requires more theoretical backing for the proposed method.', 'The choice of specific learning rates and potential overfitting issues need to be addressed in more detail.', 'The scalability of the approach to larger models and more complex tasks is not thoroughly addressed.', 'Ethical concerns related to the potential misuse of accelerated learning techniques are not addressed.']                                                                                                                                                                                     | False            | 2         | 2            | 3            | 4       | 4          | ['The paper addresses an important problem in deep learning: the grokking phenomenon.', 'The proposed layer-wise learning rate strategy is novel and shows significant improvements in experimental results.', 'Experiments demonstrate substantial improvements in both convergence speed and final performance.']                                                                                                                                                        | ['The paper lacks detailed methodological clarity, particularly regarding the exact implementation of the layer-wise learning rates and hyperparameter tuning.', 'The theoretical explanation for why layer-wise learning rates work is insufficient.', 'The scope of tasks is limited to algorithmic ones, making it unclear how well the findings generalize to other domains.', 'The choice of learning rates seems arbitrary and lacks justification.', 'More comprehensive ablation studies and comparisons with other related methods would strengthen the paper.', 'Certain sections, such as the experimental setup and ablation studies, could be more detailed and clearer.']                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | 3           | 2       | 3       | 3            | Reject   |
              | multi_style_adapter            | The paper introduces the Multi-Style Adapter, which enhances style awareness and consistency in character-level language models by integrating learnable style embeddings, a style classification head, and a StyleAdapter module into the GPT architecture. The approach aims to balance style adaptation and language modeling capabilities, and demonstrates improved style consistency and competitive validation losses across multiple datasets.                                                                                                                                                             | ['How does the model handle unseen styles during inference?', 'Can the authors provide more details on the training process and hyperparameter tuning?', "What are the potential impacts of overfitting on the model's ability to generate diverse text within each style?", 'Can the authors provide more detailed ablation studies, especially focusing on the impact of different components in the Multi-Style Adapter?', 'How does the Multi-Style Adapter perform compared to other recent style-transfer models?', 'Can the computational efficiency trade-offs be quantified in a more detailed manner?', "Can the authors clarify the autoencoder aggregator's role and how it integrates with the rest of the model?", 'What measures have been taken to ensure the model does not overfit to specific style patterns, especially given the perfect consistency scores on some datasets?', 'Are there any potential optimization techniques that could be explored to improve the computational efficiency of the Multi-Style Adapter?', 'How does the model handle cases where the input sequence contains mixed styles?', 'Could you provide more qualitative examples of generated text to demonstrate the style consistency?', 'What is the impact of reducing the number of gating parameters in the modulation function?']                                                                                                                                                                                                                                                                           | ["The reduced inference speed and potential overfitting to specific style patterns are significant limitations. Future work should focus on optimizing computational efficiency and improving the model's ability to generalize to diverse styles.", 'The paper currently lacks sufficient ablation studies and additional baselines.', "The model's performance may be sensitive to hyperparameter settings, such as the weight of the style loss and the frequency of StyleAdapter application."]                                                                                                                                                                                                                                                                                                                                             | False            | 3         | 3            | 3            | 5       | 4          | ['The paper presents a novel approach to style-aware language modeling, addressing a critical need for fine-grained stylistic control.', 'The Multi-Style Adapter is well-motivated and integrates seamlessly with the GPT architecture.', 'Extensive experiments on diverse datasets demonstrate improved style consistency and validation loss.', 'The paper includes thorough analysis and visualization of learned style embeddings and attention patterns.']          | ['The model achieves perfect style consistency scores on some datasets, which may indicate overfitting to specific style patterns.', 'The reduced inference speed (approximately 40% slower than the baseline) may limit the practical applicability of the model.', 'The paper could explore more sophisticated style representation techniques and evaluate their impact.', 'Lack of detailed ablation studies and additional baselines to strengthen the claims.', 'Clarity of the autoencoder aggregator mechanism could be enhanced.']                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | 3           | 3       | 3       | 3            | Reject   |
              | rl_lr_adaptation               | The paper explores the application of Q-learning to dynamically adjust the learning rate during transformer model training, aiming to enhance training efficiency and model performance. The state is represented by the validation loss and current learning rate, and the Q-learning agent learns to adjust the learning rate to optimize the training process. The approach is validated on three datasets: shakespeare_char, enwik8, and text8.                                                                                                                                                                | ['Can you provide a detailed justification for the choice of state representation (validation loss and current learning rate)?', 'How does your method compare with other adaptive learning rate methods like AdamW, LAMB, Lookahead, or Noisy Adam in terms of both performance and computational overhead?', 'Can you clarify the reward signal used in your Q-learning approach?', 'Why were other RL approaches not considered or compared with Q-learning?', 'Can the authors provide more details on the hyperparameter tuning process?', 'Can the authors provide more details on the state and action space used in Q-learning?', 'How sensitive is the approach to the choice of hyperparameters for Q-learning?', 'Can the authors provide a more in-depth analysis of why Q-learning leads to better performance?', 'Can you provide more details on the implementation of the Q-learning agent and its interaction with the training process?', 'What specific benefits does Q-learning offer over other RL-based hyperparameter optimization methods?', 'Can you elaborate on the marginal improvements in validation loss? Why are the differences so small?', 'How does the proposed method generalize to other types of neural network architectures or other hyperparameters?', 'Can the authors provide more insights into the robustness and generality of the proposed Q-learning based approach?', 'How does the method perform on other types of neural network architectures apart from transformers?', 'Can the authors discuss potential limitations and ethical concerns in more detail?'] | ["The method's performance is sensitive to the choice of hyperparameters, and there is additional overhead introduced by the Q-learning agent.", 'The experimental results do not convincingly demonstrate significant improvements over baseline methods.', 'The approach may not generalize well to other types of neural network architectures without further tuning.', 'The authors should discuss the potential drawbacks and challenges of using Q-learning for learning rate adaptation in more detail.', 'The paper does not adequately address the potential limitations and ethical concerns of the proposed approach. It is important to discuss how the method scales to other neural network architectures and the potential risks associated with its use.']                                                                     | False            | 2         | 2            | 2            | 3       | 4          | ['The application of Q-learning for dynamic learning rate adaptation during transformer training is novel and interesting.', 'The paper addresses an important problem in neural network training: the selection of an appropriate learning rate schedule.', 'Comprehensive experimental setup on multiple datasets.']                                                                                                                                                     | ['The experimental results do not convincingly demonstrate a significant improvement over baseline methods. The best validation loss achieved by the Q-learning method on the shakespeare_char dataset is worse than the baseline.', 'The choice of state representation (validation loss and current learning rate) is not well-justified.', 'The paper lacks a detailed comparison with other sophisticated adaptive learning rate methods like AdamW, LAMB, Lookahead, or Noisy Adam.', 'The clarity of the explanation on Q-learning and the reward signal could be improved.', 'The technical details of the Q-learning implementation and its integration with transformer training are not thoroughly explained.', 'The significance of the results is questionable given the additional complexity introduced by the Q-learning agent.', 'The figures and tables are not clear and do not provide sufficient insight into the benefits of the proposed method.', 'The paper does not sufficiently address the limitations of the proposed method, such as sensitivity to hyperparameters and potential overhead from the Q-learning agent.', 'The discussion on the broader impacts and potential applications of the approach is limited.'] | 2           | 2       | 2       | 2            | Reject   |
              | weight_initialization_grokking | The paper investigates the impact of weight initialization strategies on the grokking phenomenon in Transformer models, focusing on arithmetic tasks in finite fields. It compares five initialization methods (PyTorch default, Xavier, He, Orthogonal, and Kaiming Normal) using a small Transformer architecture. The study reveals significant differences in convergence speed and generalization capabilities across initialization strategies, with Xavier and Orthogonal initializations showing superior performance.                                                                                     | ['Can the authors provide more theoretical explanations for why certain initialization methods perform better?', 'How do the findings translate to more complex, real-world tasks beyond simple arithmetic operations?', 'Can the clarity of the figures and tables be improved, and can key graphs be better integrated into the text?', 'What are the potential negative societal impacts of the findings?']                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | ['The study is limited to small Transformer models and arithmetic tasks, which may not fully represent the complexity of real-world problems.', 'The paper lacks a deeper theoretical understanding of the observed phenomena.', 'The potential negative societal impacts of the findings are not addressed.']                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | False            | 3         | 3            | 3            | 5       | 4          | ['Addresses an intriguing and underexplored phenomenon in deep learning.', 'Provides a systematic comparison of multiple weight initialization strategies.', 'Includes rigorous empirical analysis and statistical validation.', 'Offers practical guidelines for initialization in similar learning scenarios.']                                                                                                                                                          | ['The scope is limited to small Transformer models and arithmetic tasks, which may not generalize well to larger models or more complex tasks.', 'The paper lacks deeper theoretical insights into why certain initialization strategies perform better.', 'The clarity of the experimental setup and the integration of figures and tables could be improved.', 'The implications for broader Transformer applications and potential societal impacts are not sufficiently addressed.']                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | 3           | 3       | 3       | 3            | Reject   |
              | gan_diffusion                  | The paper proposes integrating a Generative Adversarial Network (GAN) framework into diffusion models to improve sample quality and diversity. The approach includes a simple discriminator network, an adversarial loss term, and a gradient penalty to the adversarial loss. Extensive experiments on multiple 2D datasets are conducted to validate the approach, comparing results in terms of training time, evaluation loss, KL divergence, and sample quality.                                                                                                                                              | ['Can you provide more details on the architecture of the discriminator network?', "How do the hyperparameters Î»adv and Î»gp affect the model's performance?", 'Can you explain why the improvements are inconsistent across different datasets?', 'Can the authors provide more detailed descriptions of the denoiser and discriminator networks?', 'Have the authors considered using more comprehensive evaluation metrics like FID?', 'Can the authors provide more ablation studies to isolate the contributions of the gradient penalty and adversarial loss?', 'How would the proposed method perform on more complex and higher-dimensional datasets?']                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | ['The paper acknowledges the increased training time and dataset dependency of the improvements. However, it could benefit from a more thorough exploration of different architectures and higher-dimensional datasets.', "The empirical results show mixed improvements, indicating that the model's performance may be dataset-dependent.", 'The paper does not explore the limitations of the proposed approach in depth, particularly in terms of scalability to higher-dimensional data.']                                                                                                                                                                                                                                                                                                                                                 | False            | 2         | 2            | 2            | 3       | 4          | ['The integration of GAN framework with diffusion models is a novel approach to improve sample quality and diversity.', 'The introduction of a gradient penalty to improve training stability is a thoughtful addition.', 'The paper provides a comprehensive evaluation on multiple 2D datasets, using various metrics such as training time, evaluation loss, KL divergence, and sample quality.']                                                                       | ['The methodology section lacks detailed explanations for certain components, such as the exact architecture of the discriminator network and the choice of hyperparameters.', "The improvements in evaluation loss and KL divergence are not consistent across all datasets, indicating that the model's performance may be dataset-dependent.", "The experimental scope is limited to 2D datasets. Further research is needed to evaluate the model's performance on higher-dimensional data.", 'The paper lacks sufficient ablation studies to isolate the contributions of different components of the proposed method.', 'The evaluation metrics are somewhat limited; including metrics like FID could strengthen the evaluation.', 'The paper does not sufficiently address the limitations of the approach, particularly its dataset dependency and scalability to higher-dimensional data.', 'There is no discussion on potential negative societal impacts or ethical concerns related to the work.']                                                                                                                                                                                                                                      | 3           | 2       | 2       | 2            | Reject   |
              | layerwise_learning_rates       | The paper investigates the impact of learning rate schedules on language model training, specifically focusing on linear and exponential decay schedules. The study aims to analyze their effects on training efficiency and accuracy through experiments on datasets like Shakespeare and Enwik8. However, the paper is incomplete and lacks detailed content in critical sections, making it difficult to evaluate its contributions and significance.                                                                                                                                                           | ["Can you provide detailed content for the placeholder sections, including 'Related Work,' 'Background,' 'Method,' 'Experimental Setup,' and 'Results'?", 'Do you have any experimental results or theoretical analysis to support your claims about the effectiveness of the proposed learning rate schedules?', 'Can you provide more detailed explanations of the methodology used?', 'What are the specific experimental setups and results?', 'How does this work compare with other related work in the field?', 'Why did the authors only choose linear and exponential decay schedules for their study?', 'Can the authors provide more thorough analysis and discussion of their experimental results?', 'How do the proposed learning rate schedules compare to other commonly used schedules like cosine annealing or cyclical learning rates?', 'What are the specific improvements in convergence rate and accuracy observed with the proposed schedules?']                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | ['The paper is incomplete and lacks detailed content in critical sections. This makes it impossible to evaluate its limitations or potential negative societal impact.', 'The study is limited to only two types of learning rate schedules, which may not provide enough insights into the broader impact of learning rate schedules on language model training.']                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | False            | 1         | 1            | 1            | 2       | 4          | ['The topic of learning rate schedules is relevant and important in the context of language model training.']                                                                                                                                                                                                                                                                                                                                                              | ['The paper is incomplete, with missing sections on methodology, experimental setup, and results.', 'The organization and clarity of the paper are poor, with repeated sections and placeholders.', 'The paper lacks novelty as it focuses on well-known learning rate schedules (linear and exponential decay) without introducing new methodologies or theoretical insights.', 'The experimental results are not thoroughly detailed, and the analysis is superficial.', 'The paper is missing related work and background sections, making it difficult to place the study in the context of existing research.', 'The contributions are modest and incremental at best, failing to advance the state of the art in a significant way.']                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | 1           | 1       | 1       | 1            | Reject   |
              | grid_based_noise_adaptation    | The paper introduces a multi-scale grid-based noise adaptation mechanism for diffusion models to improve their performance on low-dimensional datasets. It employs a combination of coarse (5x5) and fine (20x20) grids to dynamically adjust noise levels during the diffusion process, with L1 regularization encouraging sparsity in fine-grained adjustments. The approach is evaluated on four 2D datasets: circle, dino, line, and moons, showing improvements in sample quality and distribution matching.                                                                                                  | ['Can the authors provide more detailed explanations of the multi-scale grid-based noise adaptation mechanism?', 'How does the performance of the proposed method compare to other state-of-the-art methods for low-dimensional data generation?', 'Can the authors discuss the potential societal impact and limitations of their work in more detail?', 'Can the authors provide more detailed ablation studies to isolate the impact of coarse and fine grids, as well as L1 regularization?', 'How does the proposed method perform on higher-dimensional datasets, and what are the challenges anticipated in such scenarios?', 'Can the authors elaborate on the choice of the specific grid sizes (5x5 and 20x20)? Have alternative configurations been tested?', 'Can the authors provide more visualizations for the generated samples, particularly for the dino and moons datasets?', 'Can you provide a detailed explanation of the L1 regularization term and its impact on the results?']                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | ["The paper does not discuss the potential societal impact and limitations of the proposed method in sufficient detail. It would be beneficial to address these aspects to provide a more comprehensive understanding of the work's implications.", 'The paper does not address the potential computational overhead and increased training time associated with the proposed method.', 'There is limited discussion on the generalizability of the approach to higher-dimensional datasets or other types of data.', 'The paper does not thoroughly address potential limitations of the proposed method, such as increased computational complexity and dataset-specific tuning requirements.', "The method's effectiveness on higher-dimensional datasets remains unexplored.", 'Increased computational costs for training and inference.'] | False            | 2         | 2            | 2            | 4       | 4          | ['The paper addresses a relevant problem in the application of diffusion models to low-dimensional data.', 'The proposed multi-scale grid-based noise adaptation mechanism is novel and shows potential.', 'The experimental results demonstrate improvements in sample quality and distribution matching on several 2D datasets.']                                                                                                                                        | ['The paper lacks clarity in some sections, especially regarding the detailed implementation of the proposed method.', 'The experiments, while showing improvements, lack comprehensive analyses and more ablation studies.', 'The potential societal impact and limitations of the proposed method are not adequately discussed.', 'The paper does not compare the proposed method with a wide range of existing methods, limiting the context of its contributions.', 'There are some formatting issues, such as missing figure captions (e.g., Figure 2).', 'The choice of datasets, while diverse, needs better justification in terms of their relevance and representativeness for broader applications.', 'The computational overhead and training time increases are significant and need more discussion regarding their practical implications.']                                                                                                                                                                                                                                                                                                                                                                                          | 3           | 2       | 2       | 3            | Reject   |
              | data_augmentation_grokking     | The paper investigates the impact of data augmentation on the grokking phenomenon in neural networks learning modular arithmetic operations. Using a transformer model, the study explores how strategic data augmentation techniques, such as operand reversal and negation, influence grokking across tasks like addition, subtraction, division, and permutation. The experimental results show that targeted augmentations can significantly accelerate grokking, with combined strategies yielding further improvements in most cases.                                                                        | ['Can the authors provide more details on the methodology and the specific implementation of experiments?', 'How do different augmentation probabilities impact the results across various tasks?', 'Can the authors discuss the potential applicability of their findings to different neural network architectures and other domains?', 'Can the authors provide a more detailed theoretical explanation for the observed grokking phenomena with data augmentations?', 'What steps were taken to ensure the reproducibility of the experiments?', 'Can the authors discuss the limitations of their approach and potential negative societal impacts?', 'Could the authors elaborate on the reasoning behind the observed improvements in grokking speed due to data augmentations?', 'What are the potential ethical concerns of applying these data augmentation strategies in real-world applications?', 'Can the authors include more ablation studies to dissect the individual contributions of each augmentation technique in greater detail?', 'How do the results generalize to other neural network architectures or more complex tasks beyond modular arithmetic?']                                                                                                                                                                                                                                                                                                                                                                                                                                    | ["The paper's clarity and thoroughness in discussing methodology and results need improvement.", 'The generalizability of the findings to other domains and architectures requires further exploration.', 'The study acknowledges the sensitivity of results to hyperparameters and task specificity. However, it should also consider the broader applicability and potential limitations in real-world scenarios.', 'Potential negative societal impacts are not discussed, which is important for a comprehensive evaluation of the work.']                                                                                                                                                                                                                                                                                                  | False            | 3         | 3            | 3            | 5       | 4          | ['Addresses a novel and relevant topic in deep learning, focusing on the grokking phenomenon.', 'Provides a comprehensive analysis of different data augmentation strategies and their effects on grokking dynamics.', 'Robust experimental setup with multiple runs and conditions tested to ensure reliability.', 'Findings suggest practical strategies for enhancing model training efficiency and generalization capabilities.']                                      | ['Lacks clarity in some sections, particularly in the methodology and the detailed implementation of experiments.', 'Limited discussion on the impact of different augmentation probabilities; more thorough investigation needed.', 'Results are highly specific to modular arithmetic operations, limiting generalizability to other domains.', 'Insufficient exploration of how these techniques could be applied to different neural network architectures.', 'Theoretical justifications for the observed effects are lacking.', 'Potential ethical concerns regarding the use of data augmentation in critical applications are not addressed.']                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | 3           | 3       | 3       | 3            | Reject   |
              | mdl_grokking_correlation       | This paper investigates the phenomenon of grokking in neural networks through the lens of Minimal Description Length (MDL), offering an information-theoretic perspective on sudden generalization. The authors propose a method to estimate and track MDL during training using weight pruning techniques. Experiments on modular arithmetic and permutation tasks reveal a strong connection between MDL transitions and grokking points, with varying dynamics across different tasks.                                                                                                                          | ['Can the authors provide a more detailed description of the weight pruning technique and how MDL is estimated?', 'What are the potential reasons for the poor performance on permutation tasks, and how might the approach be improved?', 'Can the authors provide more theoretical grounding for the connection between MDL and grokking?', 'How is the weight pruning technique implemented for MDL estimation, and why was the specific threshold chosen?', 'Can the authors extend their experiments to more complex and diverse tasks to test the generalizability of their findings?', 'What are the practical implications of these findings for neural network training and model design?']                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | ['The paper needs to address the clarity of the description of methods, particularly weight pruning and MDL estimation.', 'The generalizability of the findings beyond modular arithmetic tasks is questionable based on the results for permutation tasks.', 'The potential negative societal impacts of this work are not discussed, although the focus on theoretical and empirical analysis may have minimal direct societal consequences.']                                                                                                                                                                                                                                                                                                                                                                                                | False            | 2         | 2            | 2            | 3       | 4          | ['The paper addresses a significant and poorly understood phenomenon in neural networks, grokking.', 'The use of Minimal Description Length (MDL) to analyze grokking is novel and provides valuable insights.', 'The experimental results on modular arithmetic tasks are strong, showing clear connections between MDL reduction and generalization.', 'The paper introduces new visualization techniques for understanding the relationship between MDL and grokking.'] | ['The description of the weight pruning technique and how MDL is estimated lacks clarity and detail.', 'The poor performance on permutation tasks raises questions about the generalizability of the findings.', 'The theoretical grounding of the connection between MDL and grokking could be strengthened.', 'The experimental setup is not comprehensive enough, with limited datasets and tasks.', 'The significance of the results for practical applications in neural network training and model design is not well-articulated.']                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | 3           | 2       | 2       | 3            | Reject   |
              | dual_expert_denoiser           | The paper 'DualDiff: Enhancing Mode Capture in Low-Dimensional Diffusion Models via Dual-Expert Denoising' introduces a dual-expert denoising architecture aimed at enhancing diffusion models' performance on low-dimensional datasets. The method uses a gating mechanism to combine two specialized expert networks dynamically, which helps in capturing multiple modes in low-dimensional data distributions. The paper demonstrates substantial improvements in terms of mode capture and sample diversity, validated through various experiments on 2D datasets like 'circle', 'dino', 'line', and 'moons'. | ['Could you provide more detailed analysis on how the gating mechanism adapts during training?', 'How would the model perform on higher-dimensional datasets or more complex low-dimensional datasets?', 'Is the choice of the diversity loss weight (Î») empirically validated? Could different values lead to significantly different results?', 'Can the authors provide more details on the gating mechanism and how it determines the weight for each expert network?', 'How does the performance vary with different configurations of the gating network?', 'Can the authors explain the choice of hyperparameters, particularly the value of lambda in the diversity loss term?', 'Can the authors provide more detailed ablation studies to quantify the impact of each component (e.g., gating mechanism, diversity loss)?', 'How does the model perform with different types of aggregators for the expert networks?', 'Can more qualitative examples and visualizations be provided to substantiate the claims of improved mode capture?', 'Can you provide more details on the architecture of the expert networks and the gating mechanism?', 'How does the diversity loss term impact the final performance, and what are the trade-offs?', 'Can you include more comprehensive ablation studies to evaluate the impact of each component of the proposed method?', 'What are the computational costs associated with the dual-expert architecture, and how do they compare to the baseline?']                                                                                                         | ['The increased computational cost and the focus on low-dimensional datasets are the primary limitations of the proposed approach.', 'The generalizability to higher-dimensional settings remains unclear.', 'Potential negative societal impacts and limitations are not adequately addressed.']                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | False            | 3         | 3            | 3            | 5       | 4          | ['The paper addresses a relevant and challenging problem in the field of generative modeling.', 'The dual-expert architecture and dynamic gating mechanism are novel and well-formulated.', "Extensive experiments provide strong evidence of the approach's effectiveness.", 'The introduction of a diversity loss term to encourage multiple mode capture is a valuable contribution.']                                                                                  | ['The novelty of combining two expert networks with a gating mechanism is somewhat incremental.', 'The choice of datasets is limited to simple 2D shapes, which might not fully demonstrate the generalizability of the approach.', 'The evaluation of gating mechanism behavior is not sufficiently detailed.', 'The increased training and inference times are a significant drawback that may limit practical applicability.', 'The diversity loss term is weighted arbitrarily without thorough justification for the chosen value.', 'The paper lacks detailed ablation studies to isolate the impact of different components (e.g., gating mechanism, diversity loss).', 'Potential limitations and negative societal impacts are not adequately addressed.']                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | 3           | 3       | 3       | 3            | Reject   |

            ]]></content>
          </file>
          <file name="spreasheet_to_markdown.py" path="wip/spreadsheets/spreasheet_to_markdown.py">
            <content><![CDATA[
              """
              Spreadsheet to Markdown Table Converter

              This script converts various spreadsheet formats (CSV, TSV, Excel, Google Sheets, OpenDocument)
              to neatly formatted Markdown tables. It supports the following file formats:
              - CSV and TSV
              - Excel (.xlsx and .xls)
              - Google Sheets (downloaded as CSV)
              - OpenDocument Spreadsheets (.ods)

              The script reads the input file, converts its content to a Markdown table, and saves the
              result to an output file. The Markdown table is formatted for improved readability in its
              raw form, with aligned columns.

              Usage:
                  python script.py <input_file> <output_file> [delimiter]

              Dependencies:
                  - openpyxl
                  - xlrd
                  - pandas
                  - ezodf

              Install dependencies with:
                  pip install openpyxl xlrd pandas ezodf

              Author: [Your Name]
              Date: [Current Date]
              Version: 1.5
              """

              import csv
              import sys
              from typing import List, Iterable, Any, Callable, Union
              import openpyxl
              import xlrd
              import pandas as pd
              import ezodf


              def format_markdown_table(table_lines: List[str]) -> str:
                  """
                  Format a Markdown table for improved readability in raw form.

                  Args:
                      table_lines (List[str]): List of strings representing table rows.

                  Returns:
                      str: A formatted Markdown table with aligned columns.
                  """
                  rows = [line.strip().split("|")[1:-1] for line in table_lines]
                  col_widths = [max(len(cell.strip()) for cell in col) for col in zip(*rows)]

                  formatted_rows = []
                  for i, row in enumerate(rows):
                      formatted_cells = [
                          " " + cell.strip().ljust(col_widths[j]) + " " for j, cell in enumerate(row)
                      ]
                      formatted_row = "|" + "|".join(formatted_cells) + "|"
                      formatted_rows.append(formatted_row)

                      if i == 0:
                          separator = "|" + "|".join("-" * (width + 2) for width in col_widths) + "|"
                          formatted_rows.append(separator)

                  return "\n".join(formatted_rows)


              def reader_to_markdown(reader: Iterable[Union[List[Any], tuple]]) -> str:
                  """
                  Convert an iterable of rows to a formatted Markdown table.

                  Args:
                      reader (Iterable[Union[List[Any], tuple]]):
                      An iterable where each item is a list or tuple representing a row.

                  Returns:
                      str: A formatted Markdown table.
                  """
                  iterator = iter(reader)
                  headers = next(iterator)
                  markdown_lines = [f"| {' | '.join(str(h) for h in headers)} |"]
                  markdown_lines.append(f"| {' | '.join('---' for _ in headers)} |")

                  for row in iterator:
                      markdown_lines.append(f"| {' | '.join(str(cell) for cell in row)} |")

                  return format_markdown_table(markdown_lines)


              def csv_tsv_to_markdown(file_path: str, delimiter: str = ",") -> str:
                  """
                  Convert a CSV or TSV file to a Markdown table.

                  Args:
                      file_path (str): Path to the input CSV or TSV file.
                      delimiter (str, optional): Delimiter used in the file. Defaults to ','.

                  Returns:
                      str: A formatted Markdown table.
                  """
                  with open(file_path, "r", newline="", encoding="utf-8") as file:
                      reader = csv.reader(file, delimiter=delimiter)
                      return reader_to_markdown(reader)


              def xlsx_to_markdown(file_path: str) -> str:
                  """
                  Convert an Excel (.xlsx) file to a Markdown table.

                  Args:
                      file_path (str): Path to the input .xlsx file.

                  Returns:
                      str: A formatted Markdown table.
                  """
                  workbook = openpyxl.load_workbook(file_path)
                  sheet = workbook.active
                  if sheet is None:
                      raise ValueError("No active sheet found in the workbook.")
                  return reader_to_markdown(sheet.iter_rows(values_only=True))


              def xls_to_markdown(file_path: str) -> str:
                  """
                  Convert an old Excel (.xls) file to a Markdown table.

                  Args:
                      file_path (str): Path to the input .xls file.

                  Returns:
                      str: A formatted Markdown table.
                  """
                  workbook = xlrd.open_workbook(file_path)
                  sheet = workbook.sheet_by_index(0)
                  return reader_to_markdown(sheet.get_rows())


              def gsheet_to_markdown(file_path: str) -> str:
                  """
                  Convert a Google Sheet (downloaded as CSV) to a Markdown table.

                  Args:
                      file_path (str): Path to the input Google Sheet CSV file.

                  Returns:
                      str: A formatted Markdown table.
                  """
                  df = pd.read_csv(file_path)
                  return reader_to_markdown([df.columns] + df.values.tolist())


              def ods_to_markdown(file_path: str) -> str:
                  """
                  Convert an OpenDocument Spreadsheet (.ods) file to a Markdown table.

                  Args:
                      file_path (str): Path to the input .ods file.

                  Returns:
                      str: A formatted Markdown table.
                  """
                  doc = ezodf.opendoc(file_path)
                  sheet = doc.sheets[0]

                  def ods_reader():
                      for row in sheet.rows():
                          yield [cell.value for cell in row]

                  return reader_to_markdown(ods_reader())


              def main():
                  """
                  Main function to handle command-line arguments and orchestrate the conversion process.
                  """
                  if len(sys.argv) < 3:
                      print("Usage: python script.py <input_file> <output_file> [delimiter]")
                      sys.exit(1)

                  input_file = sys.argv[1]
                  output_file = sys.argv[2]
                  delimiter = sys.argv[3] if len(sys.argv) > 3 else ","

                  file_extension = input_file.split(".")[-1].lower()

                  conversion_functions: dict[str, Callable[[str], str]] = {
                      "csv": lambda f: csv_tsv_to_markdown(f, delimiter),
                      "tsv": lambda f: csv_tsv_to_markdown(f, delimiter),
                      "xlsx": xlsx_to_markdown,
                      "xls": xls_to_markdown,
                      "gsheet": gsheet_to_markdown,
                      "sheet": gsheet_to_markdown,
                      "ods": ods_to_markdown,
                  }

                  if file_extension not in conversion_functions:
                      print(f"Unsupported file format: {file_extension}")
                      sys.exit(1)

                  try:
                      markdown_content = conversion_functions[file_extension](input_file)

                      with open(output_file, "w", encoding="utf-8") as file:
                          file.write(markdown_content)

                      print(f"Conversion complete. Formatted Markdown table saved to {output_file}")
                  except (
                      IOError,
                      ValueError,
                      KeyError,
                      IndexError,
                      csv.Error,
                      xlrd.XLRDError,
                      pd.errors.EmptyDataError,
                  ) as e:
                      print(f"An error occurred during conversion: {str(e)}")
                      sys.exit(1)


              if __name__ == "__main__":
                  main()

            ]]></content>
          </file>
        </directory>
      </directory>
    </contents>
  </repository>
</codemap>
